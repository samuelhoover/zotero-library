Score Dynamics: Scaling Molecular Dynamics with Picoseconds Time Steps via Conditional Diffusion Model
Tim Hsu,* Babak Sadigh, Vasily Bulatov, and Fei Zhou*
Cite This: https://doi.org/10.1021/acs.jctc.3c01361 Read Online
ACCESS Metrics & More Article Recommendations s*ı Supporting Information
ABSTRACT: We propose score dynamics (SD), a general framework for learning accelerated evolution operators with large timesteps from molecular dynamics (MD) simulations. SD is centered around scores or derivatives of the transition log-probability with respect to the dynamical degrees of freedom. The latter play the same role as force fields in MD but are used in denoising diffusion probability models to generate discrete transitions of the dynamical variables in an SD time step, which can be orders of magnitude larger than a typical MD time step. In this work, we construct graph neural network-based SD models of realistic molecular systems that are evolved with 10 ps timesteps. We demonstrate the efficacy of SD with case studies of the alanine dipeptide and short alkanes in aqueous solution. Both equilibrium predictions derived from the stationary distributions of the conditional probability and kinetic predictions for the transition rates and transition paths are in good agreement with MD. Our current SD implementation is about 2 orders of magnitude faster than the MD counterpart for the systems studied in this work. Open challenges and possible future remedies to improve SD are also discussed.
■
INTRODUCTION
Molecular dynamics (MD) simulations are ubiquitous in condensed-matter physics, chemistry, materials science, and biology. MD can be used to study equilibrium thermodynamic
as well as kinetic properties of different phases of matter,1,2 thanks to its fine level treatment of all atomic details from firstprinciples, with no presumption other than the interatomic potential. However, such accuracy comes at a high computational cost that severely limits the spatial and temporal scales accessible. Expanding these scales is key to unlocking MD’s full predictive potential. Regarding the spatial scales, parallel computing has enabled simulations of molecular systems that can cross spatial scales
and contain as many as 1010 atoms, which is gigantic compared
to fewer than 100 at MD’s inception.3 Efforts to reduce the extravagant computational cost of MD at such large spatial scales have led to the development of strategies to reduce the number of degrees of freedom, often by disregarding irrelevant or rapidly oscillating dynamical variables and focusing instead on pertinent, slowly varying variables that sufficiently elucidate structural transformations as well as chemical reactions. A common example is implicit solvation techniques that exclude
solvent molecules (e.g., water) altogether.4 Another nonexclusive approach is to design a small set of representative particles such as united atoms or beads in dissipative particle
dynamics5 and coarse-grained force fields.6 Expanding the temporal scales is particularly important when the scientific question hinges on sampling rare events,
which are ubiquitous. The temporal scales are even more challenging to expand: the intrinsically small time step Δt ≈ 1 fs is necessitated by the ∼THz frequency of typical phonon modes. Since MD is based on time integration of the Newtonian equation of motion sequentially over such tiny Δt, scaling up the temporal scales of MD is not amenable to brute-force stacking of parallel hardware. The femtosecond time step limit therefore places a tough constraint on the practical MD duration accessible within realistic wall-clock time, notwithstanding heroic demonstrations lasting for weeks or more. To deal with these limitations and facilitate rare event
sampling, a large variety of methods have been devised.7 Here, we mention only a few without attempting a comprehensive review. On parallel hardware, parallel-replica dynamics can enhance rare event sampling with the assumption of transition
state theory.8 Methods such as dissipative particle dynamics5
and coarse-grained molecular dynamics9,10 take advantage of the correlation between the spatial and temporal scales and coarsen the particles being simulated by replacing fast degrees of freedom (e.g., light/fast atoms) with heavier/slower
Received: December 12, 2023 Revised: February 28, 2024 Accepted: March 1, 2024
pubs.acs.org/JCTC Article
© XXXX The Authors. Published by
American Chemical Society A
https://doi.org/10.1021/acs.jctc.3c01361 J. Chem. Theory Comput. XXXX, XXX, XXX−XXX
This article is licensed under CC-BY-NC-ND 4.0
Downloaded via 24.91.28.223 on March 25, 2024 at 14:26:10 (UTC).
See https://pubs.acs.org/sharingguidelines for options on how to legitimately share published articles.


particles (e.g., polymer beads), resulting in lowered effective vibrational frequency compatible with a larger time step. Some popular and productive ideas modify the potential energy surface (PES) or effective temperature to make barrier crossing
more likely. Examples include hyperdynamics,11 temperature
accelerated dynamics,12 Gaussian accelerated MD, and
metadynamics.13 In contrast to MD-like methods based on differential equations, another class of methods mixes MD with Monte Carlo (MC) methods to achieve more efficient configuration
sampling, including replica-exchange MD,14 force bias Monte
Carlo,15 and hybrid MD/MC.16,17 By interspersing MD steps that continuously evolve the atomic configuration and MC steps that introduce abrupt jumps to different energy basins, such hybrid methods quickly sample rare events while keeping particle momenta in the simulated phase space. In further departure from MD, other methods such as kinetic Monte
Carlo18 and first-passage Monte Carlo19 are formulated to promote ergodicity and efficient sampling of rare events. In these methods, momenta are ignored, and one is concerned with atomic structures only. This work proposes score dynamics (SD), an alternative machine-learning (ML) approach to the dynamical simulation of atomistic systems. Recent years have witnessed the rise of scientific ML methods for atomistic simulations. Compared to the conventional method development paradigm of physical intuition and time-consuming trial-and-error, data-driven ML approaches have the potential to deliver quantitative models within relatively short development cycles based on large
training data.20,21 SD takes advantage of recent progress in ML generative models to sample discrete Markovian steps based on the transition probability matrix of molecular configurations over large timesteps (Figure 1a). At each step, conditioned on a current configuration, the next configuration is sampled via a conditional generative model, namely, the score-based generative model also known as denoising diffusion proba
bilistic model or simply the diffusion model,22−24 using a learned score function (Figure 1b). Both position-only and position + velocity versions of SD have been developed, though we focus primarily on the former. Applied to alanine dipeptide and small alkanes as case studies, SD is shown to
faithfully reproduce both the dynamical information on transition rates and transition paths and the equilibrium distributions. Importantly, trained from a set of small alkanes not containing butane, the learned model is applicable to the unseen butane with accurate dynamic and equilibrium fidelity, demonstrating good generalizability akin to ML potentials.
■
METHODS
General Formalism. Before delving into the details of SD, we started by motivating our design principles. The first one is a stochastic approach to dynamical simulations (Figure 1a) for the following three reasons. For the purpose of adopting large (≳ ps) timesteps Δt, the time-evolution of deterministic dynamical systems often diverges. Even infinitesimally close initial states X0 can and in most practical cases will evolve in divergent trajectories to a diverse set of states {XΔt} of finite variance, as measured by the Lyapunov exponent. Here, X = (r, p) is the position-momentum state. Such a distribution of {XΔt} is amenable to the probabilistic treatment. For the purpose of spatial (particle) coarsening, the equation of motion of a smaller number of slow/relevant dynamical variables A projected from all atoms is ostensibly stochastic, as
formulated under the Mori−Zwanzig formalism25,26
t
t t Ks t s s F t
AA A
( ) i ( ) ( ) ( )d ( )
t R
0
=+
(1)
where iΩ is the frequency matrix, the memory kernel K
represents history-dependent effects,27 and FR is the contribution of fast/irrelevant variables being left out of explicit consideration. Even though the underlying all-atom Newtonian equation of motion is fully deterministic (assuming nonLangevin thermostat), the selective projection process will make the dynamics of the coarse variables appear to be affected by ostensibly “random” fluctuations, e.g., a solute molecule under the influence of ignored solvent molecules. Equation 1 for the motion of A can be interpreted as a generalized Langevin equation. Note, however, the practical closure of the
Mori−Zwanzig formalism is difficult for complex systems.27−30 Finally, practical MD runs often adopt Langevin dynamics for
Figure 1. (a) Schematic illustration of distribution of diverging simulation outcomes after finite time interval. (b) Workflow of SD, which maps both the current conditioning structure and a randomly displaced candidate structure into a new one by probabilistic denoising.
Journal of Chemical Theory and Computation pubs.acs.org/JCTC Article
https://doi.org/10.1021/acs.jctc.3c01361 J. Chem. Theory Comput. XXXX, XXX, XXX−XXX
B


thermostatting, making such simulations intrinsically stochastic. The second design decision is to adopt a discrete time approach, predicting the time series {XnΔt} (n = 0, 1, ...) rather than relying on a continuous-time ordinary differential equation (ODE, deterministic molecular dynamics) or stochastic differential equations (SDEs, stochastic Langevin dynamics). Small timesteps are required for numerically stable integration of differential equations. With large, discrete timesteps ≫ fs, SD can achieve significant temporal scaling. Given the above motivations, SD is, in the most general setting, a high-order Markov-chain Monte Carlo (MCMC) method with a history-dependent conditional transition probability matrix P(Xt′|X(0:n)Δt, Δt, T) that describes the distribution of stochastic outcomes at the next step t′ = (n + 1)Δt conditioned on the previous states X(0:n)Δt at t = 0, ..., nΔt, satisfying P(X|X(0:n)Δt, Δt, T) ≥ 0, ∫ P(X|X(0:n)Δt, Δt, T)dX = 1. SD makes autoregressive predictions by iteratively (1) drawing a random state X(n+1)Δt according to P(Xt′|X(0:n)Δt, Δt, T) and (2) updating the conditioning states to X(0:n+1)Δt. Generically formulated, SD is related to some existing approaches. For example, with all atoms and infinitesimal Δt, SD is a single-step MCMC equivalent to Langevin dynamics, which is memoryless. More specifically, it is reduced in the Δt → 0 limit to the Euler−Maruyama time integration of Langevin dynamics to t′ = t + Δt
U t Mk T
P t Mk T t
tM U t
p pw
XX r p
r rp p p p
d ( )d 2 d
( , ) ( , ), diag(0, 2 ) ,
/, ( ) ,
t
tt tt
t tt t t t
B
B
= ++
| =[ ]
=+ = +
(2)
where γ, U(r), T, and M are the damping coefficient, interatomic potential, temperature, and mass, respectively, wt
is the standard Wiener process, and xt is the mean value at t′. The conditional P(Xt′|Xt, Δt → 0) is a Gaussian distribution with its mean and covariance matrix determined from the Langevin equation. Equation 2 can be generalized from infinitesimal to any finite Δt: P(Xt′|Xt, Δt) is formally solved by the Fokker−Planck Equation (FPE) in position-momentum space, also known as the Klein−Kramers equation (KKE), a PDE for evolution of the transition probability matrix P(Xt′|Xt,
Δt) over time interval Δt,31 though solving high-dimensional KKE is difficult in practice. Similar observations are made in the important limit of overdamped Langevin dynamics or Brownian dynamics. In this regime, only the positions r = X are kept, the Langevin equation becomes a first-order SDE for r, and the corresponding transition probability can be solved from the position-only FPE, also known as the Smoluchowski
equation.31 In general, however, the dynamical variables are projected from the all-atom system, memory effects are relevant according to the Mori−Zwanzig formalism, and close-form solutions of transition P are usually not available. In the large time limit Δt → ∞, the conditional probability asymptotically becomes the stationary or equilibrium Boltzmann distribution Peq(r) = exp(−U(r)/kBT)/Z independent of the conditioning states. Comparison with MD. However, the goal of formally encapsulating all the complexities of dynamics over large spatiotemporal scales is fraught with technical difficulties. The transition probability P(Xt′|X(0:n)Δt, Δt, T) is significantly more challenging than and in many ways different from the
interatomic potential U(r) in MD. (1) The first major difference is that the initial conditioning states X(0:n)Δt and final Xt′ play equally important roles in the bivariate P(Xt′| X(0:n)Δt, Δt, T) (Figure 1b), in contrast to the univariate potential energy U(r) taking a single input structure. (2) The former needs to be adjusted or retrained when a different time step or temperature is applied, while the latter is universal and can be trained once and reused. (3) We will train the SD
model with self-supervised learning of the complex dynamics32 using high-fidelity MD trajectories as training data, i.e., forcing the model to predict the next configuration from current configurations (see the next section for details), instead of supervised learning with precalculated total energies and forces as training labels for fitting potential U. (4) SD’s training data, MD trajectories, are intrinsically noisy with significant thermal fluctuations, especially at high temperature, whereas MD can be trained with clean, well-converged total energy and forces from, e.g., quantum-mechanical calculations. (5) SD as a probabilistic model needs to be able to provide statistical information including mean and variance of the predicted next configuration. For example, to quantify the statistical distribution of configurations Xt′ starting from the same initial X0, many independent runs need to be repeated to establish statistical estimates. Therefore, SD requires a significant amount of MD trajectory data to be accurate, and the more the better. In contrast, potentials are deterministic, requiring a single set of labels (energy and forces) for each input configuration. For all of these reasons, a major drawback of SD is high upfront training cost. We expect the number of MD snapshots for SD training to be several orders of magnitude higher than that for MD. (6) Finally, even if the transition probability P(Xt′|X(0:n)Δt; Δt; T) is exactly known, sampling Xt′ from this high-dimensional distribution is far from trivial. There is relief. Note that MD directly requires forces or derivative of potential energy F = −∂U/∂X. While not obvious, a similar statement can be made for SD: the score function, here defined as the derivative of conditional log-probability
tT
P tT sX X
XX
X
( , , ,)
log ( ; ; )
t nt
t nt
t
(0: )
(0: )
=|
(3)
can be used to sample new configurations Xt′22−24 (details in the following section). This is a significant simplification: a properly defined probability, P(Xt′|X(0:n)Δt; Δt; T) requires normalization, a notoriously complex and expensive process involving integration over the whole high-dimensional phase space of configurations X. Scores circumvent the need for intractable normalization. The accuracy of SD in practice therefore hinges on the quality of the approximate score model sθ. The rest of the article will discuss how to approximate scores s with ML models sθ.
Approximations by Physical Considerations. So far, we presented SD as an exact, finite-time step reformulation of MD with no loss of accuracy. It is still not useful unless we can accurately approximate the scores by learning from small-scale MD and extrapolating to larger systems, just like the fact that MD potentials and force fields are typically trained from a small number of atoms. To do this, likewise, simplifications have been introduced to make score-fitting tractable. The first approximation in this work is to ignore history dependence and adopt a one-step MCMC approach, which is considerably simpler than multistep. Second, we assume the overdamped regime and ignore velocities. With a large time step Δt = 10 ps,
Journal of Chemical Theory and Computation pubs.acs.org/JCTC Article
https://doi.org/10.1021/acs.jctc.3c01361 J. Chem. Theory Comput. XXXX, XXX, XXX−XXX
C


the velocity of solute molecules in our case studies is essentially
uncorrelated,33 leaving X = r. This can be shown quantitatively in the velocity autocorrelation function (VAF) of alanine dipeptide in Figure S4a, which shows that the velocities at 1 ps intervals are essentially uncorrelated. To test the validity of the position-only approximation, we trained a separate SD model in the full position + velocity phase space, which reproduces vanishing VAF (red dot in Figure S4a). We focus on positiononly SD in this work thereupon. Third, our score model directly predicts the score rather than taking the derivative of log P in eq 3, similar to fitting a force-field without a scalar potential. We also limited ourselves to fixed Δt = 10 ps and T = 300 K. With these approximations and simplifications, the score is s(rt′, rt, Δt, and T). The validity of these approximations is ultimately determined by the resulting accuracy. Empirical tests show that the simplified SD with position-only MCMC is accurate enough for the relatively simple molecules considered in this work. While all high-fidelity training MD simulations include all atoms, spatial (particle) coarsening will be demonstrated with an implicit-solvation-like model with solute molecule only, and water molecules are omitted from explicit consideration.
Conditional Diffusion Model. In this section, we rewrite the transition probability matrix P(rt′|rt, Δt, T) = q(z|c) as a conditional distribution q of displacement z = rt′ − rt for brevity. Here, c = (rt, Δt, T) is a shorthand for the conditioning variables. The key technical approach of SD is to approximate
the corresponding score s(z|c) using the diffusion model.22−24 The diffusion model establishes a mapping between the real data samples z and a latent distribution p′(z′)�which is typically independent and identically distributed (iid) Gaussian�by iteratively applying Gaussian noises to z toward p′(z′). The generation of a realistic z is then achieved by sampling a latent z′ and mapping it back to the original data space. The main advantage of the diffusion model over other generative models is the overall superior performance in sampling high-dimensional data such as images and videos,
even though at the cost of higher computational burdens.34 The diffusion model derives its name from a forward and a backward diffusion process or auxiliary Langevin dynamics in fictitious time τ (not to be confused with the physical time t). The forward noising process maps data to code
fg
q
fg
z zw
zz z z I
d ( ) d ( )d
() ( ,)
( ) dlog
d , () d
d 2 dlog
d
00
2
2
2 2
=+
|= |
= = (4)
where (ατ, στ) is the so-called noise schedule, f(τ) and g(τ) are related drift and diffusion coefficients, respectively, and wτ is the standard Wiener process. The span of τ is typically set to
(τmin ∼ 0+, τmax = 1), over which ατ monotonically decreases from 1 to 0, while στ monotonically increases from 0 to 1. Following this noise schedule, it can be seen that zτ initially is the “clean”, original data z = z0 at τ → 0, and approaches the iid Gaussian at τ → 1. While the forward noising process is trivial, the reverse denoising operation, or the code-to-data mapping, is nontrivial and is learned by an ML model. It can be formulated as an SDE (not shown here) or, more conveniently for our purpose,
a probability-flow ODE24
f g qc
f gs c
fg c
zz z
zz
zz
d
d () 1
2 ( ) log ( )
() 1
2 () ( , , )
() 1
2 ( ) ( , , ),
z
2
2
2
=|
=+
(5)
where the score function ∇zτ log qτ(zτ|c) is approximated by a score model sθ with parameters θ, or a noise-prediction network εθ = −sθστ to predict the noise ε in the noisy zτ from eq 4. The conditioning variable c is kept throughout to emphasize the dependence on input c. Starting with zτ sampled from iid Gaussian at τ = 1, eq 5 can be integrated back to τ = 0 to generate the desired displacement z = z0. Operationally, SD
replaces MD’s physical time integration of Δt/1 fs ≈ 104 steps with a fictitious time integration of Nτ steps between τmax and 0. Small Nτ is needed for the SD to be competitive and
worthwhile. The recent DPM-Solver35 recognizes that the form of the denoising ODE can be solved by a more dedicated exponential integrator algorithm, thereby achieving realistic sample generation in a limited number of denoising timesteps (Nτ < 100). We used the solver3 variant of DPM-Solver to solve eq 5 for generating displacement z, in typically Nτ ≈ 8− 20 auxiliary timesteps, or 24−60 function evaluations of εθ. Inference or time evolution prediction using SD is summarized in Algorithm 1.
Score Model Training. The noise-prediction network εθ(zτ, τ, rt, Δt, and T) is trained from MD trajectory data at the desired interval Δt. In this work, we fixed the time step Δt and temperature T, so they are implicit. The network is then written as εθ(zτ, τ, and rt) with three inputs: noisy displacement zτ, fictitious time τ, and conditioning structure rt. We convert rt and zτ to a combined molecular graph, which is then subjected to graph neural network (GNN) operation. Given an MD trajectory {rt}, each data point in the training set is (rt, z = rt+Δt − rt). The network is trained with the
denoising score-matching loss36
DSM r ,z , , (z , , rt)
2 t0
= (6)
Combining the description of the training data, the forward noising process, and the loss function, Algorithm 2 lists the pseudocode for training the noise model εθ. Exponential moving average (EMA) was used to facilitate model training. Rectified Adam (RAdam) was used as the gradient descent optimizer without learning rate decay or weight decay. The noise schedule (ατ, στ) follows the variance-preserving linear
variant24
log 2 , b , 1
1 02
0
= ==
where b = 2 is a simple scaling factor for an appropriate level of signal-to-noise ratio, β0 = 0.1 and β1 = 20. For every sampled
Journal of Chemical Theory and Computation pubs.acs.org/JCTC Article
https://doi.org/10.1021/acs.jctc.3c01361 J. Chem. Theory Comput. XXXX, XXX, XXX−XXX
D


minibatch of the molecules, atomic coordinates r are randomly rotated as part of the data augmentation process.
The training parameters are detailed in Table 1. All other parameters, if unspecified, default to implementations in
PyTorch 1.11.037 and PyTorch-Geometric 2.0.4,38 were used for the GNN implementation and its training.
Score Model Architecture. The noise-prediction GNN consists of three components: an encoder, a processor for graph-message-passing, and a decoder. The input molecular graph to these operations is denoted as (V, E), where V consists of node features hi for the ith node (or atom), and E consists of edge (or bond) features eij from node i to node j. In contrast to conventional molecular graph representations applied to interatomic potential or molecular property predictions with a single input structure r = rt, our representation needs to incorporate two extra variables: proposed structure r′ = rt+Δt and fictitious time τ. This is implemented using a special graph encoder. The node feature hi combines a one-hot embedding of the atom/node type a followed by a simple multilayer perception (MLP) MH and an embedding of fictitious time τ by Gaussian Fourier features
(Gaussian RFF)39 and an MLP MT
h M (one hot(a )) M (Gaussian RFF( ))
iH i T
+ (7)
The set of edges E is determined by a cutoff distance rc according to the conditioning structure r, within which a pair of nodes forms an edge connection. The edge feature eij is the concatenation of the conditioning bond vectors rij = rj − ri at t and the bond vectors after noisy displacements rij′ = rij + zτ,j-zτ,i at fictitious time τ, followed by an MLP ME
e M (r r r r )
ij E ij ij ij ij (8)
where rij and rij′ represent distance, ⊕ designates feature concatenation, and MH, ME, and MT consist of two dense linear
layers, with SiLU activation function40 after the first layer and layer normalization after the second.
We used the processor from MeshGraphNets41 for implementing graph convolutions. Although we have consid
ered rotationally equivariant GNNs for predicting the noise ε (a vector quantity), the simple design of MeshGraphNets renders the computation to be fast without much loss to accuracy, as demonstrated in previous surrogate models for
dislocation42,43 and microstructure evolution.44 The computational speed is an important aspect when performing long rollouts that would take hundreds of thousands of function evaluations. Of course, the choice of a nonequivariant model means that random rotational data augmentation is necessary at training time. The processor updates edge features eij and the node features hi with MLPs MPE and MPO that have the same architecture as MH, ME, and MT
M
M
e e e h hh
hh e
( ),
()
ij ij ij i j i
ii ji
ij
PE
PH ()
+
+
(9)
where (i) stands for the neighbor nodes of node i. Equation 9 constitutes one graph convolution or message-passing layer, and the processor consists of stacking or composition of such layers into a deep graph network. We refer to the original paper
for further details of MeshGraphNets.41 Note that in the original MeshGraphNets work, there are two sets of edges, whereas there is only one (eij) in our work. The decoder is a simple MLP MO that maps the node features at the last layer into the predicted noise
M (h )
i O i (10)
MO has the same architecture as that of MH, ME, and MT, except that there is no layer normalization at the end. The model parameters are given in Table 1. Related Works. Machine-learned interatomic potentials, one of the best known ML applications to atomistic modeling, have attracted tremendous interests and investments and are becoming powerful potential energy approximations that promise quantum mechanical accuracy at vastly reduced
computational costs.45 Meanwhile, deep generative methods, including variational autoencoders, normalizing flow, and diffusion model, have also been applied for protein folding
prediction,46 static structure generation,47−52 and collective
variable analysis.53 Employing the denoising score-matching algorithm on perturbed atomic structures, we recently developed a score-based denoising method for identification
of crystal phases and defects with state-of-the-art accuracy.54 In comparison, the development of ML methods to accelerate MD simulations is overall in a relative nascent stage that is quickly expanding and growing with promising new ideas and results. Similar to MD potential development, coarse-grained simulators can benefit from more accurate representations with
ML.55,56 In the past few years, probabilistic generative
methods, including most notably normalizing flow57,58 and
diffusion models,59−63 are being applied toward surrogate simulators trained with ground-truth data from high-fidelity trajectory. Liu and co-workers employed normalizing flows as the deep generative model to drawn samples of the Fokker−
Planck equation.57 Timewarp, a recent MCMC method, achieved significantly accelerated thermodynamic sampling of the configurational space of small peptides using a conditional normalizing flow model for proposing new moves combined with parallel replicas and Metropolis algorithm. Good agreement with the equilibrium distribution and transferability to unseen structures were shown for small peptide molecules, though at the cost of deviating from the ground truth kinetics.
Table 1. Model and Training Hyperparameters
name value
number of graph convolutions 5 node, edge, time feature dimensions 200 edge cutoff rc 4 Å (τmin, τmax) (0.001, 1) number of model updates 800 k learning rate η 0.001 EMA decay rate β 0.999 minibatch size 256
Journal of Chemical Theory and Computation pubs.acs.org/JCTC Article
https://doi.org/10.1021/acs.jctc.3c01361 J. Chem. Theory Comput. XXXX, XXX, XXX−XXX
E


Timewarp was also demonstrated with even higher efficiency as a tool for configuration exploration in an alternative rejection-free mode, i.e., without enforcing equilibrium
thermodynamics.58 Fu et al. applied a graph clustering algorithm to train coarse-grained force fields that can be used in MD with very large time steps (∼ns) and optionally adopted a conditional diffusion model to refine the coarsegrained molecular dynamics and bring the simulated structures
closer to physical or likely ones.59 While their conditional diffusion models are similar to those in this work, the timeevolution simulation is carried out by MD. Wu and Li proposed DiffMD, a diffusion model for trajectory prediction, with the noted distinction of injecting the current config
uration into the noise level.60 Arts et al. developed a coarsegrained force field using a diffusion model for accelerated
molecular (Langevin) dynamics.61 Lu et al. enhanced the efficiency of sampling protein conformations by large random displacements followed by structural refinement with a
diffusion model.62 Schreiner et al. proposed the implicit transfer operator, a multiresolution surrogate model for protein folding implemented using a diffusion model with large,
variable timesteps.63 MD Simulations. MD Simulations are performed with the
GROMACS (Version 2022.2)64 package with mixed precision and CUDA support. Each case study consists of a single molecule (alanine dipeptide or small alkane molecule) solvated in water and considers only intramolecular effects. Atomic interactions are parametrized with the OPLS all-atom force
field65 and solvated in a periodic cubic box with about 3000
TIP4 water molecules.66 The linear constraint solver (LINCS)
algorithm was used to enforce bond constraints.67 The particle
mesh Ewald scheme68 was used to treat long-range electrostatics with a cutoff radius of 1.4 nm. After energy minimization of the initial solvated structure, the cubic cell
size was equilibrated with NPT simulations at 1 bar for 0.2 ns
using the Berendsen barostat,69 and all subsequent simulations were performed in the NVT ensemble. All runs were integrated with Langevin dynamics at 300 K with a time step of 1 fs and a friction coupling time of 0.1 ps unless otherwise noted. The validation trajectory was 120 ns. The MD speed is 247 wallclock hours per μs of MD simulation using 1 IBM-Power9 central processing unit (CPU) core and 1 NVidia V100 graphics processing unit (GPU), or 101 wall-clock hours on 14 cores and 4 GPUs. Training Set. All training MD trajectories were recorded at 1 ps intervals. During training, pairs of configurations were randomly sampled at recorded frame t and t + 10, i.e., interval of 10 ps. An NVT trajectory of 100 ns was included for each molecule. To add a more diverse set of initial configurations in the training set, each alkane molecule was simulated in vacuum
at a very high temperature of 104 K at a time step of 0.5 fs for 4
× 106 steps with 4000 high-temperature alkane structures saved at 1000 step intervals. Each of the 4000 molecules was solvated in water and equilibrated under NVT for 20 ps with free water and fixed solute molecules, followed by production NVT runs for 8 ps. The 4000 shorter trajectories were added to the long NVT training trajectory for each alkane molecule. Adding diverse initial structures for alanine dipeptide was done by randomly sampling the dihedral angles φ and ψ ∈ [0, 2π] in each of 6000 initial structures, followed by similar solvation and equilibration procedures, as well as final short NVT training runs for 20 ps. The long and short NVT training MD runs have the same cell sizes and numbers of water molecules.
■
RESULTS
We attempt to accurately approximate the scores by learning from small-scale MD, just like MD potentials and force fields are typically trained from a small number of atoms. In this
Figure 2. Alanine dipeptide SD trajectory is realistically relative to literature reference and in-house MD data. (a) Alanine dipeptide structure and the dihedral angles (φ, ψ) used for describing the instantaneous configurations. (b) Free energy profile reference, reprinted (adapted) with permission from ref 70 (copyright 2010 American Chemical Society). (c,d) SD trajectory (over 100 ns with 1 ps timesteps) visualized in Ramachandran plots, in both scatter and KDE (kernel density estimation) energy contour format. Two energy minima are annotated as α and β. (e,f) In-house MD trajectory reference based on the OPLS-aa potential visualized in Ramachandran plots. (g,h) First passage time (α → β and β → α) distribution from the SD trajectory compared to the in-house MD reference. The energy levels in (b,d,f) are in the unit of kJ/mol.
Journal of Chemical Theory and Computation pubs.acs.org/JCTC Article
https://doi.org/10.1021/acs.jctc.3c01361 J. Chem. Theory Comput. XXXX, XXX, XXX−XXX
F


work, we begin with relatively simple molecules in aqueous solutions, including alanine dipeptide and small alkanes at fixed time step Δt = 10 ps and temperature T = 300 K, with a focus on detailed analysis of the performance of SD compared to the ground truth MD simulations. A generalization test (to unseen butane) was also performed. Applications to larger molecular systems will be investigated in a future work.
Rejection Criteria and Resampling. The sampling procedure of SD described in Algorithm 1 is rejection-free if the trained score model is exact. However, in practice, unphysical configurations are found to be sampled in very rare cases, e.g., breaking of C−C covalent bonds, due to inexact scores. To maintain the stability of a very long simulation is always a challenge for dynamical methods. We mitigate this by simply rejecting such outlier samples based on some predefined criteria, followed by resampling. Accordingly, the raw transition matrix P is replaced by
P(r |r) = P(r |r) × A(r ) (11)
where the acceptance A(r′) is 1 if the proposed new r′ satisfies the predefined criteria and 0 otherwise. This makes the SD implementation presented in this work a Monte Carlo method with very rare rejections (typically less than 0.01%). In the current work, the criterion involves checking the distances between the bonded pairs to ensure that no “bondbreaking” occurs. Additionally, for the alanine dipeptide, which is studied in this work, the chirality is also monitored. Such evaluations have negligible computational cost relative to that of the diffusion model sampling process and are observed to not be a bottleneck in dynamic rollouts. The details of the bond length criteria implemented in this work are as follows: for alanine dipeptide, the range of (0.9, 1.16) Å, in angstroms, for X−H bond lengths (where X is either of {C, N, O}), and (1.1, 1.7) for X−X bonds; and for alkanes, (1.02, 1.16) for C− H bonds and (1.38, 1.70) for C−C bonds. More quantitative results on the frequency of rejection will be given later. Alanine Dipeptide. Trained with MD trajectories of alanine dipeptide, a common molecule for prototyping, the corresponding SD trajectory is largely faithful to the MD
Figure 3. Ramachandran plots of conditional distributions P(Xt|X0) from SD trajectories and in-house MD data. In each plot, 1000 scatter points (from 1000 independent runs) and their KDE contours are plotted.
Journal of Chemical Theory and Computation pubs.acs.org/JCTC Article
https://doi.org/10.1021/acs.jctc.3c01361 J. Chem. Theory Comput. XXXX, XXX, XXX−XXX
G


training set (Figure 2). Specifically, the equilibrium dihedral angle (see Figure 2a) distribution or Ramachandran plot (c, d), measured over a 100 ns trajectory length, matches a
literature reference PES70 in the low-energy regions (b) and our ground-truth MD results (e, f). Note that the higher
energy regions in (b) were computed from metadynamics70 and too rare to be accessible from MD or SD simulations (c, e, d, f) within 100 ns. Further, the dynamical information, quantified as the histogram of the first-passage time between two energy minima (labeled α and β) extracted from the same 100 ns trajectory, is also in quantitative agreement with ground-truth MD (g, h). In the current work, the SD trajectory tends to be slightly more “jumpy”, or more likely to overcome energy barriers, than the MD reference. This can be observed in (c, d) where high-energy configurations around φ = 90° were oversampled compared to that of the reference MD in (e, f) and in (g, h), where the mean first passage time is slightly less than that of the MD reference. There is indeed a systematic slight underestimation of the energy barriers by the
trained score model. The origin of this discrepancy in the highenergy regions is currently unclear and will be investigated in future works. Possible speculative reasons include deficiency in the score model architecture or training data set. We also compared the SD result with MD in terms of the conditional distribution P(Xt|X0), which is empirically measured by running 1000 independent trajectories starting with the same initial configuration X0. This is straightforward in SD: simply rerun from the same initial condition with different random seeds. In MD, we first fixed the atoms on the dipeptide while allowing the water molecules to equilibrate and then restarted with random equilibrium velocities on all atoms. As shown in Figure 3, the SD conditional distributions (described by the dihedral angles ψ and φ), either from the energy minimum α or from β as the starting point, are similar to that of the MD reference data. Good agreement between SD and MD is observed with increasing simulation times t = 10, 100, 200, and 400 ps, and both methods gradually approach the equilibrium distribution, as expected. Based on this result,
Figure 4. Transition probability matrix PΔt(Cj|Ci) for alanine dipeptide SD and MD trajectories, where C are classes of dihedral distributions labeled as α, α′, β, β′, and “other”, as shown in Figure 2d. Subscript i corresponds to rows of the matrix, and j corresponds to columns. This data was measured from the 1000 independent trajectory runs (starting from α) shown in Figure 3.
Figure 5. Convergence analysis of the alanine dipeptide equilibrium dihedral distribution and first passage time distribution with respect to training set size and number of denoising iterations. (a) JSD of the dihedral distributions between SD trajectories and the in-house MD trajectory as a function of training set size and number of denoising steps (M). (b,c) First passage time distribution (α → β and β → α) measured from SD trajectories based on varying training set sizes, and from the MD reference trajectory. In (b,c), the number of denoising steps is fixed to 20.
Journal of Chemical Theory and Computation pubs.acs.org/JCTC Article
https://doi.org/10.1021/acs.jctc.3c01361 J. Chem. Theory Comput. XXXX, XXX, XXX−XXX
H


SD is therefore reasonably accurate in terms of the conditional distributions P(Xt|X0) over a wide range of t and up until t is significantly large such that the conditional distribution approaches the equilibrium distribution P(Xt|X0) → Peq(X). In addition to the conditional distribution P(Xt|X0) of alanine dipeptide, we also estimated the transition probability PΔt(Cj|Ci), where the dihedral distribution is classified into distinct rectangular regions or classes C ∈ {α, α′, β, β′, other} according to Figure 2d (the “other” class is the region not in α, α′, β, β′). In short, PΔt(Cj|Ci) describes the probability of state Ci transitioning to Cj after a certain amount of time Δt. As shown in Figure 4, the transition probability matrix of SD is largely similar to that of the MD counterpart for Δt = 10, 100, 200, and 400 ps, further validating the fidelity of the SD trajectory for replicating dynamic statistics. Using alanine dipeptide as a standard test molecule, a brief convergence analysis was conducted in order to investigate the SD trajectory quality with respect to two relevant quantities: the training set size and the number of denoising iterations, Nτ for each SD sampling. The training set size directly dictates the amount of effort and resource one can feasibly commit for a reasonably realistic SD fitting, and the number of denoising iterations Nτ directly influences the compute cost of rolling out SD trajectories. The Jensen−Shannon divergence (JSD) of dihedral distributions between SD and MD is used as a measure of the quality of the equilibrium distribution from the SD. Figure 5a shows that the JSD metric is certainly impacted by training set size, as expected, and begins to plateau after
roughly 105 samples. A data set of such size is well within the capabilities of modern hardware. On the other hand, the number of denoising iterations does not appear to influence the JSD metric in any certain fashion, i.e., more denoising iterations do not always result in lower JSD. This suggests that the inner-loop denoising ODE integration does not require a large number of iterations to be solved accurately. Therefore, for a reasonably accurate SD trajectory, one can use a fairly small number of denoising iterations, roughly in the range of 10−20, a range that is also claimed to be effective by the DPM
Solver paper.35 Unless otherwise stated, we use Nτ = 20 throughout the paper. Besides the quality of the equilibrium distribution produced from SD, the dynamical information, namely, the first passage time distribution, is also examined with respect to training set size. Figure 5b,c shows that overall, the first passage time distribution is not impacted by the training set size. In (b),
there is a systematic discrepancy between the SD result and the MD reference, namely, the slightly faster transition rates in SD, which was previously discussed. To reiterate here, the score model tends to underestimate the energy barriers possibly due to a lack of observations of high-energy configurations in the MD-generated training data set. We do not expect that increasing the training set size to include more high-energy configurations can single-handedly and completely address the issue of energy barrier underestimation, which is indeed shown in (b). However, in (c), raising the training set size beyond 10 000 samples noticeably improves the first passage time distribution. Still, these improved distributions remain to have slightly faster transition rates relative to those of the MD reference. Future work is needed to address the issue of slightly underestimated barriers, likely by devising more accurate neural network architectures, expanding and diversifying the training data set to include more transitions around highenergy configurations, and/or improving the model with domain knowledge and physical constraints. The impact of the number of denoising iterations Nτ on SD’s speed and stability for alanine dipeptide is briefly analyzed. The single-GPU SD trajectory speed is typically 80−180× faster than that of its MD counterpart for all values of Nτ considered in this work and can significantly vary depending on Nτ (Figure 6a). This result lends credence to the performance potential of SD for accelerated dynamic simulations of atomistic systems. Note that this rudimentary analysis is only based on the single-GPU performance of our first Python + PyTorch implementation for a tiny molecule, which tends to involve relatively large computational overhead, compared to that of the highly optimized Gromacs. As part of future work, we intend to conduct a more in-depth analysis on SD performance in a multinode, parallel setup with much larger molecules, which is commonly adopted in typical MD simulations. On the other hand, SD trajectories are generally stable. Since each time step in SD is a probabilistic process, if an unphysical structure is encountered during SD (based on some user-defined criteria), it can simply be rejected, followed by resampling. Notably, our predefined criteria were mainly designed to prevent catastrophic failures such as bond breaking during SD rollouts, but such events have not been observed when the score model is well trained. The number of unphysical structures sampled over a 100 000-step rollout (a long enough time interval for statistical significance) generally is affected by the number of denoising iterations Nτ but
Figure 6. Impact of the number of denoising iterations on SD trajectory compute speed (a) and stability (b). The compute speed is measured in simulation time per wall time (ns/h). The stability is measured in terms of the number of “unphysical” structures (those that did not satisfy the criteria detailed in Rejection Criteria and Resampling) sampled over 1 μs. In (a), the single-GPU (NVIDIA V100) MD compute speed is also provided. The SD speed is based on a NVIDIA RTX 2070S GPU.
Journal of Chemical Theory and Computation pubs.acs.org/JCTC Article
https://doi.org/10.1021/acs.jctc.3c01361 J. Chem. Theory Comput. XXXX, XXX, XXX−XXX
I


occasionally can be low regardless of Nτ (Figure 6b). Importantly, the rate of sampling unphysical structures is rare and does not impact the SD compute speed. Lastly, as will be shown in the following section, the SD-sampled structures are largely physical in terms of dihedral angles, bond angles, and bond lengths.
Generalization to Unseen n-Butane. Here, we demonstrate the generalizability of SD to an unseen molecule, namely, n-butane. In this experiment, the score model was trained with MD trajectory data of n-ethane, n-propane, and n-pentane. After training, the model was applied to a butane configuration, which is unseen by the model. The subsequent SD trajectory over 100 ns is then analyzed and compared with a groundtruth n-butane MD trajectory of the same length (Figure 7), which shows that the butane SD result is highly accurate in terms of (a, b) the equilibrium dihedral distribution, (c, d) the first passage time distribution of the dihedral angle dynamics, (e, f) the C−C−C bond angle distribution, and (g, h) the C− C bond length distribution. The estimated free-energy distribution plots (b, f, and h) are calculated by F = −kBT
log P, where F denotes free energy, P is the kernel density estimation of the corresponding marginal distribution, and T = 300 K. Note that the high energies are slightly underestimated, similar to alanine dipeptide. Regardless, we consider the overall SD generalization result on n-butane to be accurate. We attribute such an effective generalizability of the SD model to the node-centric nature of the underlying graph network model. Since the graph network prediction is peratom, SD essentially amounts to predicting and sampling atomic displacements based on the local environments of each atom. In this regard, SD is expected to have similar utility and transferability of ML potentials, where information learned from small-scale MD data can be applied to larger, more complex systems. Future work will focus more on the generalization of SD to unseen structures.
Generalization to Out-of-Distribution i-Butane. Here, we also investigate the generalizability of SD to an unseen and out-of-distribution i-butane, in which the center carbon bond to three methyl groups is unobserved in the training set of straight-chain n-alkanes. Regardless, the SD model was able to
Figure 7. SD model applied to unseen n-butane, resulting in a realistic trajectory compared to that of in-house MD (OPLS-aa) reference. The comparison was made in terms of equilibrium distributions of the C−C−C−C dihedral angle (a,b), the first passage time distribution of the dihedral angle (c,d), distribution of all C−C−C bond angles (e,f), and distribution of all C−C bond lengths (g,h). The model was trained with small alkanes trajectories (ethane, propane, and pentane). An example configuration is visualized in (a).
Figure 8. SD model applied to out-of-distribution i-butane, resulting in a 100 ns trajectory. (a) Equilibrium distribution of (a) C−C−C bond angles and (b) C−C bond lengths were compared to that of a reference MD (OPLS-aa) trajectory. The i-butane structure, after 100 ns of SD trajectory, is shown in (c).
Journal of Chemical Theory and Computation pubs.acs.org/JCTC Article
https://doi.org/10.1021/acs.jctc.3c01361 J. Chem. Theory Comput. XXXX, XXX, XXX−XXX
J


generate a stable trajectory over 100 ns, given an initial frame of i-butane. However, the generated topology, especially the C−C−C bond angle (Figure 8a), does not accurately replicate that of the reference MD trajectory. This is within reasonable expectation as the SD model never observed the i-alkane topology during training. On the other hand, the C−C bond length distribution (Figure 8b) to a large extent is similar to the MD reference, presumably because of the abundance of C−C bonds in the training data. Overall, this result still shows promising potential for SD generalizability; we believe that with sufficient and highly varied training data, SD can generalize to a wide range of molecules. While we focus on the fundamentals and the faithful reproduction of small prototype molecules in this work, we intend to focus on generalizability and scalability in future work. A separate test was performed for n-hexane to test the extrapolating performance. However, the results were problematic as the same SD model experienced incidents of bond breaking and could not produce long and stable trajectories. Such limited generalizability of the current SD model is a motivation for future work to address this issue.
Comparison to Previous Results. We briefly compare our approach and the corresponding results to those of previous works. Some recent generative models were able to employ larger timesteps such as 50 ps for the same alanine
dipeptide molecule58 or ∼ns.59,63 Our work aims at faithfully reproducing both the equilibrium and transient properties of MD simulations. In the alanine dipeptide and small alkane examples shown above, the typical first-passage time is about 200−1000 ps, and the chosen Δt = 10 ps is suitable for simulating such dihedral angle dynamics with efficiency and accuracy. In general, Δt is determined by the time scale of the fastest degrees of freedom that we care about reproducing. If one wants to accelerate the simulation with faithful thermodynamics and all kinetics except the thermal vibrations or phonon modes, then Δt ∼ ps is a sensible choice. In ref 58, good agreement on equilibrium distributions in Ramachandran plots were obtained, while the time correlation and jump rates were notably different from those of the ground truth MD, and the proposal acceptance rate was <2%. In contrast, our approach was designed to follow both thermodynamics and kinetics, with >99.99% acceptance rate. Reference 63 employed variable timesteps up to ns for alanine dipeptide and other fast-folding peptides and was therefore able to reach larger speed-up than this work, while we have focused the reproduction of low-level details such as C−C bond distance and C−C−C bond angle distribution, as well as preliminary demonstration of generalizability in alkanes.
■
CONCLUSIONS
This paper proposes SD, a computational method to scale up MD simulations both spatially and temporally. Rather than accelerating MD by modifying the simulated physics, SD strives to faithfully reproduce MD predictions of both kinetic processes, including transition rates and transition paths and equilibrium distributions. This is achieved by learning the transition probability matrix between atomic configurations separated by large timesteps (ps) from the MD trajectory. The transition matrix is formulated with a conditional diffusion model, whose key ingredient is a score function approximated by a GNN dependent on the current and next configurations. In this introductory paper, we focused on detailed investigations on a few small molecules, namely, alanine dipeptide
and short alkanes, in aqueous solution. By taking large timesteps and omitting solvent molecules, our SD approach was shown to quantitatively reproduce both equilibrium distributions over order parameters and kinetic properties such as transition rates/first-passage time and conditional distributions of the transition path. That both equilibrium predictions derived from the stationary distributions of the conditional probability and kinetic predictions for the transition rate paths are well reproduced is a testament for the efficacy of SD. Generalization to an unseen molecule (butane) was also demonstrated. SD can be used as an accurate surrogate of high-fidelity MD method trained from MD trajectory or applied in combination with other MDderived methods such as replica exchange or hybrid MC/MD to achieve even larger scales. Decent wall-clock speedup (up to 180×) was obtained for the small molecules considered with inexpensive classical force field, while more sizable gains could be achieved in larger systems using more accurate and expensive potentials with an optimized implementation. While a stretch, it might even be plausible to train SD using ab initio MD without fitting interatomic potentials. The purpose of this introductory paper is not to fully develop and refine the SD method for complicated systems but to introduce the basic framework and draw attention to related challenges in future studies. There are a fair number of open questions about SD that have barely been barely addressed. First, while we have demonstrated the first example of generalization in butane, extension to larger, more challenging molecules will be the subject of follow-up works. Second, the basic formalism and assumptions need a thorough investigation. For example, we considered neither history dependence nor effects of velocities, simplifications that might be justifiable in the relatively simple example of small molecules in aqueous solution. For more challenging problems, memory or velocities should be considered. Additionally, a single time step Δt = 10 ps and a single temperature T = 300 K were chosen. More work needs to be done to push these boundaries to reach even higher speedup or to generalize to an adjustable temperature. Other issues not considered include the details of ground-truth MD simulations, e.g., thermostat types and parameters. The second class of big open questions revolves around the score or log P model. Parallels can be drawn between the importance of accurate interatomic potential models for MD and that of score model for SD. The former is both a wellknown technical challenge and a fruitful test-bed of innovative solutions. We hope the issue of finding optimal functional approximations for the score or log P model, which directly determines the accuracy of SD, will attract interest from the community. The currently implemented score model has several obvious issues and needs to be improved. It directly predicts scores or displacement vectors for efficiency rather than derivatives of log P and is therefore not strictly conservative, similar to the difference between force-field and interatomic potential models. Future works will determine whether a conservative score model can offer better accuracy at an additional computational cost. The current score model also suffers from a lack of rotational equivariance and inaccuracy in its long tail of sampling rare unphysical moves (bond breaking). Similar to fitting interatomic potentials, we expect that a diverse and representative training data set, including more transitions around high-energy configurations, will be beneficial for the accuracy and generalizability of fitted score
Journal of Chemical Theory and Computation pubs.acs.org/JCTC Article
https://doi.org/10.1021/acs.jctc.3c01361 J. Chem. Theory Comput. XXXX, XXX, XXX−XXX
K


models. A notable distinction from potentials is that score functions as probabilistic models require large training data sets to learn the conditional distribution. Going forward, a high training cost is a major concern for future applications of SD. It is justifiable when the time saving in large or repeated SD simulations outweighs the initial training costs. Additionally, an improved model should be imbued with not only large MD data but also domain knowledge and physical constraints. Finally, quantifying the uncertainty and long-term stability of SD simulations is likely more difficult than MD. Future studies are needed to address these challenging problems.
■
ASSOCIATED CONTENT Data Availability Statement
The data required to reproduce this work, namely, the MD data set and the trained score model parameters, can be requested from Tim Hsu and Fei Zhou. The source code, the trained score models, and a demo for implementing the SD rollout are available at http://www.github.com/llnl/graphite. s*ı Supporting Information
The Supporting Information is available free of charge at https://pubs.acs.org/doi/10.1021/acs.jctc.3c01361.
SD results for n-ethane, n-propane, and n-pentane and a brief experiment of learning SD with both position and velocity information (PDF)
■
AUTHOR INFORMATION Corresponding Authors
Tim Hsu − Lawrence Livermore National Laboratory, Livermore, California 94551, United States; orcid.org/ 0000-0003-0274-4444; Email: hsu16@llnl.gov
Fei Zhou − Lawrence Livermore National Laboratory, Livermore, California 94551, United States; Email: zhou6@ llnl.gov
Authors
Babak Sadigh − Lawrence Livermore National Laboratory, Livermore, California 94551, United States Vasily Bulatov − Lawrence Livermore National Laboratory, Livermore, California 94551, United States
Complete contact information is available at: https://pubs.acs.org/10.1021/acs.jctc.3c01361
Author Contributions
T.H. implemented score model training and performed SD rollouts, with technical contribution from F.Z. F.Z. generated the MD data and supervised the research with inputs from all authors. Notes
The authors declare no competing financial interest.
■
ACKNOWLEDGMENTS
T.H. and F.Z. acknowledge the support by the Critical Materials Innovation Hub, an Energy Innovation Hub funded by the U.S. Department of Energy, Office of Energy Efficiency and Renewable Energy, and Advanced Materials and Manufacturing Technologies Office. B.S. and V.B. are partially supported by the Laboratory Directed Research and Development (LDRD) program (22-ERD-016) at Lawrence Livermore National Laboratory. We thank Drs. Roland Netz and Benjamin Dalton for helpful discussions regarding butane simulations. Computing support for this work came from
LLNL Institutional Computing Grand Challenge program. This work was performed under the auspices of the U.S. Department of Energy by LLNL under contract DE-AC5207NA27344.
■
REFERENCES
(1) Frenkel, D.; Smit, B. Understanding Molecular Simulation; Elsevier, 2002, p 658.
(2) Tuckerman, M. Statistical Mechanics: Theory and Molecular Simulation; Oxford Graduate Texts: Oxford, 2010. (3) Alder, B. J.; Wainwright, T. E. Phase Transition for a Hard Sphere System. J. Chem. Phys. 1957, 27, 1208−1209. (4) Feig, M.; Brooks, C. L. Recent advances in the development and application of implicit solvent models in biomolecule simulations. Curr. Opin. Struct. Biol. 2004, 14, 217−224.
(5) Hoogerbrugge, P. J.; Koelman, J. M. V. A. Simulating Microscopic Hydrodynamic Phenomena with Dissipative Particle Dynamics. Europhys. Lett. 1992, 19, 155−160. (6) Chen, C.; Depa, P.; Sakai, V. G.; Maranas, J. K.; Lynn, J. W.; Peral, I.; Copley, J. R. A comparison of united atom, explicit atom, and coarse-grained simulation models for poly(ethylene oxide). J. Chem. Phys. 2006, 124, 1−11.
(7) Perez, D.; Uberuaga, B. P.; Shim, Y.; Amar, J. G.; Voter, A. F. Annual Reports in Computational Chemistry; Elsevier, 2009; Vol. 5, pp 79−98. (8) Voter, A. F. Parallel replica method for dynamics of infrequent events. Phys. Rev. B 1998, 57, R13985−R13988. (9) Clementi, C. Coarse-grained models of protein folding: toy models or predictive tools? Curr. Opin. Struct. Biol. 2008, 18, 10−15. (10) Monticelli, L.; Kandasamy, S. K.; Periole, X.; Larson, R. G.; Tieleman, D. P.; Marrink, S. J. The MARTINI coarse-grained force field: Extension to proteins. J. Chem. Theory Comput. 2008, 4, 819− 834. (11) Voter, A. F. Hyperdynamics: Accelerated Molecular Dynamics of Infrequent Events. Phys. Rev. Lett. 1997, 78, 3908−3911. (12) So/rensen, M. R.; Voter, A. F. Temperature-accelerated dynamics for simulation of infrequent events. J. Chem. Phys. 2000, 112, 9599−9606. (13) Laio, A.; Parrinello, M. Escaping free-energy minima. Proc. Natl. Acad. Sci. U.S.A. 2002, 99, 12562−12566.
(14) Sugita, Y.; Okamoto, Y. Replica-exchange molecular dynamics method for protein folding. Chem. Phys. Lett. 1999, 314, 141−151. (15) Neyts, E. C.; Bogaerts, A. Combining molecular dynamics with monte carlo simulations: Implementations and applications. Theor. Chem. Acc. 2013, 132, 1320.
(16) Sadigh, B.; Erhart, P.; Stukowski, A.; Caro, A.; Martinez, E.; Zepeda-Ruiz, L. Scalable parallel Monte Carlo algorithm for atomistic simulations of precipitation in alloys. Phys. Rev. B 2012, 85, 184203. (17) Sadigh, B.; Erhart, P. Calculation of excess free energies of precipitates via direct thermodynamic integration across phase boundaries. Phys. Rev. B 2012, 86, 134204. (18) Voter, A. F. Radiation Effects in Solids; Springer: Netherlands, Dordrecht, 1985; Vol. 13, pp 1−23. (19) Opplestrup, T.; Bulatov, V.; Gilmer, G.; Kalos, M.; Sadigh, B. First-Passage Monte Carlo Algorithm: Diffusion without All the Hops. Phys. Rev. Lett. 2006, 97, 230602.
(20) Butler, K. T.; Davies, D. W.; Cartwright, H.; Isayev, O.; Walsh, A. Machine learning for molecular and materials science. Nature 2018, 559, 547−555. (21) Zhang, X.; Wang, L.; Helwig, J.; Luo, Y.; Fu, C.; Xie, Y.; Liu, M.; Lin, Y.; Xu, Z.; Yan, K.; et al. Artificial intelligence for science in quantum, atomistic, and continuum systems. arXiv 2023, arXiv:2307.08423. (22) Sohl-Dickstein, J.; Weiss, E.; Maheswaranathan, N.; Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. International Conference on Machine Learning; PMLR, 2015, pp 2256− 2265.
Journal of Chemical Theory and Computation pubs.acs.org/JCTC Article
https://doi.org/10.1021/acs.jctc.3c01361 J. Chem. Theory Comput. XXXX, XXX, XXX−XXX
L


(23) Ho, J.; Jain, A.; Abbeel, P. Denoising diffusion probabilistic models Advances in Neural Information Processing Systems; NeurIPS Proceedings, 2020; Vol. 33, pp 6840−6851. (24) Song, Y.; Sohl-Dickstein, J.; Kingma, D. P.; Kumar, A.; Ermon, S.; Poole, B. Score-based generative modeling through stochastic differential equations. International Conference on Learning Representations; ICLR, 2021. (25) Mori, H. Transport, collective motion, and Brownian motion. Prog. Theor. Phys. 1965, 33, 423−455.
(26) Zwanzig, R. Nonlinear generalized Langevin equations. J. Stat. Phys. 1973, 9, 215−220. (27) Klippenstein, V.; Tripathy, M.; Jung, G.; Schmid, F.; Van Der Vegt, N. F. Introducing Memory in Coarse-Grained Molecular Simulations. J. Phys. Chem. B 2021, 125, 4931−4954. (28) Li, Z.; Bian, X.; Li, X.; Karniadakis, G. E. Incorporation of memory effects in coarse-grained modeling via the Mori-Zwanzig formalism. J. Chem. Phys. 2015, 143, 243128. (29) Li, Z.; Bian, X.; Caswell, B.; Karniadakis, G. E. Construction of dissipative particle dynamics models for complex fluids via the Mori− Zwanzig formulation. Soft Matter 2014, 10, 8659−8672. (30) Wang, Q.; Ripamonti, N.; Hesthaven, J. S. Recurrent neural network closure of parametric POD-Galerkin reduced-order models based on the Mori-Zwanzig formalism. J. Comput. Phys. 2020, 410, 109402. (31) Risken, H. The Fokker-Planck Equation; Springer Series in Synergetics; Springer: Berlin, 1989; Vol. 18. (32) Yang, K.; Cao, Y.; Zhang, Y.; Fan, S.; Tang, M.; Aberg, D.; Sadigh, B.; Zhou, F. Self-supervised learning and prediction of microstructure evolution with convolutional recurrent neural networks. Patterns 2021, 2, 100243. (33) Daldrop, J. O.; Kappler, J.; Brünig, F. N.; Netz, R. R. Butane dihedral angle dynamics in water is dominated by internal friction. Proc. Natl. Acad. Sci. U.S.A. 2018, 115, 5169−5174.
(34) Yang, L.; Zhang, Z.; Song, Y.; Hong, S.; Xu, R.; Zhao, Y.; Zhang, W.; Cui, B.; Yang, M.-H. Diffusion models: A comprehensive survey of methods and applications. ACM Comput. Surv. 2024, 56, 1− 39. (35) Lu, C.; Zhou, Y.; Bao, F.; Chen, J.; Li, C.; Zhu, J. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps Advances in Neural Information Processing Systems; NeurIPS Proceedings, 2022; Vol. 35, pp 5775−5787. (36) Vincent, P. A connection between score matching and denoising autoencoders. Neural comput. 2011, 23, 1661−1674. (37) Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.; Chanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.; et al. Pytorch: An imperative style, high-performance deep learning library Advances in Neural Information Processing Systems; NeurIPS Proceedings; Vol. 32, 2019, pp 8026−8037. (38) Fey, M.; Lenssen, J. E. Fast graph representation learning with PyTorch Geometric. arXiv 2019, arXiv:1903.02428v3. (39) Tancik, M.; Srinivasan, P.; Mildenhall, B.; Fridovich-Keil, S.; Raghavan, N.; Singhal, U.; Ramamoorthi, R.; Barron, J.; Ng, R. Fourier features let networks learn high frequency functions in low dimensional domains Advances in Neural Information Processing Systems; NeurIPS Proceedings, 2020; Vol. 33, pp 7537−7547. (40) Hendrycks, D.; Gimpel, K. Gaussian error linear units (gelus). arXiv 2016, arXiv:1606.08415. (41) Pfaff, T.; Fortunato, M.; Sanchez-Gonzalez, A.; Battaglia, P. W. Learning mesh-based simulation with graph networks. arXiv 2020, arXiv:2010.03409. (42) Bertin, N.; Zhou, F. Accelerating discrete dislocation dynamics simulations with graph neural networks. J. Comput. Phys. 2023, 487, 112180. (43) Bertin, N.; Bulatov, V. V.; Zhou, F. Learning dislocation dynamics mobility laws from large-scale MD simulations. arXiv 2023, arXiv:2309.14450. (44) Fan, S.; Hitt, A. L.; Tang, M.; Sadigh, B.; Zhou, F. Accelerate Microstructure Evolution Simulation Using Graph Neural Networks
with Adaptive Spatiotemporal Resolution. arXiv 2023, arXiv:2310.15153. (45) Behler, J. Perspective: Machine learning potentials for atomistic simulations. J. Chem. Phys. 2016, 145, 170901. (46) Wu, J.; Shen, T.; Lan, H.; Bian, Y.; Huang, J. SE(3)-Equivariant Energy-based Models for End-to-End Protein Folding. 2021, bioRxiv. (47) Noé, F.; Olsson, S.; Köhler, J.; Wu, H. Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning. Science 2019, 365, 141−147.
(48) Xu, Y.; Liu, Z.; Tegmark, M.; Jaakkola, T. Poisson flow generative models. Advances in Neural Information Processing Systems; NeurIPS Proceedings, 2022; Vol. 35, pp 16782−16795. (49) Xie, T.; Fu, X.; Ganea, O.-E.; Barzilay, R.; Jaakkola, T. Crystal diffusion variational autoencoder for periodic material generation arXiv preprint 2021 arXiv:2110.06197. (50) Xu, M.; Yu, L.; Song, Y.; Shi, C.; Ermon, S.; Tang, J. Geodiff: A Geometric Diffusion Model for Molecular Conformation Generation; ICLR, 2022. (51) Jo, J.; Lee, S.; Hwang, S. J. Score-based generative modeling of graphs via the system of stochastic differential equations. International Conference on Machine Learning; PMLR, 2022, pp 10362−10383.
(52) Zheng, S.; He, J.; Liu, C.; Shi, Y.; Lu, Z.; Feng, W.; Ju, F.; Wang, J.; Zhu, J.; Min, Y.; et al. Towards Predicting Equilibrium Distributions for Molecular Systems with Deep Learning. arXiv 2023, arXiv:2306.05445. (53) Wang, Y.; Herron, L.; Tiwary, P. From data to noise to data for mixing physics across temperatures with generative artificial intelligence. Proc. Natl. Acad. Sci. U.S.A. 2022, 119, 1−8.
(54) Hsu, T.; Sadigh, B.; Bertin, N.; Park, C. W.; Chapman, J.; Bulatov, V.; Zhou, F. Score-based denoising for atomic structure identification. arXiv 2022, arXiv:2212.02421. (55) Wang, J.; Olsson, S.; Wehmeyer, C.; Pérez, A.; Charron, N. E.; De Fabritiis, G.; Noé, F.; Clementi, C. Machine Learning of CoarseGrained Molecular Dynamics Force Fields. ACS Cent. Sci. 2019, 5, 755−767. (56) Doi, H.; Matsuoka, S.; Okuwaki, K.; Hatada, R.; Minami, S.; Suhara, R.; Mochizuki, Y. Machine learning to improve efficiency of non-empirical interaction parameter for dissipative particle dynamics (DPD) simulation. Jpn. J. Appl. Phys. 2023, 62, 070901. (57) Liu, S.; Li, W.; Zha, H.; Zhou, H. Neural Parametric Fokker− Planck Equation. SIAM J. Numer. Anal. 2022, 60, 1385−1449. (58) Klein, L.; Foong, A. Y.; Fjelde, T. E.; Mlodozeniec, B.; Brockschmidt, M.; Nowozin, S.; Noé, F.; Tomioka, R. Timewarp: Transferable acceleration of molecular dynamics by learning timecoarsened dynamics. arXiv 2023, arXiv:2302.01170. (59) Fu, X.; Xie, T.; Rebello, N. J.; Olsen, B. D.; Jaakkola, T. Simulate time-integrated coarse-grained molecular dynamics with geometric machine learning. arXiv 2022, arXiv:2204.10348. (60) Wu, F.; Li, S. Z. DIFFMD: a geometric diffusion model for molecular dynamics simulations Proceedings of the AAAI Conference on Artificial Intelligence; AAAI, 2023; Vol. 37, pp 5321−5329. (61) Arts, M.; Garcia Satorras, V.; Huang, C.-W.; Zugner, D.; Federici, M.; Clementi, C.; Noé, F.; Pinsler, R.; van den Berg, R. Two for one: Diffusion models and force fields for coarse-grained molecular dynamics. J. Chem. Theory Comput. 2023, 19, 6151−6159. (62) Lu, J.; Zhong, B.; Tang, J. Score-based Enhanced Sampling for Protein Molecular Dynamics. arXiv 2023, arXiv:2306.03117. (63) Schreiner, M.; Winther, O.; Olsson, S. Implicit Transfer Operator Learning: Multiple Time-Resolution Surrogates for Molecular Dynamics. arXiv 2023, arXiv:2305.18046. (64) Abraham, M. J.; Murtola, T.; Schulz, R.; Páll, S.; Smith, J. C.; Hess, B.; Lindahl, E. Gromacs: High performance molecular simulations through multi-level parallelism from laptops to supercomputers. SoftwareX 2015, 1−2, 19−25. (65) Jorgensen, W. L.; Maxwell, D. S.; Tirado-Rives, J. Development and testing of the OPLS all-atom force field on conformational energetics and properties of organic liquids. J. Am. Chem. Soc. 1996, 118, 11225−11236.
Journal of Chemical Theory and Computation pubs.acs.org/JCTC Article
https://doi.org/10.1021/acs.jctc.3c01361 J. Chem. Theory Comput. XXXX, XXX, XXX−XXX
M


(66) Jorgensen, W. L.; Chandrasekhar, J.; Madura, J. D.; Impey, R. W.; Klein, M. L. Comparison of simple potential functions for simulating liquid water. J. Chem. Phys. 1983, 79, 926−935. (67) Hess, B.; Bekker, H.; Berendsen, H. J. C.; Fraaije, J. G. E. M. LINCS: A linear constraint solver for molecular simulations. J. Comput. Chem. 1997, 18, 1463−1472.
(68) Darden, T.; York, D.; Pedersen, L. Particle mesh Ewald: An Nlog(N) method for Ewald sums in large systems. J. Chem. Phys. 1993, 98, 10089−10092. (69) Berendsen, H. J. C.; Postma, J. P. M.; van Gunsteren, W. F.; DiNola, A.; Haak, J. R. Molecular dynamics with coupling to an external bath. J. Chem. Phys. 1984, 81, 3684−3690. (70) Vymetal, J.; Vondrasek, J. Metadynamics As a Tool for Mapping the Conformational and Free-Energy Space of Peptides� The Alanine Dipeptide Case Study. J. Phys. Chem. B 2010, 114, 5632−5642.
Journal of Chemical Theory and Computation pubs.acs.org/JCTC Article
https://doi.org/10.1021/acs.jctc.3c01361 J. Chem. Theory Comput. XXXX, XXX, XXX−XXX
N