Title:          Synthesizer: Rethinking Self-Attention for Transformer Models
Subject:        Proceedings of the International Conference on Machine Learning 2020
Keywords:       Machine Learning, ICML
Author:         Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, Che Zheng
Creator:        LaTeX with hyperref
Producer:       pdfTeX-1.40.21
CreationDate:   Tue May 25 01:09:12 2021
ModDate:        Tue May 25 01:09:12 2021
Tagged:         no
Form:           none
Pages:          10
Encrypted:      no
Page size:      612 x 792 pts (letter) (rotated 0 degrees)
File size:      726782 bytes
Optimized:      no
PDF version:    1.5
