Algorithms to Estimate Shapley Value Feature Attributions
Hugh Chen
A dissertation
submitted in partial fulfillment of the
requirements for the degree of
Doctor of Philosophy
University of Washington
2022
Reading Committee:
Su-In Lee, Chair
Tim Althoff
Linda Shapiro
Program Authorized to Offer Degree:
Paul G. Allen School of Computer Science and Engineering


©Copyright 2022
Hugh Chen


University of Washington
Abstract
Algorithms to Estimate Shapley Value Feature Attributions
Hugh Chen
Chair of the Supervisory Committee: Paul G. Allen Professor Su-In Lee Paul G. Allen School of Computer Science and Engineering
Black box machine learning models are increasingly prevalent. Their complex nature en
ables strong predictive accuracy but also makes them hard for humans to understand. One
popular strategy to bridge the gap between complex models and interpretable models is to
explain complex models using local feature attributions where a single sample’s prediction
is attributed to each of its features. In this class of explanation methods, Shapley value
feature attributions have recently caught on. Although the Shapley value is appealing for its
nice properties, it is NP-hard to compute in general. Here, we describe several works cen
tered around tractably estimating the two most common variants of Shapley value feature
attributions: marginal and conditional Shapley values.


TABLE OF CONTENTS
Page
Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iii
Chapter 1: Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
Chapter 2: Shapley value explanation algorithms . . . . . . . . . . . . . . . . . . . 4
2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.2 Feature attributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.3 Shapley values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.4 Shapley value explanations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.5 Algorithms to estimate Shapley value explanations . . . . . . . . . . . . . . 18
2.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
2.7 Recommendations based on data domain . . . . . . . . . . . . . . . . . . . . 38
2.8 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
Chapter 3: Explaining linear models . . . . . . . . . . . . . . . . . . . . . . . . . . 44
3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
3.2 Linear SHAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
3.3 Effects of Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
3.4 True to the Model or True to the Data . . . . . . . . . . . . . . . . . . . . . 51
3.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
Chapter 4: Explaining tree models . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
4.2 Advantages of tree-based models . . . . . . . . . . . . . . . . . . . . . . . . . 59
4.3 Tree SHAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
4.4 Local explanations for trees . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
4.5 Local explanations as building blocks for global insights . . . . . . . . . . . . 67
i


4.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
Chapter 5: Self-supervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . 80
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
5.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
5.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
5.4 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
Chapter 6: Explaining deep models/a series of models . . . . . . . . . . . . . . . . 108
6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
6.2 G-DeepSHAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
6.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
6.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
6.5 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
Chapter 7: Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
ii


GLOSSARY
FEATURES: features within the dataset, of which there are d.
MODEL: machine learning model f : Rd → R.
EXPLICAND: sample being explained xe ∈ Rd.
BASELINE: sample(s) being compared to xb ∈ Rd.
LOCAL FEATURE ATTRIBUTION: importance of each feature for a given explicand’s prediction φ(f, xe) ∈ Rd.
PLAYERS: a set of participants in a coalitional game D = {p1, · · · , pd}.
COALITIONAL GAME: a game, also known as a cooperative game, which is defined by a set (value) function which maps from subsets of players to a value v : 2D → R.
THE SHAPLEY VALUE: a unique solution concept in coalitional game theory to allocate credit to players in a coalitional game φ(v) ∈ Rd.
iii


DEDICATION
to my fiance, Rachel; my parents, Hsinchun and Hsiao-Hui; my sister, Hillary; my dog,
Tubulin; my grandparents; and all my friends and family ,
iv


1
Chapter 1
INTRODUCTION
Machine learning models are increasingly prevalent because they have matched or sur
passed human performance in many applications: these include Go [176], poker [138], Star
craft [201], protein folding [92], language translation [88], and more. One critical component
in their success is flexibility, or expressive power [22, 112, 35], which has been facilitated
by more complex models and improved hardware [182]. Unfortunately, their flexibility also
makes models opaque, or challenging for humans to understand. Combined with the ten
dency of machine learning to rely on shortcuts [69] (i.e., unintended learning strategies that
fail to generalize to unseen data), there is a growing demand for model interpretability [52].
This demand is reflected in increasing calls for explanations by diverse regulatory bodies,
such as the General Data Protection Regulation’s “right to explanation” [171] and the Equal
Credit Opportunity Act’s adverse action notices [102].
There are many possible ways to explain machine learning models (e.g., counterfactu
als, exemplars, surrogate models, etc.), but one extremely popular approach is local feature
attribution. In this approach, individual predictions are explained by an attribution vector
φ ∈ Rd, with d being the number of features used by the model. One prominent example
is LIME [160], which fits a simple interpretable model that captures the model’s behavior
in the neighborhood of a single sample; when a linear model is used, the coefficients serve
as attribution scores for each feature. In addition to LIME, many other methods exist to
compute local feature attributions [160, 121, 123, 175, 20, 47, 189]. One popular class of
approaches is additive feature attribution methods, which are those whose attributions sum
to a specific value, such as the model’s prediction [121].
To unify the class of additive feature attribution methods, Lundberg and Lee [121] in


2
troduced SHAP as a unique solution determined by additional desirable properties (Sec
tion 2.3). Its uniqueness depends on defining a coalitional game (or set function) based on
the model being explained (a connection first introduced in Strumbelj and Kononenko [183]).
Lundberg and Lee [121] initially defined the game as the expectation of the model’s output
when conditioned on a set of observed features. However, given the difficulty of comput
ing conditional expectations in practice, the authors suggested using a marginal expectation
that ignores dependencies between the observed and unobserved features. This point of
complexity has led to distinct Shapley value approaches that differ in how they remove fea
tures [108, 188, 87, 80, 42], as well as subsequent interpretations of how these two approaches
relate to causal interventions [87, 80] or information theory [34, 42]. Moving forward, we will
refer to all feature attributions based on the Shapley value as Shapley value explanations.
Alongside the definition of the coalitional game, another challenge for Shapley value ex
planations is that calculating them has computational complexity that is exponential in the
number of features. The original SHAP paper [121] therefore discussed several strategies
for approximating Shapley values, including weighted linear regression (KernelSHAP [121]),
sampling feature combinations (IME [183]), and several model-specific approximations (Lin
earSHAP [121, 29], MaxSHAP [121], DeepSHAP [121, 30]). Since the original work, other
methods have been developed to estimate Shapley value explanations more efficiently, using
model-agnostic strategies (permutation [25], multilinear extension [146], FastSHAP [90]) and
model-specific strategies (linear models [29], tree models [123], deep models [30, 9, 203]).
The abundance of distinct algorithms to estimate Shapley value explanations coupled
with their complexity have made this literature inaccessible, which is problematic given the
widespread use of these approaches. We describe twenty-four such algorithms by disentan
gling their complexity into two factors: (1) the approach to remove features and (2) the
tractable estimation strategy [32]1 (Chapter 2). These factors provide a natural lens through
which we can better comprehend and compare Shapley value explanation algorithms. Un
1This article is currently under review at Nature Machine Intelligence.


3
derstanding these factors can also ensure the algorithms are not misused and help identify
fundamental limitations and important future research directions. In addition, this chapter
will also serve to carefully introduce concepts of feature attributions, the Shapley value, and
Shapley value explanations.
Then, we will describe earlier projects that specifically aim to produce Shapley value
explanations tractable for key model types: linear, tree, deep, and model pipelines.
First, we develop algorithms to estimate Shapley value explanations for linear models
where they are easier to compute and compare in order to intuitively understand tradeoffs
between marginal and conditional Shapley values [29] (Chapter 3). We find that
conditional Shapley values spread credit according to statistical dependencies in the data
and marginal Shapley values provide a closer explanation of the model’s functional form.
Next, we develop methods to estimate Shapley value feature attributions for tree
models [123]2, where we find that it is surprisingly possible to tractably estimate marginal
Shapley values exactly and conditional Shapley values approximately (Chapter 4).
Then, we validate a self-supervised learning approach applied to physiological
signals [31] (Chapter 5). In this application, we train tree models based on features extracted
from neural networks. Using the tractable algorithms we developed to explain tree models,
we explain adverse outcomes in a surgical setting in terms of the physiological signals that
contributed to risk.
We develop an approach to estimate Shapley value feature attributions for series
of models [30] (Chapter 6). This approach generalizes the previous application where a
tree model sits on top of a neural network (i.e., a series of models). By propagating the
explanations through each model, we are able to explain a series of models in terms of the
original input space.
2My primary contribution to this paper was the development of the tractable algorithm to estimate marginal Shapley values exactly for tree models.


4
Chapter 2
SHAPLEY VALUE EXPLANATION ALGORITHMS
2.1 Introduction
Machine learning models are increasingly prevalent because they have matched or surpassed
human performance in many applications: these include Go [176], poker [138], Starcraft [201],
protein folding [92], language translation [88], and more. One critical component in their
success is flexibility, or expressive power [22, 112, 35], which has been facilitated by more
complex models and improved hardware [182]. Unfortunately, their flexibility also makes
models opaque, or challenging for humans to understand. Combined with the tendency of
machine learning to rely on shortcuts [69] (i.e., unintended learning strategies that fail to
generalize to unseen data), there is a growing demand for model interpretability [52]. This
demand is reflected in increasing calls for explanations by diverse regulatory bodies, such as
the General Data Protection Regulation’s “right to explanation” [171] and the Equal Credit
Opportunity Act’s adverse action notices [102].
There are many possible ways to explain machine learning models (e.g., counterfactu
als, exemplars, surrogate models, etc.), but one extremely popular approach is local feature
attribution. In this approach, individual predictions are explained by an attribution vector
φ ∈ Rd, with d being the number of features used by the model. One prominent example
is LIME [160], which fits a simple interpretable model that captures the model’s behavior
in the neighborhood of a single sample; when a linear model is used, the coefficients serve
as attribution scores for each feature. In addition to LIME, many other methods exist to
compute local feature attributions [160, 121, 123, 175, 20, 47, 189]. One popular class of
approaches is additive feature attribution methods, which are those whose attributions sum
to a specific value, such as the model’s prediction [121].


5
To unify the class of additive feature attribution methods, Lundberg and Lee [121] intro
duced SHAP as a unique solution determined by additional desirable properties (Section 2.3).
Its uniqueness depends on defining a coalitional game (or set function) based on the model
being explained (a connection first introduced in [183]). Lundberg and Lee [121] initially
defined the game as the expectation of the model’s output when conditioned on a set of
observed features. However, given the difficulty of computing conditional expectations in
practice, the authors suggested using a marginal expectation that ignores dependencies be
tween the observed and unobserved features. This point of complexity has led to distinct
Shapley value approaches that differ in how they remove features [108, 188, 87, 80, 42], as
well as subsequent interpretations of how these two approaches relate to causal interven
tions [87, 80] or information theory [34, 42]. Moving forward, we will refer to all feature
attributions based on the Shapley value as Shapley value explanations.
Alongside the definition of the coalitional game, another challenge for Shapley value ex
planations is that calculating them has computational complexity that is exponential in the
number of features. The original SHAP paper [121] therefore discussed several strategies
for approximating Shapley values, including weighted linear regression (KernelSHAP [121]),
sampling feature combinations (IME [183]), and several model-specific approximations (Lin
earSHAP [121, 29], MaxSHAP [121], DeepSHAP [121, 30]). Since the original work, other
methods have been developed to estimate Shapley value explanations more efficiently, using
model-agnostic strategies (permutation [25], multilinear extension [146], FastSHAP [90]) and
model-specific strategies (linear models [29], tree models [123], deep models [30, 9, 203]). Of
these two categories, model-agnostic approaches are more flexible but stochastic, whereas
model-specific approaches are significantly faster to calculate. To better understand the
model-agnostic approaches, we present a categorization of the approximation algorithms
based on equivalent mathematical definitions of the Shapley value, and we empirically com
pare their convergence properties (Section 2.4). Then, to better understand the model
specific approaches, we highlight the key assumptions underlying each approach (Section 2.5).
These two sources of complexity, properly removing features and accurately approximat


6
SHAP value (log-odds)
Feature value
Age SHAP value (log-odds)
Education-Num
(a) Popularity of the SHAP github repository (b) Summary of important features (c) Non-linear effects
(d) Individualized explanation
(Exec-managerial) (Husband) (Married-civilian-spouse) (Self-emp-not-inc)
Figure 2.1: Shapley value explanations are popular and practical. (a) The large number of Github stars on shap (https://github.com/slundberg/shap), the most famous package to estimate Shapley value explanations, indicates their popularity. (b)-(d) A real-world example of Shapley value explanations for a tree ensemble model trained to predict whether individuals have income greater than 50,000 dollars based on census data. (b) Local feature attributions enable a global understanding of important features. (c) Local feature attributions help explain non-linear and interaction effects. (d) Local feature attributions explain how an individual’s features influence their outcome.
ing Shapley values, have led to a wide variety of papers and algorithms on the subject.
Unfortunately, this abundance of algorithms coupled with the inherent complexity of the
topic have made the literature difficult to navigate, which can lead to misuse, especially
given the popularity of Shapley value explanations (Figure 2.1a). To address this, we pro
vide an approachable explanation of the sources of complexity underlying the computation
of Shapley value explanations.
We discuss these difficulties in detail, beginning by introducing the preliminary concepts
of feature attribution (Section 2.2) and the Shapley value (Section 2.3). Based on the various
feature removal approaches, we then describe popular variants of Shapley value explanations
as well as approaches to estimate the corresponding coalitional games (Section 2.4). Next,
based on the estimation strategies, we describe model-agnostic and model-specific algorithms
that rely on approximations and/or assumptions to tractably estimate Shapley value explana


7
tions (section 2.5). These two sources of complexity provide a natural lens through which we
present what is, to our knowledge, the first comprehensive survey of 24 distinct algorithms1
that combine different feature removal and tractable estimation strategies to compute Shap
ley value explanations. Finally, we identify gaps and important future directions in this area
of research throughout the article.
2.2 Feature attributions
Given a model f and features x1, . . . , xd, feature attributions explain predictions by assigning
scalar values that represent each feature’s importance. For an intuitive description of feature
attributions, we first consider linear models. Linear models of the form f (x) = β0 + β1x1 +
· · · + βdxd are often considered interpretable because each feature is linearly related to the
prediction via a single parameter. In this case, a common global feature attribution that
describes the model’s overall dependence on feature i is the corresponding coefficient βi. For
linear models, each coefficient βi describes the influence that variations in feature xi have on
the model output.
Alternatively, it may be preferable to give an individualized explanation that is not for
the model as a whole, but rather for the prediction f (xe) given a specific sample xe. These
types of explanations are known as local feature attributions, and the sample being explained
(xe) is called the explicand. For linear models, one reasonable local feature attribution is
φi(f, xe) = βixe
i , because it is exactly the contribution that feature i makes to the model’s
prediction for the given explicand. However, note that this attribution hides within it an
implicit assumption that we want to compare against an alternative feature value of xi = 0,
but we may wish to account for other plausible alternative values, or more generally for the
feature’s distribution or statistical relationships with other features (Section 2.4).
Linear models offer a simple case where we can understand each feature’s role via the
model parameters, but this approach does not extend naturally to more complex model types.
1This count excludes minor variations of these algorithms.


8
For model types that are most widely used today, including tree ensembles and deep learning
models, their large number of operations prevents us from understanding each feature’s role
by examining the model parameters. These flexible, non-linear models can capture more
patterns in data, but they require us to develop more sophisticated and generalizable notions
of feature importance. Thus, many researchers have recently begun turning to Shapley
value explanations to summarize important features (Figure 2.1b), surface non-linear effects
(Figure 2.1c), and provide individualized explanations (Figure 2.1d) in an axiomatic manner
(Figure 2.2b).
2.3 Shapley values
Shapley values are a tool from game theory [172] designed to allocate credit to players in
coalitional games. The players are represented by a set D = {1, . . . , d}, and the coalitional
game is a function that maps from subsets of the players to a scalar value. A game is
represented by a subset function v(S) : P(D) 7→ R, where P(D) is the power set of D
(representing all possible subsets of players) (Figure 2.2a).
To make these concepts more concrete, we can imagine a company that makes a profit
v(S) determined by the set of employees S ⊆ D that choose to work that day. A natural
question is how to compensate the employees for their contribution to the total profit. As
suming we know the profit for all subsets of employees, Shapley values assign credit to an
individual i by calculating a weighted average of the profit increase when i works with group
S versus when i does not work with group S (the marginal contribution). Averaging this
difference over all possible subsets S to which i does not belong (S ⊆ D \ {i}), we arrive at
the definition of the Shapley value:
i’s Shapley value
z }| {
φi(v) =
X
S⊆D\{i}
|S|!(|D| − |S| − 1)!
|D|!
| {z }
S’s weight
(
i’s marginal contribution
z }| {
v(S ∪ {i}) − v(S)) (2.1)
Shapley values offer a compelling way to spread credit in coalitional games, and they have


9
(b) Axioms
All players
No players
Red player
Coalitional games
Set
Shapley values
(a) Defining terms Efficiency
Monotonicity
Shapley values sum to the value of all players minus the value of none
⎬
⎭
⎫⎪
⎪
⎬
⎭
⎫⎪
⎪
⎬
⎭
⎫⎪
⎪
⎭
⎫
⎬
⎭⎫
⎬
then they should have higher credit in that game
⎬
⎭
⎫
⎪
⎪
If a player always contributes more in one game than the other,
⎭⎫
⎬
⎭⎫
⎬
Missingness
If a player never helps, they get no credit
⎭⎫
⎬
⎬
⎭
⎫⎪
⎪
Symmetry
then they should have equal credit
⎬⎭
⎫
If a player always contributes as much as another player,
⎭⎫
⎬
⎭⎫
⎬
⎭⎫
⎬
⎬
⎭
⎫
⎪
⎪
⎬
⎭
⎫
⎪
⎪
Figure 2.2: (a) Defining terms related to the Shapley value. Players either participate or abstain from the coalitional game, and the game maps from any subset of participating players to a scalar value. Shapley values are a solution concept to allocate credit to each player in a coalitional game. (b) A sufficient, but not exhaustive set of axioms that uniquely define the Shapley value.
been widely adopted in fields including computational biology [120, 140], finance [191, 192],
and more [109, 12]. Furthermore, they are a unique solution to the credit allocation problem
as defined by several desirable properties [172, 213] (Figure 2.2b).
2.4 Shapley value explanations
In this section, we present common strategies to define local feature attributions based on
the Shapley value. We also present intuitive examples based on explaining linear models,
and we discuss the tradeoffs between various approaches to removing features.


10
2.4.1 Machine learning models are not coalitional games
Although Shapley values are an attractive solution for allocating credit among players in
coalitional games, our goal is to allocate credit among features x1, . . . , xd in a machine
learning model f (x) ∈ R. Machine learning models are not coalitional games by default,
so to use Shapley values we must first define a coalitional game v(S) based on the
model f (x) (Figure 2.3a). The coalitional game can be chosen to represent various model
behaviors, including the model’s loss for a single sample or for the entire dataset [42], but
our focus is the most common choice: explaining the prediction f (xe) for a single sample xe.
When explaining a machine learning model, it is natural to view each feature xi as a
player in the coalitional game. However, we then must define what is meant by the presence
or absence of each feature. Given our focus on a single explicand xe, the presence of feature
i will mean that the model is evaluated with the observed value xe
i (Figure 2.3b). As for the
absent features, we next consider how to remove them to properly assess the influence of the
present features.
2.4.2 Removing features with baseline values
One straightforward way to remove a feature is to replace its value using a baseline sample
xb. That is, if a feature i is absent, we simply set that feature’s value to be xb
i . Then,
the coalitional game is defined as v(S) = f (τ (xe, xb, S)), where we define τ (xe, xb, S)i =
xe
i if i ∈ S or xb
i otherwise (Figure 2.3c). In words, we evaluate the model on a new sample
where present features are the explicand values and absent features are the baseline values.
As shorthand notation, we will refer to f (τ (xe, xb, S)) as f (xe
S, xbS ̄) in the remainder of the
paper.
The Shapley values for this coalitional game are referred to as baseline Shapley values
[188]. This approach is simple to implement, but the choice of the baseline is not straight
forward and can be somewhat arbitrary. Many different baselines have been considered,


11
2947
Explicand Present
Features
Absent Features
Baseline
0000
(b) Defining present features (c) Defining absent features (baseline)
2991 4814 3765 2947 4918
Baselines Uniform
(a) ML models are not coalitional games
Sets as inputs
Vectors as inputs
2711
4998
2 4 3 2 4
9 8 7 9 9
9 1 6 4 1
1 4 5 7 8
Product of marginals
29 91 48 14 37 65 29 47 49 18
Marginal Conditional
2991 4814 3765 2947 4918
(d) Defining absent features (distribution)
29 2900
Distributional assumptions
Corresponding games 29
11
98
29
9 1 6 4 1
1 4 5 7 8
29
91 14 65 47 18
2991 2947
Figure 2.3: Empirical strategies for handling absent features. (a) Machine learning models have vector inputs and coalitional games have set inputs. For simplicity of notation we assume real-valued features, but Shapley value explanations can accommodate discrete features (unlike gradient-based methods). (b) Present features are replaced according to the explicand. (c) Absent features can be replaced according to a baseline. (d) Alternatively, absent features can be replaced according to a set of baselines with different distributional assumptions. In particular, the uniform approach uses the range of the baselines’ absent features to define independent uniform distributions to draw absent features from. The product of marginals approach draws each absent feature independently according to the values seen in the baselines. The marginal approach draws groups of absent feature values that appeared in the baselines. Finally, the conditional approach only considers samples that exactly match on the present features. Note that this figure depicts empirically estimating each expectation; however, in practice, the conditional approach is estimated by fitting models (Section 2.5.1).
including an all-zeros baseline, an average across features2, a baseline drawn from a uniform
distribution, and more [189, 175, 63, 186, 95, 159]. Unfortunately, the choice of baseline
heavily influences the feature attributions, and the criteria for choosing a baseline can be
unclear. One possible motivation could be to find a neutral, uninformative baseline, but
2This baseline is most natural for image data [63, 186].


12
(a) Comparing zero baselines (c) Comparing marginal and conditional
Equivalent model and explicand
Different attributions
(b) Comparing mean baselines
10 0 0 0
-1 135 0 -135
2 70 0 140
0
-5
0
0.5
135
70 0
0
0
1
010 001
0
0
1
0.99
01
1
0.99
0
0
0
1
010 001
0
0
1
0.99
01
1
0.99
0
1 2 3
1 2 3
1 2 0
1 2 0
1 2 3
1 2 3
1 2 3
1 2.495 2.505
1 2 0
1 2 0
1 2 0
1 1.01 0.99
Independent full model
Dependent full model
Independent partial model
Dependent partial model
-10 1 0 -10
-1 135 0 -135
2 70 0 140
10
-10 1
-1 135
2 70
-5
0
0
10
Equivalent model and explicand
Same attributions
10 0
-1 135
2 70
0
0.5
135
70
Figure 2.4: Shapley values for linear models. (a)-(b) A linear model (β), an explicand (xe), a baseline (xb), and baseline Shapley values (φ) where feature 1 represents height (inches), feature 2 represents weight (lbs), and feature 3 represents gender. Features x3 and x′3 denote different ways to
represent gender, where x3 = 1 is male and x′3 = 1 is female. (a) The models and explicands on the left and right are equivalent, but a zero baseline has a different meaning in each example and thus produces different attributions. (b) In this case, we use an mean baseline, for which the encoding of gender does not affect the baseline Shapley values. (c) Comparing marginal and conditional Shapley values for different models and feature dependencies with explicand xe = (1, 1, 1) and baseline xb = (0, 0, 0). Vectors β (linear model coefficients), φm (marginal Shapley values), and φc (conditional Shapley values) have elements corresponding to x1, x2, x3, and matrix Σ’s columns and rows are x1, x2, x3. The independent models have no correlation between features and the dependent models have a surrogate feature (a highly correlated pair of features). The full model has all non-zero coefficients whereas the partial model has a zero coefficient for the third feature.
such a baseline value may not exist. For these reasons, it is common to use a distribution of
baselines instead of relying on a single baseline.
2.4.3 Removing features with distributional values
Rather than setting the removed features to fixed baseline values, another option is to average
the model’s prediction across randomly sampled replacement values; this may offer a better
method to represent absent feature information. A first approach is to sample from the


13
conditional distribution for the removed features. That is, given an explicand xe and subset
S ⊆ D, we can consider the set of present features xe
S and then sample replacement values
for the absent features according to xS ̄ ∼ p(xS ̄ | xe
S). In this case, the coalitional game is
defined as the expectation of the prediction f (xe
S, xS ̄) across this distribution. There are
several names for Shapley values with this coalitional game: conditional Shapley values [29],
conditional expectation Shapley [188], and finally conditional Shapley values [80], which is
how we will refer to them. Two issues with this approach are that estimating the conditional
expectation is challenging (Section 2.5.1), and that the resulting explanations will spread
credit among correlated features even if the model does not directly use all of them, which
may not be desirable (Section 2.4.5).
An alternative approach is to use the marginal distribution when sampling replacement
values. That is, we ignore the values for the observed features xe
S and sample replacement
values according to xS ̄ ∼ p(xS ̄). As in the previous case, the coalitional game is defined
as the expectation of the prediction across this distribution. This approach is equivalent
to averaging over baseline Shapley values with baselines drawn from the data distribution
p(x) [188]. It also has an interpretation based in causal interventions on the feature values,
but not interventions on the real-world values the features represent, interventions on the
feature values in the computer going into the machine learning model. This is equivalent to
assuming a flat causal graph (i.e., a causal graph with no causal links among features) [87, 80].
The latter interpretation has led to the name marginal Shapley values, but to avoid ambiguity
we opt for the name marginal Shapley values [80].
The conditional and marginal approaches are by far the most common feature removal
approaches in practice. Two other formulations based on random sampling are (1) the
uniform approach, where absent features are drawn from a uniform distribution covering
the feature range, and (2) the product of marginals approach, where absent features are
drawn from their individual marginal distributions (which assumes independence between
all absent features) [47, 132]. However, these distributions make a strong assumption of
independence between all features, which may be why marginal Shapley values, which make


14
a milder assumption of independence between the observed and unobserved features, are more
commonly used. In addition, there are several other approaches for handling absent features
in Shapley value-like explanations, but these can often be interpreted as approximations
of the aforementioned approaches [42]. We visualized the three main removal approaches in
Figure 2.3d, where, for simplicity, we show empirical versions that use a finite set of baselines
(e.g., a training data set) to compute each expectation [188].
2.4.4 Shapley value explanations for linear models
To highlight intuitive differences between baseline, marginal, and conditional Shapley values,
we consider the case of linear models where Shapley value explanations are easier to compute
and compare (Figure 2.4). In the following examples, we consider different linear models,
data distributions, and feature encodings to call attention to properties of these three types
of Shapley value explanations.
Baseline Shapley values are simple and can be intuitive, but choosing an appropriate
baseline is difficult. One common baseline is a sample with all zero feature values. However,
we show that an all-zeros baseline can produce counterintuitive attribution values because
the meaning of zero can be arbitrary (Figure 2.4a). In particular, we consider a case with
equivalent models and explicand, but where the gender feature is encoded such that male is 0
(x3) or male is 1 (x′
3). In the first case, the baseline Shapley value for x3 is zero (Figure 2.4a
left), signaling that being male does not impact the prediction; however, in the second case,
the baseline Shapley value for x′
3 is −10 (Figure 2.4a right), suggesting that being male leads
to lower predictions. These differing explanations are perhaps counterintuitive because the
model and explicand are exactly equivalent, but the discrepancy arises because the meaning
of zero is often arbitrary.
As an alternative to the all-zeros baseline, the mean baseline is arguably a reasonable
choice for linear models. When using this baseline value, as in Figure 2.4b, the baseline
Shapley value is zero for height and weight because the explicand’s height and weight are
equal to their average values. In addition, for a mean baseline, the baseline Shapley value is


15
the same regardless of how we encode gender. Although the mean baseline can work well for
linear models (Section 2.5.2), it can be unappealing in other cases for two reasons. First, the
mean of discrete features generally does not have a natural interpretation. For instance, the
mean of the gender variable is half female and half male, a value that is never encountered
in the dataset. This issue is compounded for non-ordinal discrete and categorical features
with more than two possible values. Second, it may be impossible for any single baseline
to represent the absence of feature information. For example, in images, removing features
with a baseline cannot give credit to pixels that match the baseline [42, 186, 30]; for a mean
baseline, this means that regions of images that resemble the mean will be biased towards
lower importance.
Rather than baseline Shapley values, we may instead prefer to use marginal and con
ditional Shapley values, which we compare in Figure 2.4c. In this example, we compute
Shapley value explanations for the same explicand and baseline, but with different models
and data distributions (multivariate Gaussians with different covariances). We generate data
in two ways: independent (zero covariance between features) or dependent (high covariance
between features x2 and x3). In addition, we consider two models: full (all coefficients are
non-zero) or partial (β3 is zero).
Comparing the independent full model case to the dependent full model case, we can
see that conditional Shapley values split credit between correlated features. This behavior
may be desirable if we want to detect whether a model is relying on a protected class
through correlated features. However, spreading credit can feel unnatural in the dependent
partial model case, where the conditional Shapley value for feature x3 (φc
3) is as high as the
conditional Shapley value for feature x2 (φc
2) even though feature x3 is not explicitly used
by the model (β3 = 0). In particular, a common intuition is that features not algebraically
used by the model should have zero attribution3 [188]. One concrete example is within a
3This intuition is described by Sundararajan and Najmi [188] as the dummy axiom [188]. Notably, their axiom is defined relative to the model, whereas the game theory literature has an existing dummy axiom defined relative to the coalitional game [137]. Shapley value explanations always satisfy the original dummy axiom, as well as all other Shapley value axioms defined in terms of the coalitional game [172, 137].


16
mortality prediction setting (NHANES), where Chen et al. [29] [29] show that for a model
that does not explicitly use body mass index (BMI) as a feature, conditional Shapley values
still give high importance to BMI due to correlations with other influential features such as
arm circumference and systolic blood pressure.
2.4.5 Tradeoffs between removal approaches
Given the many ways to formulate the coalitional game, or to handle absent features, a
natural question is which Shapley value explanation is preferred ? This question is fre
quently debated in Shapley value literature, with some papers defending marginal Shapley
values [188, 87], others advocating for conditional Shapley values [9, 4, 42], and still others
arguing for causal solutions [80]. Before discussing differences, one way the approaches are
alike is that each Shapley value explanation variant always satisfies the same axioms for its
corresponding coalitional game, although the interpretation of the axioms can differ; this
point has been discussed in prior work [188], but it is important to avoid conflating axioms
defined relative to the coalitional game and relative to the model. Below, we discuss tradeoffs
between the two most popular approaches, marginal and conditional Shapley values, because
these are most commonly implemented in public repositories and discussed in the literature.
As we have seen with linear models, conditional Shapley values tend to spread credit
between correlated features, which can surface hidden dependencies [66], whereas marginal
Shapley values yield attributions that are a description of the model’s functional form [29].
This discrepancy arises from the distributional assumptions, where conditioning on a feature
implicitly introduces information about all correlated features, thereby leading groups of
correlated features to share credit (Figure 2.3 conditional). For example, if the feature
weight is introduced when BMI is absent, then conditional Shapley values will only consider
values of BMI that make sense given the known value of weight (i.e., “on-manifold” values);
as a consequence, if the model depends on BMI but not weight, we would still observe that
introducing weight affects the conditional expectation of the model output. In contrast,
although marginal Shapley values perturb the data in less realistic ways (“off-manifold”),


17
they are able to distinguish between correlated variables and identify whether the model
functionally depends on BMI or weight, which is useful for model debugging [123].
Having two popular types of Shapley value explanations has been cited as a weak
ness [108], but this issue is not unique to Shapley values; it is encountered by a large number
of model explanation methods [42], and it is fundamental to understanding feature impor
tance with correlated or statistically dependent features. For example, with linear models,
there are similar issues with handling correlation, where multicollinearity can result in differ
ent coefficients with equivalent accuracy. One solution to handle multicollinearity for linear
models is to utilize appropriate regularization (e.g., ridge regression) [29], otherwise credit
(coefficients) can be split among correlated features somewhat arbitrarily. In the model ex
planation context, correlated features represent a similar challenge, and the multiple ways
of handling absent features can be understood as different approaches to disentangle credit
for correlated features.
Another solution to address correlated features is to incorporate causal knowledge. Causal
inference approaches typically assume knowledge of an underlying causal graph (i.e., a di
rected graph where edges indicate that one feature causes another) from which correlations
between features arise. There are a number of Shapley value explanation methods that
leverage an underlying causal graph, such as causal Shapley values [80], asymmetric Shapley
values [67], and Shapley Flow [202]. The major drawback of these approaches is that they
assume prior knowledge of causal graphs that are unknown in the vast majority of applica
tions. For this reason, conditional and marginal Shapley values represent more viable options
in many practical situations.
In this paper, we advocate for marginal and conditional Shapley values because they are
more practical than causal Shapley values, and they avoid the problematic choice of a fixed
baseline as in baseline Shapley values. In addition, they cover two of the most common use
cases for Shapley value explanations and model interpretation in general: (1) understanding
a model’s informational dependencies, and (2) understanding the model’s functional form.
An important final distinction between marginal and conditional Shapley values is the ease


18
of estimation. As we discuss next in Section 2.5, marginal Shapley values turn out to be
much simpler to estimate than conditional Shapley values.
2.5 Algorithms to estimate Shapley value explanations
Here, we describe algorithmic approaches to address the two main challenges for generating
Shapley value explanations: (1) removing features to estimate the coalitional game, and
(2) tractably calculating Shapley values despite their exponential complexity.
2.5.1 Feature removal approaches
Previously we introduced three main feature removal approaches and discussed tradeoffs
between them. In this section, we discuss how to calculate the coalitional games that corre
spond to the most popular variants of Shapley value explanations: baseline Shapley values,
marginal Shapley values, and conditional Shapley values.
Baseline Shapley values
The coalitional game for baseline Shapley values is defined as
v(S) = f (xe
S, xbS ̄), (2.2)
where f (xe
S, xbS ̄) denotes evaluating f on a hybrid sample where present features are taken
from the explicand xe and absent features are taken from the baseline xb. To compute the
value of this coalitional game, we can simply create a hybrid sample and then return the
model’s prediction for that sample. It is possible to exactly compute this coalitional game,
unlike the remaining approaches. The only parameter is the choice of baseline, which can be
a somewhat arbitrary decision.


19
Marginal Shapley values
For marginal Shapley values, the coalitional game is the marginal expectation of the model
output,
v(S) = Ep(xS ̄)[f (xe
S, xS ̄)], (2.3)
where xS ̄ is treated as a random variable representing the missing features and we take the
expectation over the marginal distribution p(xS ̄) for these missing features.
A natural approach to compute the marginal expectation is to leverage the training or
test data to calculate an empirical estimate. A standard assumption in machine learning is
that the data are independent draws from the data distribution p(x), so we can designate a
set of observed samples E as an empirical distribution and use their values for the absent
features (Figure 2.3d Marginal):
v(S) = 1
|E|
X
xb∈E
f (xe
S, xbS ̄). (2.4)
From Equation (2.4), it is clear that the empirical marginal expectation is the average
over the coalitional games for baseline Shapley values with many baselines (Equation (2.2)).
As a consequence, marginal Shapley values are also the average over many baseline Shapley
values [30]. Due to this, some algorithms estimate marginal Shapley values by first estimating
baseline Shapley values for many baselines and then averaging them [123, 30]. Note that
marginal Shapley values based on empirical estimates are unbiased if the baselines are drawn
i.i.d. from the baseline distribution (e.g., a random subset of rows from the dataset). As
such, empirical estimates are considered a reliable way to approximate the true marginal
expectation.
The empirical distribution can be the entire training dataset, but in practice it is often
a moderate number of samples from the training or test data [188, 123]. The primary
parameter is the number of baseline samples and how to choose them. If a large number of
baselines is chosen, they can safely be chosen uniformly at random; however, when using a


20
smaller number of samples, approaches based on k-means clustering can be used to ensure
better coverage of the data distribution. This empirical approach also applies to other
coalitional games such as the uniform and product of marginals, which are similarly easy to
estimate [132].
Conditional Shapley values
For conditional Shapley values, the coalitional game is the conditional expectation of the
model output,
v(S) = Ep(xS ̄|xe
S)[f (xe
S, xS ̄)], (2.5)
where xS ̄ is considered a random variable representing the missing features, and we take the
expectation over the conditional distribution p(xS ̄ | xe
S) of these missing features given the
known features xe
S from the explicand.
Computing conditional Shapley values is more difficult because the required conditional
distributions are not readily available from the training data. We can empirically estimate
conditional expectations by averaging model predictions from samples that match the expli
cand’s present features (Figure 2.3d conditional), and this exactly estimates the conditional
expectations as the number of baseline samples goes to infinity. However, this empirical
estimate does not work well in practice: the number of matching rows may be too low in
the presence of continuous features or a large number of features, leading to inaccurate and
unreliable estimates [188]. For instance, if one conditions on a height of 5.879 feet, there are
likely very few individuals with that exact height, so the empirical conditional expectation
will average over very few samples’ predictions, or potentially just the single prediction from
the explicand itself.
One natural solution is to approximate the conditional expectation based on similar
feature values rather than exact matches [128, 188, 4]. For instance, rather than condition on
baselines that are 5.879 feet tall, we can condition on baselines that are between 5.879±0.025
feet tall. This approach requires a definition of similarity, which is not obvious and may be


21
an undesirable prerequisite for an explanation method. Furthermore, these approaches do
not fully solve the curse of dimensionality, and conditioning on many features can still lead
to inaccurate estimates of the conditional expectation.
Instead of empirical estimates, a number of approaches based on fitting models have been
proposed to estimate the conditional expectations. Many of these have been identified in the
broader context of removal-based explanations [42], but we reiterate them here, summarizing
practical strengths and weaknesses:
• Parametric assumptions. Chen et al. [29] and Aas et al. [4] assume Gaussian or
Gaussian-copula distributions. Conditional expectations for Gaussian random vari
ables have closed-form solutions and are computationally efficient once the joint dis
tribution’s parameters have been estimated, but these approaches can have large bias
if the parametric assumptions are incorrect.
• Generative model. Frye et al. [66] use a conditional generative model to learn the
conditional distributions given every subset of features. The generative model provides
samples from approximate conditional distributions, and with these we can average
model predictions to estimate the conditional expectation. In general, this approach
is more flexible than simple parametric assumptions, but it has variance due to the
stochastic nature of training deep generative models, and it is difficult to assess whether
the generative model accurately approximates the exponential number of conditional
distributions.
• Surrogate model. Frye et al. [66] use a surrogate model to learn the conditional
expectation of the original model given every subset of features. The surrogate model
is trained to match the original model’s predictions with arbitrarily held-out features,
and doing so has been shown to directly approximate the conditional expectation,
both for regression and classification models [42]. This approach is as flexible as the
generative model, but it has several practical advantages: it is simpler to train, it


22
requires only one model evaluation to estimate the conditional expectation, and it has
been shown to provide a more accurate estimate in practice [66].
• Missingness during training. Covert et al. [42] describe an approach for directly
estimating the conditional expectation by training the original model to accommodate
missing features. Unlike the previous approaches, this approach cannot be applied
post-hoc with arbitrary models because it requires modifying the training process.
• Separate models. Lipovetsky and Conklin [117], ˇStrumbelj et al. [185], and Williamson
and Feng [208] directly estimate the conditional expectation given a subset of features
as the output of a model trained with that feature subset. If every model is optimal
(e.g., the Bayes classifier), then the conditional expectation estimate is exact [42]. In
practice, however, the various models will be sub-optimal and unrelated to the orig
inal one, making it unsatisfying to view it as an explanation for the original model
trained on all features. Furthermore, the computational demands of training models
with many feature subsets is significant, particularly for non-linear models such as tree
ensembles and neural networks.
As we have just shown, there are a wide variety of approaches to model conditional dis
tributions or directly estimate the conditional expectations. These approaches will generally
be biased, or inexact, because the coalitional game we require is based on the true underly
ing conditional expectation. Compounding this, it is difficult to quantify the approximation
quality because the conditional expectations are unknown, except in very simple cases (e.g.,
synthetic multivariate Gaussian data).
Of these approaches, the empirical approach produces poor estimates, parametric ap
proaches require strong assumptions, missingness during training is not model-agnostic, and
separate models is not exactly an explanation of the original model. Instead, we believe
approaches based on a generative model or a surrogate model are more promising. These
approaches are more flexible, but both require fitting an additional deep model. To assess


23
these deep models, Frye et al. [66] propose two reasonable metrics based on mean squared
error of the model’s output to evaluate the generative and surrogate model approaches.
Future work may include identifying robust architectures/hyperparameter optimization for
surrogate and generative models, analyzing how conditional Shapley value estimates change
for non-optimal surrogate and generative models, and evaluating bias in conditional Shapley
value estimates for data with known conditional distributions.
Some of the approaches we discussed approximate the intermediate conditional distri
butions (empirical, parametric assumptions, generative model) whereas others directly ap
proximate conditional expectations (surrogate model, missingness during training, separate
models). It is worth noting that approaches based on modeling conditional distributions
are independent of the particular model f . This suggests that if a researcher fits a high
quality generative model to a popular dataset, then any subsequent researchers can re-use
this generative model to estimate conditional Shapley values for their own predictive models.
However, even if fit properly, approaches based on modeling conditional distributions may
be more computationally expensive, because they require evaluating the model with many
generated samples to estimate the conditional expectation. As such, the surrogate model
approach may be more effective than the generative model approach in practice [66], and it
has been used successfully in recent work [90, 43].
In summary, in order to compute conditional Shapley values, there are two primary
parameters: (1) the approach to model the conditional expectation, for which there are
several choices. Furthermore, within the approaches that rely on deep models (generative
model and surrogate model), the training and architecture of the deep model becomes an
important yet complex dependency. (2) The baseline set used to estimate the conditional
distribution or model the conditional expectation, because each approach requires a set of
baselines (e.g., the training dataset) to learn dependencies between features. Different sets
of baselines can lead to different scientific questions [132]. For instance, using baselines
drawn from older male subpopulations, we can ask ”why does an older male individual have
a mortality risk of X% relative to the subpopulation of older males? ” [30].


24
2.5.2 Tractable estimation strategies
Calculating Shapley values is, in the general case, an NP-hard problem [50, 59]. Intuitively, a
brute force calculation based on Equation (2.1) has exponential complexity in the number of
features because it involves evaluating the model with all possible feature subsets. Given the
long history of Shapley values, there is naturally considerable research into their calculation.
Within the game theory literature, two types of estimation strategies have emerged [26,
61, 84]: (1) approximation-based strategies that produce unbiased Shapley value estimates
for any game [25], and (2) assumption-based strategies that can produce exact results in
polynomial time for specific types of games [131, 74, 25].
These two strategies have also prevailed in Shapley value explanation literature. How
ever, because some approaches rely on both assumptions and approximations, we instead
categorize the approaches as model-agnostic or model-specific. Model-agnostic estimation
approaches make no assumptions on the model class and often rely on stochastic, sampling
based estimators [183, 121, 146, 90]. In contrast, model-specific approaches rely on assump
tions about the machine learning model’s class to improve the speed of calculation, although
sometimes at the expense of exactness [29, 123, 30, 9, 203].
Model-agnostic approaches
There are several types of model-agnostic approaches to estimate Shapley value explanations.
In general, these are approximations that sidestep evaluating the model with an exponential
number of subsets by instead using a smaller number of subsets chosen at random. These
approaches are generally unbiased but stochastic; that is, their results are non-deterministic,
but they are correct in expectation.
To introduce these approaches, we describe how each approximation can be tied to a
distinct mathematical characterization of the Shapley value. We provided one such charac
terization in Section 2.3 (Equation (2.1)), but there are multiple equations to represent the
Shapley value, and each one suggests its own approximation approach. For simplicity, we


25
discuss these approaches in the context of a game v where we ignore the choice of feature
removal technique (baseline, marginal or conditional).
The classic Shapley value definition is as a semivalue [55], where each player’s credit is
a weighted average of the player’s marginal contributions. For this we require a weighting
function P (S) that depends only on the subset’s cardinality, or where P
S⊆D\{i} P (S) = 1
for i = 1, . . . , d. Then, the value for player i is given by
φi(v) =
X
S⊆D\{i}
P (S) v(S ∪ {i}) − v(S). (2.6)
Castro et al. [25] proposed an unbiased, stochastic estimator (ApproSemivalue) for any
semivalue (i.e., with arbitrary weighting function) that involves sampling subsets from D\{i}
with probability given by P (S). In this algorithm, each player’s Shapley value is estimated
one at a time, or independently. To use ApproSemivalue to estimate Shapley values, we
simply have to draw subsets according to the distribution P (S) = |S|!(|D|−|S|−1)!
|D|! . While ap
parently simple, Shapley value estimators inspired directly by the semivalue characterization
are uncommon in practice because sampling subsets from P (S) is not straightforward.
Two related approaches are Local Shapley (L-Shapley) and Connected Shapley (C
Shapley) [34]. Unlike other model-agnostic approaches, L-Shapley and C-Shapley are de
signed for structured data (e.g., images) where nearby features are closely related (spatial
correlation). Both approaches are biased Shapley value estimators because they restrict
the game to consider only coalitions of players within the neighborhood of the player being
explained4. They are variance-free for sufficiently small neighborhoods, but for large neigh
borhoods it may still be necessary to use sampling-based approximations that introduce
variance.
Next, the Shapley value can also be viewed as a random order value [172, 137],
where a player’s credit is the average contribution across many possible orderings. Here,
4Technically L-Shapley and C-Shapley are probabilistic values [137], a generalization of semivalues, because their weighting functions differ based on the spatial location of the current feature.


26
π : {1, . . . , n} → {1, . . . , n} denotes a permutation that maps from each position j to the
player π(j). Then, Π(D) denotes the set of all possible permutations and P rei(π) denotes
the set of predecessors of player i in the order π (i.e., P rei(π) = {π(1), . . . , π(j − 1)}, if
i = π(j)). Then, the Shapley value’s random order characterization is the following:
φi(v) = 1
|D|!
X
π⊆Π(D)
(v(P rei(π) ∪ {i}) − v(P rei(π))). (2.7)
There are two unbiased, stochastic estimation approaches based on this characterization.
The first approach is IME (Interactions-based Method for Explanation) [183], which esti
mates Equation (2.7) for each player with a fixed number of random permutations from
Π(D). Perhaps surprisingly, IME is analogous to ApproSemivalue, because identifying the
preceding players in a random permutation can be understood as sampling from the prob
ability distribution P (S). One variant of IME improves the estimator’s convergence by
allocating more samples to estimate φi(v) for players with high variance in their marginal
contributions, which we refer to as adaptive sampling [184].
The second approach is ApproShapley, which explains all features simultaneously given
a set of sampled permutations [25]. Rather than draw permutations independently for each
player, this approach iteratively adds all players according to each sampled permutation so
that all players’ estimates rely on the same number of marginal contributions based on the
same permutations. There are many variants that aim to draw samples efficiently (i.e., reduce
the variance of the estimates): antithetic sampling [164, 135], stratified sampling [126, 26],
orthogonal spherical codes [135], and more [196, 135, 84]. Of these approaches, antithetic
sampling is the simplest. After sampling a subset and evaluating its marginal contribution,
antithetic sampling also evaluates the marginal contribution of the inverse of that subset
(N \ S). Recent work finds that antithetic sampling provides near-best convergence in
practice compared to several more complex methods [135].
The primary difference between IME and ApproShapley is that IME estimates φi(v)
independently for each player, whereas ApproShapley estimates them simultaneously for i =


27
1, . . . , d. This means that IME can use adaptive sampling, which evaluates a different number
of marginal contributions for each player and can greatly improve convergence when many
players have low importance. In contrast, walking through permutations as in ApproShapley
is advantageous because (1) it halves the number of evaluations of the game (which are
expensive) by reusing them, and (2) it guarantees that the efficiency axiom is satisfied (i.e.,
the estimated Shapley values sum to the model’s prediction).
The third characterization of the Shapley value is as a least squares value [28, 166]. In
this approach, the Shapley value is viewed as the solution to a weighted least squares (WLS)
problem. The problem requires a weighting kernel W (S), and the credits are the coefficients
that minimize the following objective,
φ(v) = arg min
β
X
S⊆D
W (S)(u(S) − v(S))2, (2.8)
where u(S) = β0 + P
i∈S βi is an additive game5. In order to obtain the Shapley value, we
require the weighting kernel W (S) = |D|−1
(|D|
|S|)|S|(|D|−|S|) .
Based on this definition, a natural estimation approach is to sample a moderate number of
subsets according to W (S) and then solve the approximate WLS problem (Equation (2.8)).
This approach is known as KernelSHAP [121], and its statistical properties have been studied
in recent work: KernelSHAP is consistent and asymptotically unbiased6 [208], and it is
empirically unbiased even for a moderate number of samples [39]. Variants of this approach
include (1) a regularized version that introduces bias while reducing variance [121], and
(2) an antithetic version that pairs each sampled subset with its complement to improve
convergence [39].
Another approach that we refer to as SGD-Shapley is also based on the least squares
characterization. It samples subsets according to the weighting kernel W (S), but it iter
5We present a generalized version of the least squares value, which originally involved additional constraints on the coefficients [166].
6Williamson and Feng [208] prove this result for a global version of KernelSHAP, but it holds for the original version as well because it is an M-estimator [197].


28
atively estimates the solution from a random initialization using projected stochastic gra
dient descent [177]. Although it is possible to prove meaningful theoretical results for this
strategy [177], our empirical evaluation shows that the KernelSHAP estimator consistently
outperforms this approach in terms of its convergence (i.e., consistently lower estimation
error given an equal number of samples).
Finally, the last approach based on the least squares characterization is FastSHAP [90,
43]. FastSHAP learns a separate model (an explainer ) to estimate Shapley values in a single
forward pass, and it is trained by amortizing the WLS problem (Equation (2.8)) across many
data examples. As a consequence of its WLS training objective, the globally optimal estima
tion model is a function that outputs exact Shapley values. Since the explanation model will
generally be non-optimal, the resultant estimates offer imperfect accuracy and are random
across separate training runs [90]. The major advantage of FastSHAP is that developers
can frontload the cost of training the explanation model, thereby providing subsequent users
with fast Shapley value explanations.
The fourth characterization of the Shapley value is based on a multilinear extension
of the game [148, 146]. The multilinear extension extends a coalitional game to be a function
on the d-cube [0, 1]d that is linear separately in each variable. Based on an integral of the
multilinear extension’s partial derivatives, the Shapley value can be defined as
φi(v) =
Z1
0
gi(q)dq, (2.9)
where gi(q) = E[v(Gi ∪ {i}) − v(Gi)] and Gi is a random subset of D \ {i}, with each feature
having probability q of being included. Perhaps surprisingly, as with the random order value
characterization, estimating this formulation involves averaging many marginal contributions
where the subsets are effectively drawn from P (S) = |S|!(|D|−|S|−1)!
|D|! .
Based on this characterization, Okhrati and Lipani [146] introduced an unbiased sampling
based estimator that we refer to as multilinear extension sampling. The estimation consists
of (1) sampling a q from the range [0, 1], and then (2) sampling random subsets based on q


29
and evaluating the marginal contributions. This procedure introduces an additional param
eter, which is the balance between the number of samples of q and the number of subsets
Ei to generate for each value of q. The original version draws 2 random subsets for each q,
where q is sampled at fixed intervals according to the trapezoid rule [146]. Finally, in terms
of variants, Okhrati and Lipani [146] find that antithetic sampling improves convergence,
where for each subset they also compute the marginal contribution for the inverse subset.
To summarize, there are three main characterizations of the Shapley value from which
unbiased, stochastic estimators have been derived: random order values, least squares val
ues, and multilinear extensions. Within each approach, there are a number of variants.
(1) Adaptive sampling, which has only been applied to IME (per-feature random order), but
can easily be applied to a version of multilinear extension sampling that explains features
independently. (2) Efficient sampling, which aims to carefully draw samples to improve
convergence over independent sampling. In particular, one version of efficient sampling, an
tithetic sampling, is easy to implement and effective; it has been applied to ApproShapley,
KernelSHAP, and multilinear extension sampling, and it can also be easily extended to IME7.
Although the other efficient sampling techniques have mainly been examined in the context
of ApproShapley, similar benefits may exist for IME, KernelSHAP, and multilinear extension
sampling. Finally, there is (3) amortized explanation models, which have only been applied
to the least squares characterization [90], but may be extended to other characterizations
that where the Shapley value can be viewed as the solution to an optimization problem.
Empirically comparing model-agnostic approaches
In Figure 2.5, we examine the convergence of the four main stochastic estimators (IME,
ApproShapley, KernelSHAP and multilinear extension sampling) as well as several popular
7The approach of drawing an inverse subset for each independently drawn subset was introduced as “paired sampling” in KernelSHAP [39], “antithetic sampling” for ApproShapley [135], and “halved sampling” for multilinear extension sampling [146]. We refer to all of these approaches as “antithetic sampling.” [164]


30
variants of these approaches (adaptive and antithetic sampling8). We experiment with three
datasets with varying numbers of features, all using XGBoost models. To calculate conver
gence, we calculate estimation error (mean squared error) of estimates of baselines Shapley
values from the true baseline Shapley values computed using Interventional TreeSHAP. We
also introduce four new approaches: IME with antithetic sampling, a new version of multilin
ear extension sampling that explain one feature at a time9, and antithetic/adaptive sampling
variants of this new multilinear extension sampling approach.
For the diabetes and NHANES datasets, which have 10 and 79 features respectively, the
anthithetic version of KernelSHAP converges fastest to the true Shapley values. However,
for the blog dataset, which has 280 features, we observe that IME and multilinear (feature)
converge fastest, likely due to their use of adaptive sampling. Based on this finding, we
hypothesize that adaptive sampling is important in the presence of many features, because
with many features there are more likely to be features without interaction effects or with
near-zero contributions. Such features will have little to no variance in their marginal con
tributions. We tested this hypothesis by modifying the diabetes dataset by adding 100 zero
features that have no impact on the model and therefore no interaction effects. In this sce
nario, we find that the dummy features slow convergence and that the adaptive sampling
approaches are least affected, likely because they can determine which features converge
rapidly and then ignore them.
Finally, an important desideratum of feature attribution estimates is the efficiency ax
iom [172], which means that the attributions sum to the game’s value with all players minus
the value with none (Figure 2.2b). In terms of the sampling-based estimators, only Ap
proShapley and KernelSHAP are guaranteed to satisfy the efficiency property. However, it
is possible to adjust attributions by evenly splitting the efficiency gap between all features
using the additive efficient normalization operation [166]. This normalization step is used to
8The other efficient sampling techniques and amortized explanation models are more complex and out of the scope for this review.
9The initial version of multilinear extension sampling [146] explains all features simultaneously. This enables re-use of model evaluations, but prohibits the use of adaptive sampling.


31
ensure that the efficiency property holds for both FastSHAP and SGD-Shapley [90, 177]. It
can also be used to ensure efficiency as a final post-processing step for IME and multilinear
extension sampling, and it is guaranteed to improve estimates in terms of Euclidean distance
to the true Shapley values without affecting the estimators’ bias [90].
These model-agnostic strategies for estimating Shapley values are appealing because they
are flexible: they can be applied to any coalitional game and therefore any machine learning
model. However, one major downside of these approaches is that they are inherently stochas
tic. Although most methods are guaranteed to be correct given an infinite number of samples
(i.e., they are consistent estimators), users have finite computational budgets, leading to es
timators with potentially non-trivial variance. In response, some methods utilize techniques
to forecast and detect convergence when the estimated variance drops below a fixed thresh
old [41, 39]. However, even with convergence detection, model-agnostic explanations can be
prohibitively expensive. Motivated in part by the computational complexity of these meth
ods, a number of approaches have been developed to estimate Shapley value explanations
more efficiently by making assumptions about the type of model being explained.
Model-specific approaches
In terms of model-specific approaches, algorithms have been designed for several popular
model types: linear models, tree models, and deep models. These approaches are less flexible
than model-agnostic approaches, in that they assume a specific model type and often a
specific feature removal approach, but they are generally significantly faster to calculate.
First, one of the simplest model types to explain is linear models (also discussed in
Section 2.4). For linear models, baseline and marginal Shapley values have a closed-form
solution based on the coefficients of the linear model (β) and the values of the explicand
(xe) and the baseline(s). If we let E represent a set of baseline values for marginal Shapley
values, or E = {xb} for baseline Shapley values, we can write the mean value for feature xi as
μi = 1
|E|
P
xb∈E xb. Then, LinearSHAP [121, 184] gives the following result for the marginal
or baseline Shapley values:


32
φi(f, xe) = βi(xe
i − μb
i ). (2.10)
Computing these values is straightforward, and it has linear complexity in the number of
features, versus exponential complexity in the general case. Interestingly, for linear models
the marginal Shapley value is equivalent to the baseline Shapley value with a mean baseline,
which is why the mean baseline may be a good choice for linear models (see Section 2.4.4).
Alternatively, correlated LinearSHAP [29] estimates conditional Shapley values for linear
models assuming that the data follows a multivariate Gaussian distribution. The key idea
behind this approach is that for linear models, the expectation of the model’s prediction
equals the model’s prediction for the average sample, or E[f (x) | xS] = f (E[x | xS])), which
is not true in general. Using the closed-form solution for E[x | xS] given multivariate normal
data, we can calculate the conditional Shapley values as
φi(f, xe) = β⊤Aiμ + β⊤Bixe, (2.11)
where μ is the mean feature vector and Ai and Bi are summations over an exponential number
of coalitions (see [29] for more details). In practice, Ai and Bi are estimated using a sampling
based procedure. Then, subsequent explanations are extremely fast to compute regardless
of the explicand value. This approach resembles FastSHAP [90] because both approaches
incur most of their computation up-front and then provide very fast explanations. Correlated
LinearSHAP is biased if the data does not follow a multivariate Gaussian distribution, which
is rarely the case in practice, and it is stochastic because Ai and Bi are estimated.
Next, another popular model type is tree models. Tree models include decision trees,
as well as ensembles like random forests and gradient boosted trees. These are more complex
than linear models and they can represent non-linear and interaction effects, but perhaps
surprisingly, it is possible to calculate baseline and marginal Shapley values exactly (Interven
tional TreeSHAP) and approximate conditional Shapley values (Path-dependent TreeSHAP)
tractably for such models.


33
Interventional TreeSHAP is an algorithm that exactly calculates baseline and marginal
Shapley values in time linear in the size of the tree model and the number of baselines [123].
This is possible because a tree model can be represented as a disjoint set of outputs for
each leaf in the tree. Then, each leaf’s contribution to any given feature’s Shapley value
can be computed at the leaves of the tree assuming a coalitional game whose players are the
features along the path from the root to the current leaf. Using a dynamic programming
algorithm, Interventional TreeSHAP computes the Shapley value explanations for all features
simultaneously by iterating through the nodes in the tree.
Then, Path-dependent TreeSHAP is an algorithm designed to estimate conditional Shap
ley values, where the conditional expectation is approximated by the structure of the tree
model [123]. Given a set of present features, the algorithm handles internal nodes for ab
sent features by traversing each branch in proportion to how many examples in the dataset
follow each direction. This algorithm can be viewed as an application of Shapley cohort
refinement [128], where the cohort is defined by the preceding nodes in the tree model and
the baselines are the entire training set. In the end, it is possible to estimate a biased,
variance-free version of conditional Shapley values in O(LH2) time, where L is the number
of leaves and H is the depth of the tree. Path-dependent TreeSHAP is a biased estimator for
conditional Shapley values because its estimate of the conditional expectation is imperfect.
In comparison to Interventional TreeSHAP, Path-dependent TreeSHAP does not have a
linear dependency on the number of baselines, because it utilizes node weights that represent
the portion of baselines that fall on each node (based on the splits in the tree). Finally, in
order to incorporate a tree ensemble, both approaches calculate explanations separately for
each tree in the ensemble and then combine them linearly. This yields exact estimates for
baseline and marginal Shapley values, because the Shapley value is additive with respect to
the model [123].
Finally, another popular but opaque class of models are deep models (i.e., deep neural
networks). Unlike for linear and tree models, we are unaware of any approach to estimate
conditional Shapley values for deep models, but we discuss several approaches that estimate


34
baseline and marginal Shapley values.
One early method to explain deep models, called DeepLIFT, was designed to propagate
attributions through a deep network for a single explicand and baseline [175]. DeepLIFT
propagates activation differences through each layer in the deep network, while maintaining
the Shapley value’s efficiency property at each layer using a chain rule based on either a
Rescale rule or a RevealCancel rule, which can be viewed as approximations of the Shapley
value [30]. Due to the chain rule and these local approximations, DeepLIFT produces biased
estimates of baseline Shapley values. Later, an extension of this method named DeepSHAP
was designed to produce biased estimates of marginal Shapley values [30]. Despite its bias,
DeepSHAP is useful because the computational complexity is on the order of the size of
the model and the number of baselines, and the explanations have been shown to be useful
empirically [158, 30]. In addition, the Rescale rule is general enough to propagate attributions
through pipelines of linear, tree, and deep models [30].
Another method to estimate baseline Shapley values for deep models is Deep Approximate
Shapley Propagation (DASP) [9]. DASP utilizes uncertainty propagation to estimate baseline
Shapley values. To do so, the authors rely on a definition of the Shapley value that averages
the expected marginal contribution for each coalition size. For each coalition size k and a zero
baseline, the input distribution from the random coalitions is modeled as a normal random
variable whose parameters are a function of k. Since the input distributions are normal
random variables, it is possible to propagate uncertainty for specific layers by matching first
and second-order central moments and thereby estimate each expected marginal contribution.
Based on an empirical study, DASP produces baseline Shapley values estimates with lower
bias than DeepLIFT [9]. However, DASP is more computationally costly and requires up to
O(d2) model evaluations, where d is the number of features. Although DASP is deterministic
(variance-free) with O(d2) model evaluations, it is biased because the moment propagation
relies on an assumption of independent inputs that is violated at internal nodes whose inputs
are given by the previous layer’s outputs.
One final method to estimate baseline Shapley values for deep models is Shapley Ex


35
planation Networks (ShapNets) [203]. ShapNets restrict the deep model to have a specific
architecture for which baseline Shapley values are easier to estimate. The authors make a
stronger assumption than DASP or DeepLIFT/DeepSHAP by not only restricting the model
to be a neural network, but by requiring a specific architecture where hidden nodes have a
small input dimension h (typically between 2-4). In this setting, ShapNets can construct
baseline Shapley values for each hidden node because the exponential cost is low for small h.
The authors present two methods that follow the architecture assumption. (1) Shallow Shap
Nets: networks that have a single hidden layer, and where baseline Shapley values can be
calculated exactly. Although they are easy to explain, these networks suffer in terms of model
capacity and have lower predictive accuracy than other deep models. (2) Deep ShapNets:
networks with multiple layers through which we can calculate explanations hierarchically.
For Deep ShapNets, the final estimates are biased because of this hierarchical, layer-wise
procedure. However, since Deep ShapNets can have multiple layers, they are more perfor
mant in terms of making predictions, although they are still more limited than standard deep
models. An additional advantage of ShapNets is that they enable developers to regularize
explanations based on prior information without a costly estimation procedure [203].
DASP and Deep ShapNets are originally designed to estimate baseline Shapley values
with a zero baseline: DASP assumes a zero baseline to obtain an appropriate input dis
tribution, and Deep ShapNets uses zero baselines in internal nodes. However, it may be
possible to adapt DASP and Deep ShapNets to use arbitrary baselines (as in DeepLIFT and
Shallow ShapNets), in which case it would be possible to estimate marginal Shapley values
as DeepSHAP does. In terms of computational complexity, DeepLIFT, Shallow ShapNets,
and Deep ShapNets can estimate baseline Shapley values with a constant number of model
evaluations (for a fixed h). In contrast, DASP requires a minimum of d model evaluations
and up to O(d2) model evaluations for a single estimate of baseline Shapley values.
A final difference between these approaches is in their assumptions. Shallow ShapNets and
Deep ShapNets make the strongest assumptions by restricting the deep model’s architecture.
DASP makes a strong assumption that we can perform first and second-order central moment


36
matching for each layer in the deep model, and the original work only describes moment
matching for affine transformations, ReLU activations and max pooling layers. Finally,
DeepLIFT and DeepSHAP assume deep models, but they are flexible and support more types
of layers than DASP or ShapNets. However, as a consequence of DeepLIFT’s flexibility, its
baseline Shapley value estimates have higher bias compared to DASP or ShapNets [9, 203].
2.6 Discussion
In this work, we provided a detailed overview of numerous algorithms for generating Shapley
value explanations. In particular, we delved into the two main factors of complexity under
lying such explanations: the feature removal approach and the tractable estimation strategy.
Disentangling the complexity in the literature into these two factors allows us to more easily
understand the key innovations in recently proposed approaches.
In terms of feature removal approaches, algorithms that aim to estimate baseline Shapley
values are generally unbiased, but choosing a single baseline to represent feature removal
is challenging. Similarly, algorithms that aim to estimate marginal Shapley values will also
generally be unbiased in their Shapley value estimates. Finally, algorithms that aim to
estimate conditional Shapley values will be biased because the conditional expectation is
fundamentally challenging to estimate. Conditional Shapley values are currently difficult to
estimate with low bias and variance, except in the case of linear models; however, depending
on the use case, it may be preferable to use an imperfect approximation rather than switch
to baseline or marginal Shapley values.
In terms of the exponential complexity of Shapley values, model-agnostic approaches are
often more flexible and bias-free, but they produce estimators with non-trivial variance. By
contrast, model-specific approaches are typically deterministic and sometimes unbiased. Of
the model-specific methods, only LinearSHAP and Interventional TreeSHAP have no bias
for baseline and marginal Shapley values. In particular, we find that the Interventional
TreeSHAP explanations are fairly remarkable for being non-trivial, bias-free, and variance
free. As such, tree models including decision trees, random forests, and gradient boosted


37
trees are particularly well-suited to Shapley value explanations.
Furthermore, based on the feature removal approach and estimation strategy of each
approach, we can understand the sources of bias and variance within many existing algo
rithms (Table 2.1). IME [183], for instance, is bias-free, because marginal Shapley values
and the random order value estimation strategy are both bias-free. However, IME estimates
have non-zero variance because the estimation strategy is stochastic (random order value
sampling). In contrast, Shapley cohort refinement estimates [128] have both non-zero bias
and non-zero variance. Their bias comes from modeling the conditional expectation using
an empirical, similarity-based approach, and the variance comes from the sampling-based
estimation strategy (random order value sampling).
In practice, Shapley value explanations are widely used in both industry and academia.
Although they are powerful tools for explaining models, it is important for users to be aware
of important parameters associated with the algorithms used to estimate them. In partic
ular, we recommend that any analysis based on Shapley values should report parameters
including the type of Shapley value explanation (the feature removal approach), the base
line distribution used to estimate the coalitional game, and the estimation strategy. For
sampling-based strategies, it is important for users to include a discussion of convergence
in order to validate their feature attribution estimates. Finally, developers of Shapley value
explanation tools should strive to be transparent about convergence by explicitly performing
automatic convergence detection. Convergence results based on the central limit theorem are
straightforward for the majority of model-agnostic estimators we discussed, although they
are not always implemented in public packages. Note that convergence analysis is more dif
ficult for the least squares estimators, but Covert and Lee [39] discuss this issue and present
a convergence detection approach for KernelSHAP.
Future research directions include investigating new stopping conditions for convergence
detection. Existing work proposes stopping once the largest standard deviation is smaller
than a prescribed threshold [39], but depending on the threshold, the variance may still be
high enough that the relative importance of features can change. Therefore, a new stopping


38
condition could be when additional marginal contributions are highly unlikely to change the
relative ordering of attributions for all features. Another important future research direction
is Shapley value estimation for deep models. Current model-specific approaches to explain
deep models are biased, even for marginal Shapley values, and no model-specific algorithms
exist to estimate conditional Shapley values. One promising model-agnostic approach is
FastSHAP [90, 43], which speeds up explanations using an explainer model, although it
requires a large upfront cost to train this model. Finally, because approximating the con
ditional expectation for conditional Shapley values is so hard, it constitutes an important
future research direction that would benefit from new methods or systematic evaluations of
existing approaches.
2.7 Recommendations based on data domain
What we have discussed in this paper is largely agnostic to the type of data. However, there
are a few characteristics of the data being analyzed that may change the best practices for
generating Shapley value explanations.
The first characteristic is the number of features. Models with more input features will
be more computationally expensive to explain. In particular, for model-agnostic algorithms,
as the number of features increases, the total number of possible coalitions increases expo
nentially. In this setting it may be valuable to reduce the number of features by carefully
filtering the ones which do not vary or are highly redundant with other features. This type
of feature selection can even be performed prior to model-fitting and is already a common
practice.
The second characteristic is the number of samples. The number of samples likely plays
a larger role in fitting the original predictive model than in generating the explanation.
However, for conditional Shapley values, having a large number of samples is important for
creating accurate estimates of the conditional expectations/distributions. If the number of
samples is very low, it may be better to rely on parametric assumptions or use marginal
Shapley values instead.


39
The third characteristic is the feature correlation. In highly correlated settings, one
can expect larger discrepancies between marginal and conditional Shapley values. Then,
carefully choosing the feature removal approach or comparing estimates of both marginal
and conditional Shapley values can be valuable. Highly correlated features can also make it
harder to understand feature attributions in general. For images in particular, the importance
of a single pixel may not be semantically meaningful in isolation. In these cases, it may be
useful to use explanation methods that aim to understand higher level concepts [103].
Beyond correlated features, there may also be structure within the data. Tabular data are
typically considered unstructured whereas image and text data is structured because neigh
boring pixels and words are strongly correlated. For tabular data, it may be best to use tree
ensembles (gradient boosted trees or random forests) which are both performant and easy to
explain using the two versions fo TreeSHAP (Interventional and Path-dependent) [123]. For
structured data, it may be valuable to use methods such as L-Shapley and C-Shapley that are
designed to estimate Shapley value explanations more tractably in structured settings [34].
Furthermore, grouping features may be natural for structured data (e.g., superpixels for im
age data or sentences/n-grams for text data), because it greatly reduces the computational
complexity of most algorithms. Finally, since structured data often calls for complex deep
models, which are expensive to evaluate, methodologies such as FastSHAP can be useful for
accelerating explanations.
The fourth characteristic is prior knowledge of causal relationships. Although causal
knowledge is unavailable for the vast majority of datasets, it can be used to generate Shapley
value explanations that better respect causal relationships [80, 67] or generate explanations
that assign importance to edges in the causal graph [202]. These techniques may be a better
alternative to conditional Shapley values, which respect the data manifold, because they
respect the causal relationships underlying the correlated features.
The fifth characteristic is whether there is a natural interpretation of absent features. For
certain types of data, there may be preconceived notions of feature absence. For instance,
in text data it may be natural to remove features from models that take variable length


40
inputs or use masking tokens to denote feature removal. In images, it is often common
to assume some form of gray, black, or blurred baseline; these approaches are somewhat
dissatisfying because they are data-specific notions of feature removal. However, given that
model evaluations are often exorbitantly expensive in these domains, these techniques may
provide simple, yet tractable alternatives to marginal or conditional Shapley values10.
2.8 Related work
In this paper, we focused on describing popular algorithms to estimate local feature attri
butions based on the Shapley value. However, there are a number of adjacent explanation
approaches that are not the focus of this discussion. Two broad categories of such ap
proaches include alternative definitions of coalitional games, and different game-theoretic
solution concepts.
We focus on three popular coalitional games where the players represent features and the
value is the model’s prediction for a single example. However, as discussed by Covert et al.
[42], there are several methods that use different coalitional games, including global feature
attributions where the value is the model’s mean test loss [41], and local feature attributions
where the value is the model’s per-sample loss [123]. Other examples include games where
the value is the maximum flow of attention weights in transformer models [57], where the
players are analogous to samples in the training data [70], where the players are analogous
to neurons in a deep model [71], and where the players are analogous to edges in a causal
graph [202]. Although these methods are largely outside the scope of this paper, a variety of
applications of the Shapley value in machine learning are discussed in Rozemberczki et al.
[163].
Secondly, there are game-theoretic solution concepts beyond the Shapley value that can
be utilized to explain machine learning models. The first method, named asymmetric Shapley
10Note that for the conditional expectation, surrogate models are tractable once they are trained [66, 90] because they directly estimate the conditional expectation in a single model evaluation. However, using a surrogate requires training or fine-tuning an additional model.


41
values, is designed to generate feature attributions that incorporate causal information [67].
To do so, asymmetric Shapley values are based on random order values where weights are
set to zero if they are inconsistent with the underlying causal graph. Next, L-Shapley
and C-Shapley (also discussed in Section 2.5.2) are computationally efficient estimators for
Shapley value explanations designed for structured data; they are technically probabilistic
values, a generalization of semivalues [34, 137]. Similarly, Banzhaf values, an alternative
to Shapley values, are also semivalues, but each coalition is given equal weight in the sum
mation (see Equation (2.1)). Banzhaf values have been used to explain machine learning
models in a variety of settings [96, 33]. Another solution concept designed to incorporate
structural information about coalitions is the Owen value. The Owen value has been used
to design a hierarchical explanation technique named PartitionExplainer within the SHAP
package11 and as a way to accommodate groups of strongly correlated features [134]. Fi
nally, Aumann-Shapley values are an extension of Shapley values to infinite games [11], and
they are connected to an explanation method named Integrated Gradients [189]. Integrated
Gradients requires gradients of model’s prediction with respect to the features, so it cannot
be used for certain types of non-differentiable models (e.g., tree models, nearest neighbor
models), and it represents features’ absence in a continuous rather than discrete manner
that typically requires a fixed baseline (similar to baseline Shapley values).
11https://shap.readthedocs.io/en/latest/generated/shap.explainers.Partition.html


42
Factors of complexity Properties
Method Estimation strategy Removal approach Removal variant Model-agnostic Bias-free Variance-free
ApproSemivalue [25] SV None Exact Yes Yes No L-Shapley [34] SV Marginal Empirical Yes No No♣ C-Shapley [34] SV Marginal Empirical Yes No No♣ ApproShapley [25] RO None Exact Yes Yes No IME [183] RO Marginal Empirical Yes Yes No CES [188] RO Conditional Empirical Yes No No Shapley cohort refinement [128] RO Conditional Empirical* Yes No No Generative model [66] RO Conditional Generative Yes No No Surrogate model [66] RO Conditional Surrogate Yes No No Multilinear extension sampling [146] ME Marginal Empirical Yes Yes♢ No SGD-Shapley [177] WLS Baseline Exact Yes No♡ No KernelSHAP [121, 39] WLS Marginal Empirical Yes Yes♠ No Parametric KernelSHAP [4] WLS Conditional Parametric Yes No No Nonparameteric KernelSHAP [4] WLS Conditional Empirical* Yes No No FastSHAP [90] WLS Conditional Surrogate Yes No No
LinearSHAP [29] Linear Marginal Empirical No Yes Yes Correlated LinearSHAP [29] Linear Conditional Parametric No No No Interventional TreeSHAP [123] Tree Marginal Empirical No Yes Yes Path-dependent TreeSHAP [123] Tree Conditional Empirical* No No Yes DeepLIFT [175] Deep Baseline Exact No No Yes DeepSHAP [121] Deep Marginal Empirical No No Yes DASP [9] Deep Baseline Exact No No No♣ Shallow ShapNet [203] Deep Baseline Exact No Yes Yes Deep ShapNet [203] Deep Baseline Exact No No Yes
Table 2.1: Methods to estimate Shapley value explanations. We order approaches based on whether or not they are model-agnostic. Then, there are two factors of complexity. The first is the estimation strategy to handle the exponential complexity of Shapley values. For the model-agnostic approaches, the strategies include semivalue (SV), random order value (RO), multilinear extension (ME), and least squares value (LS). Note that the model-agnostic estimation strategies can generally be adapted to apply for any removal approach. For model-specific approaches, the strategies differ for linear, tree, and deep models. Then, the second factor of complexity is the feature removal approach which determines the type of Shapley value explanation (Section 2.5.1). “Any” denotes that it was introduced in game theory, and not for the sake of explaining a machine learning model. Then, we describe the specific removal variant employed by each algorithm. Baseline Shapley values are always computed exactly (Section 2.5.1), marginal Shapley values are always estimated empirically (Section 2.5.1), and conditional Shapley values have a variety of estimation procedures (Section 2.5.1). *These empirical estimates also involve defining a similarity metric. Finally, we report whether approaches are bias-free and/or variance-free. ♢Multilinear extension sampling is unbiased when sampling q uniformly. However, it is more common to use the trapezoid rule to determine q which improves convergence, but can lead to higher bias empirically at smaller numbers of subsets. ♡SGD-Shapley is consistent, but based on our empirical analysis it has high bias relative to other approaches. ♠One version of KernelSHAP has been proven to be bias-free and the original version is asymptotically unbiased [208], although empirically it also appears to be unbiased for moderate numbers of samples [39]. ♣These approaches can be deterministic with a polynomial number of model evaluations, but are often run with fewer evaluations for computational speed.


43
(a) Variants of Strategies
(b) Comparing best variants
Figure 2.5: Benchmarking unbiased, model-agnostic algorithms to estimate baseline Shapley values for a single explicand and baseline on XGB models with 100 trees. For simplicity, we calculate baseline Shapley values for all methods because we aim to evaluate the tractable estimation strategy rather than the feature removal approach. In particular, the stochastic estimators include each sampling-based approach, where multilinear, random order, random order (feature), and least squares correspond to multilinear extension sampling [146], ApproShapley [25], IME [183], and KernelSHAP [121] respectively. Multilinear (feature) is a new approach based on the multilinear extension sampling approach which explains one feature at a time. In addition, some methods are variants: either antithetic or adaptive sampling. On the x-axis we report the number of samples (subsets) used for each estimate, and on the y-axis we report the MSE relative to the true baseline Shapley value for 100 estimates with that many samples. We use three real-world datasets: diabetes (10 features, regression), NHANES (79 features, classification), blog (280 features, regression). For some variants, no error is shown for small numbers of samples; this is because each approach requires a different minimum number of samples to produce estimates for each feature. (a) Variants of the random order, feature-wise strategy. (b) Benchmarking the most competitive variant of each stochastic estimator chosen according to the lowest error for 105 samples. Note that the full blog error plot is truncated to better showcase differences.


44
Chapter 3
EXPLAINING LINEAR MODELS
3.1 Introduction
One of the most popular approaches to machine learning interpretability in recent years has
involved using Shapley values to attribute importance to features [184, 121]. The Shapley
value is a concept from coalitional game theory that fairly allocates the surplus generated by
the grand coalition in a game to each of its players [172]. In this general sense, the Shapley
value allocated to a player i is defined as:
φi = 1
|N |!
X
R
[v(SR ∪ i) − v(SR)], (3.1)
where R is one possible permutation of the order in which the players join the coalition,
SR is the set of players joining the coalition before player i, and v : S ∈ P(N ) → R1 is a
coalitional game that maps from the power set P of all players N to a scalar value.
While the Shapley value is provably the unique solution which satisfies a variety of axioms
for an abstract n-person game, figuring out how to represent a machine learning model (f )
as a coalitional game (v) is non-trivial. Previous work has suggested a variety of different
functional forms for v, for tasks like data valuation and global feature importance [70, 41].
In this work, we focus on local feature attribution – trying to understand how much each
feature contributed to the output of a model for a particular sample. For this application,
the reward of the game is typically the conditional expectation of the model’s output, where
the players in the game are the known features in the conditional expectation. There are
two ways the model’s output (f : x ∈ R|N|×1 → R1) for a particular sample is used to define


45
v(S):
1. Conditional expectation: This is the formulation in [121, 4, 67]. The coalitional game
is
v(S) = E[f (x)|S] (3.2)
where conditioning on S means considering the input X to be a random variable where the
features in S are known (E[f (X)|XS = xS]).
2. Interventional conditional (marginal) expectation: This approach is endorsed by [87,
188, 47], and in practice is used to approximate the conditional expectation in [121]. Here
the coalitional game is defined as:
v(S) = E[f (x)|do(S)] (3.3)
where we “intervene” on the features by breaking the dependence between features in S and
the remaining features. We refer to Shapley values obtained with either approach as either
conditional or marginal Shapley values.
Previous work has pointed out issues with each choice of value function. For example,
Janzing et al. [87] and Sundararajan and Najmi [188] both point out that the conditional
Shapley value can attribute importance to irrelevant features – features which were not used
by the model. While this does not violate the original Shapley axioms, it does violate a
new axiom called Dummy proposed by Sundararajan and Najmi [188], which requires that
a feature i will get attribution φi = 0, if for any two values xi and x′
i and for every value
xN\i, f (xi; xN\i) = f (x′
i; xN\i). On the other hand, papers like Frye et al. [67] have noted
that using the marginal Shapley value (which breaks the dependence between features) will
lead to evaluating the model on “impossible data points” that lie off the true data manifold.
While recent work has gone so far as to suggest that having two separate approaches
presents an irreconcilable problem with using Shapley values for feature attribution [108], in
this paper, we argue that rather than representing some critical flaw in using the Shapley
value for feature attribution, each approach is meaningful when applied in the proper context.


46
Further, we argue that this choice depends on whether you want attributions that reflect the
behavior of a particular model (true to the model), or attributions that reflect the correlations
in the data (true to the data).
3.2 Linear SHAP
In order to understand both approaches, we will focus on linear models where we present a
novel algorithm to compute the conditional Shapley values. Moving forward, f (x) = βx + b
where β ∈ R1×|N| is a row vector and b ∈ R1 a scalar.
3.2.1 Marginal expectation
For a marginal expectation, the Shapley values (which we denote as φi(f, x)) are:
φi(f, x) = βi(xi − μi) (3.4)
This was shown for independent features [4] and the marginal expectation gives the same
explanations.
3.2.2 Conditional expectation
Computing the Shapley values for an conditional expectation is substantially harder, with
a number of proposed algorithms for doing so. Sundararajan and Najmi [188] utilizes the
empirical distribution, which often assigns zero probability to plausible samples even for
large samples. Mase et al. [128] extends this empirical distribution by including a similarity
metric. In Aas et al. [4], the unknown features are sampled from either a multivariate
gaussian conditional, a gaussian copula conditional, or an empirical conditional distribution.
In Frye et al. [67], the conditional distribution is modeled using an autoencoder. For a linear
model, the problem reduces to estimating the conditional expectation of x given different


47
subsets1:
φi(f, x) = 1
|N |!
X
R
E[f (x) | xSR∪i] − E[f (x) | xSR], (3.5)
=β 1
|N |!
X
R
E[x | xSR∪i] − E[x | xSR]. (3.6)
Estimating this conditional expectation is hard in general, so we assume the inputs x ∼
N (μ, Σ) are multivariate normal. Then, denote the projection matrix that selects a set S as
PS ∈ (0, 1)|S|×|N| (therefore, PSx ∈ R|S|×1 returns the features from x in S), then E[x | xS]
is2:
Conditional expectation for xi∈S
z }| {
[PS ̄μ + PS ̄ΣP T
S (PSΣP T
S )−1(PSx − PSμ)] PS ̄
|{z}
Project to R|N|
+
Zero xi∈/S
z }| {
xP T
S PS . (3.7)
At this point, we have a natural solution to obtain the conditional expectation Shapley
value for a single sample. If we compute (Equation (3.7)) for all sets SR, we can use the
combinations definition of Shapley values (P
S∈N\i W (S, N )(E[x|xS∪i]−E[x|xS])) to compute
the Shapley value exactly.
Computational complexity: Each term in the summation requires a matrix multiplica
tion/inversion which is O(n3) complexity in the size of the matrix. Since we do this for
all possible subsets, the computational complexity to obtain φi(f, x) is O(|N |32|N|−1). To
obtain φi(f, x)∀i, re-running this algorithm would result in a complexity of O(|N |42|N|−1).
Alternatively, if we re-use terms in the summation we get a complexity of O(|N |32|N|).
Finally, to obtain φi(f, x)∀i for M samples, we have to incur this exponential cost M
1The key observation is the for a linear f (x), the expectation (and the conditional expectation) has the following property E[f (X)] = f (E[X])
2In words, the conditional expectation for a normal distributed random variable is known to be PS ̄μ +
PS ̄ΣP T
S (PS ΣP T
S )−1(PSx − PSμ); however, this gives us a vector in R|S|. Since f (x) expects an input
in R|N|, we project the conditional expectation into R|N| by multiplying by PS ̄, where S ̄ ≡ N \ S. The resulting vector has all of the features not in S set to zero. These features can simply be set to their known values since we are conditioning on them, hence the addition of xP T
S PS .


48
times for each explanation. Instead, we can isolate the exponential computation to a matrix
that does not depend on x itself. This implies that if we can incur an exponential cost
once, we can explain all samples in low order polynomial time. To do so, we can factor
(Equation (3.7)) to get:
E[x | xS] = [QS ̄ − US]μ + [QS + US]x, (3.8)
where US = PS ̄T PS ̄ΣP T
S (PSΣP T
S )−1PS and QS = P T
S PS. Then, if we use equation (Equa
tion (3.8)) to revisit (Equation (3.5)), we get:
φi(f, x) = βT (μ)μ+βT (x)x, (3.9)
where T (μ) = 1
M!
P
R([QS ̄R∪i − USR∪i] − [QS ̄R − USR ]) and T (x) = 1
M!
P
R([QSR∪i + USR∪i] −
[QSR + USR]). Here, we can see that computing T (μ) and T (x) is exponential 3, however, once
we have computed T (μ) and T (x), we can compute the Shapley value φi(f, x) quickly4.
Note that just as the original Shapley values have been approximated using Monte Carlo
sampling, we can likewise approximate T (μ) and T (x) by sampling from the permutations (or
combinations) of feature orderings. In contrast to traditional sampling approaches which
approximate the summation in Equation (3.5), approximating T (μ) and T (x) converges much
faster because we do not need to separately converge for each input feature.
3.3 Effects of Correlation
3.3.1 Impact of correlation on convergence
In order to build intuition about the conditional Shapley values for linear models, we first
examine a simulated example with a known distribution. In the following example, the
features are x ∈ R3 ∼ N (0, Σ), the model is f (x) = 1 × x1 + 2 × x2 + 3 × x3, and the sample
3In fact, the complexity to compute them for all features is O(|N |32|N|)
4In a few matrix multiplications and an addition.


49
log10(# samples) log10(# samples) log10(# samples) log10(# samples)
Attributions
a.) Independent b.) Pair correlation (0.99) c.) Off-diagonal correlation (0.1) d.) Off-diagonal correlation (0.9)
Figure 3.1: Convergence of correlated linear Shapley value estimates. x1’s attributions are blue, x2 is orange, and x3 is green. We report the standard deviations from 20 estimates of the conditional Shapley values using a fixed number of samples of combinations S.
being explained is xf = [1, 1, 1].
In Figure 3.1, there are three cases: (1) Independent implies that the correlation is the
identity matrix Σ = I, (2) Pair correlation (ρ) implies that Σ = I, except Σ2,3 = Σ3,2 = ρ,
and (3) Off-diagonal correlation (ρ) implies that Σi,j = 1 if i = j and Σi,j = ρ otherwise.
We can observe that when features are independent, the conditional Shapley value es
timates are φ(xf , f ) = [1, 2, 3] which coincides with the marginal Shapley value estimates.
Furthermore, for data that is truly independent, there is no variance in the estimates and
they converge immediately. For other correlation patterns, we can observe two trends: 1.)
correlation splits the β as credit between correlated variables and 2.) higher levels of corre
lation leads to slower convergence of the conditional Shapley value estimates.
3.3.2 Explaining a feature not used by the model
In order to compare marginal/conditional Shapley values on real data we utilize data from
the National Health and Nutrition Examination Survey (NHANES). In particular, we focus
on the task of predicting 5-year mortality within individuals (n=25,535) from 1999-2014,
where mortality status is collected in 20155. Note that conditional Shapley values require
the covariance and mean of the underlying distribution, for which we use the sampling
5We filter out individuals with unknown mortality status.


50
Low Feature Value High
Age
Income Ratio Waist Circ. Systolic BP BMI
a.)
Age
Income Ratio
Waist Circ.
Systolic BP
b.) Age
Income Ratio
Waist Circ. Systolic BP
BMI
SHAP value (impact on model output)
c.)
SHAP value (impact on model output) SHAP value (impact on model output)
Figure 3.2: Marginal vs. conditional Shapley values for NHANES. In a.), we use the marginal approach whereas in b.) and c.), we use the conditional approach with different sets of features. In these summary plots, each point is an individual where the x-axis is the Shapley value, or the impact on the model’s output). The color is the value of the feature listed in the y-axis.
covariance and mean.
For Figure 3.2, we use five features: Age, Income ratio, Systolic blood pressure, Waist
circumference, and Body mass index (BMI). In particular, we train a linear model on the first
four features (excluding BMI (test AUC: 0.772, test AP: 0.186). Then, the marginal Shapley
values give credit depending on the corresponding coefficient. In particular, Age positively
impacts mortality prediction whereas Income ratio, Waist circumference, and Systolic blood
pressure all negatively impact mortality prediction. Finally, BMI has no importance because
it is not used in the model.
However, for the conditional Shapley values, we first observe that the number of features
used to explain the model impact the attributions (four features in Figure 3.2b and five
features in Figure 3.2c). In Figure 3.2b, we see that the relationships are similar, but slightly
different to the marginal Shapley values in Figure 3.2a due to correlation in the data. In
Figure 3.2c, we can see that when we include BMI, the importance of the other features
is relatively lower. This implies that even though BMI is not included in the model, the
correlation between BMI and other features makes BMI important under conditional Shapley
values. Being able to explain features not in the model has implications for detecting bias.
For instance, a linear model may use correlations between features to implicitly depend on a
sensitive feature that was explicitly excluded. Conditional Shapley values provide a tool to
identify such bias (though we note that in this case there are other approaches to identify


51
surrogate variables such as correlation analysis).
3.4 True to the Model or True to the Data
We now consider two examples using real world datasets and use cases that demonstrate
why neither the conditional nor the marginal expectation are the right choice in general, but
can be chosen based on the desired application. We use these applications to argue that the
choice of conditional expectation comes down to whether you want your attributions to be
true to the model or true to the data.
3.4.1 True to the Model
Figure 3.3: Modifying features according to the marginal Shapley values helps applicants decrease their predicted log odds of default much more than conditional Shapley values. Solid line indicates mean change in log odds, while shaded region indicates standard deviation over all applicants. The wide range is expected as applicants who are very close to the mean or with very low odds of default to begin with will not be able to further decrease their odds of default by setting features to the mean.
We first consider the case of a bank that uses an algorithm to determine whether or not to
grant loans to applicants. Applicants who have been denied loans may want an explanation
for why they were denied, and to understand what they would have to change to receive a


52
loan [19]. In this case, the mechanism we want to explain is the particular model the bank
uses. In that case, we argue that we want our feature attributions to be true to the model.
Therefore, we hypothesize that the marginal expectation is preferable, as it is the choice of
value function that satisfies the Dummy axiom – only features that are referenced by the
model will be given importance.
To investigate this, we downloaded the LendingClub dataset6, a large dataset of loans
issued from a peer-to-peer lending site which includes loan status and latest payment infor
mation, as well as a variety of features describing the applicants such as number of open
bank accounts, age, and amount requested. We trained a logistic regression model to predict
whether or not an applicant would default on their loan. We obtained feature attributions
using either the conditional or marginal expectation.
To see which set of explanations was more useful to hypothetical applicants, we wanted to
see which set of explanations helped applicants most decrease their risk of default according
to the model (and consequently most increase their likelihood of being granted a loan). We
therefore ranked all of the features for each applicant by their Shapley value, and allowed
each applicant to “modify their risk” by setting that feature to the mean. We then measured
the change in the model’s predicted log odds of default after each feature (up to 10 features)
had been mean-imputed. For this metric, the better the explanation, the faster the predicted
log odds of default will decrease.
We find that using the marginal expectation leads to significantly better results than
the conditional expectation (Figure 3.3). Intervening on the features ranked by the marginal
Shapley values lead to a far greater decrease in predicted likelihood of default. In other words,
the marginal Shapley values enabled interventions on individuals’ features that drastically
changed their predicted likelihood of receiving a loan.
When we consider the axioms fulfilled by each choice of value function, this result makes
sense. As pointed out in Janzing et al. [87] and Sundararajan and Najmi [188], and as shown
6https://www.kaggle.com/wendykan/lending-club-loan-data


53
in subsection 3.3.2, the conditional Shapley value spreads importance among correlated fea
tures that may not be explicitly used by the machine learning model. Intervening on such
features will not impact the model’s output. In contrast, the marginal Shapley value is
true to the model in the sense that it gives importance to features explicitly used by the
model. For a linear model, this means the marginal approach will first change a feature i
where |βi(xi − μi)| is largest. Compared to other features, mean imputing xi will provide the
greatest change to the predicted output of a linear model. Being true to the model is the
best choice for most applications of explainable AI, where the goal is to explain the model
itself.
Figure 3.4: Left: When explaining a sparse model (Lasso regression), more true features are recovered when using the conditional Shapley value to spread credit among correlated features than using the marginal Shapley value. Right: When using the marginal Shapley value, we recover more true features when the underlying model spreads credit among groups of correlated features (Elastic Net) than when the underlying model is sparse (Lasso).
3.4.2 True to the Data
We now consider the complementary case where we care less about the particular machine
learning model we have trained, and more about a natural mechanism in the world. We use a


54
dataset of RNA-seq gene expression measurements in patients with acute myeloid leukemia,
a blood cancer [195]. An important problem in cancer biology is to determine which genes’
expression determine a particular outcome (e.g., response to anti-cancer drugs). One common
approach is to measure gene expression in a set of patient samples, measure response to drugs
in vitro, then use machine learning to model the data and examine the weights of the model
[217, 113, 86].
To create an experimental setting where we have access to the ground truth, we take
the real RNA-seq data and simulate a drug response label as a function of 40 randomly
selected causal genes (out of 1000 total genes). The label is defined to be the sum of the
causal genes plus gaussian noise. After training a Lasso model, we explain the model for
many samples using the conditional and marginal Shapley values and rank the genes by their
average magnitude Shapley value to get two sets of global feature importance values [123].
We see that ranking the features according to the conditional Shapley values recovers more of
the true causal features at each position in the ranked list than the marginal Shapley values
(Figure 3.4, left). The green line in the figure represents the expected number of true genes
that would be cumulatively found at each position in the ranked list if the gene list were
sorted randomly. While we see that both Shapley value-based rankings outperform random
rankings, the conditional approach outperforms the marginal one.
This example helps to illustrate why the Dummy axiom, while important for model
explanation, is not necessarily a useful axiom in general. In the case of biological discovery,
we do not care about the particular linear model we have trained on the data. We instead
care about the true data generating process, which may be equally well-represented by a
wide variety of models [24]. Therefore, when ranking genes for further testing, we want
to spread credit among correlated features that are all informative about the outcome of
interest, rather than assigning no credit to features that are not explicitly used by a single
model.
While the conditional Shapley value may be preferable to the marginal Shapley value
when explaining a Lasso model, Elastic Net (i.e., a penalty on L1 and L2 norm of the


55
coefficients) is actually more popular for this application [217]. While a Lasso model may
achieve high predictive performance, it will attempt to sparsely pick out features from among
groups of correlated features. We re-run the same experiment, but rather than comparing
the conditional and marginal Shapley values applied to a Lasso model, we focus on the
marginal Shapley values for (1) a Lasso model (as in the previous experiment), or (2) an
Elastic Net model (Figure 3.4, right). We find that by using the Elastic Net regularization
penalty, the model itself is able to spread credit among correlated features, better respecting
the correlation in the data. It is worthwhile to point out here that Elastic Net models became
popular for this task because it is typical practice to interpret linear models by examining
the coefficient vector, which is itself an marginal style explanation (partial derivative). Since
this marginal explanation does not spread credit among correlated features, it is necessary
to spread the credit using modeling decisions.
We have seen that when the goal is to be true to the data, there are two methods for
spreading credit to correlated features. One is to spread credit using the conditional Shapley
value as a feature attribution. The other is to train a model that itself spreads credit among
correlated features, in this case by training an Elastic Net regression. When we factor in
the computation time for these two approaches, the choice becomes clear. Estimating the
transform matrices for the conditional with 1000 samples took 6 hours using the CPUs on a
2018 MacBook Pro, while hyperparameter tuning and fitting an Elastic Net regression took
a matter of seconds.
3.5 Discussion
In this paper, we analyzed two approaches to explain models using the Shapley value solution
concept for cooperative games. In order to compare these approaches we focus on explaining
linear models and present a novel methodology for explaining linear models with correlated
features. We analyze two different settings where either the marginal Shapley values or the
conditional Shapley values are preferable. In the first setting, we consider a model trained
on loans data that might be used to determine which applicants obtain loans. Because


56
applicants in this setting are ultimately interested in why the model makes a prediction, we
call this case ”true to the model” and show that marginal Shapley values serve to modify
the model’s prediction more effectively. In the second setting we consider a model trained
on biological data that aims to understand an underlying causal relationship. Because this
setting is focused on scientific discovery, we call this case ”true to the data” and show that
for a sparse model (Lasso regularized) conditional Shapley values discover more of the true
features. We also find that modeling decisions can achieve some of the same effects, by
demonstrating that the marginal Shapley values recover more of the true features when
applied to a model that itself spreads credit among correlated features than when applied to
a sparse model.
Limitations and future directions: In the RNA-seq experiment we identified two solu
tions to identify true features: (1) Lasso regression with conditional Shapley values where
correlation is spread through the attribution method and (2) Elastic Net regression with
marginal Shapley values where correlation is spread through the model estimation. While
both approaches achieved similar efficacy, we found that the latter was far more computa
tionally tractable. As future work, we aim to further analyze which of these approaches are
preferable or even feasible for scientific discovery beyond linear models.
Currently, the best case for feature attribution is when the features that being perturbed
are independent to start with. In that case, both the marginal and conditional approaches
yield the same attributions. Therefore, future work that focuses on reparameterizing the
model to get at the underlying independent factors is a promising approach to eliminate the
true to the model vs. true to the data tradeoff, where we can perturb the data interventionally
without generating unrealistic input values.


57
Chapter 4
EXPLAINING TREE MODELS
4.1 Introduction
Machine learning models based on trees are the most popular non-linear models in use today
[94, 65]. Random forests, gradient boosted trees, and other tree-based models are used
in finance, medicine, biology, customer retention, advertising, supply chain management,
manufacturing, public health, and other areas to make predictions based on sets of input
features (Figure 4.1A left). For these applications, models often must be both accurate and
interpretable, where interpretability means that we can understand how the model uses input
features to make predictions [121]. However, despite the rich history of global interpretation
methods for trees, which summarize the impact of input features on the model as a whole,
much less attention has been paid to local explanations, which reveal the impact of input
features on individual predictions (i.e., for a single sample) (Figure 4.1A right).
Current local explanation methods include: 1) reporting the decision path, 2) using a
heuristic approach that assigns credit to each input feature [167], and 3) applying various
model-agnostic approaches that require repeatedly executing the model for each explanation
[160, 47, 184, 121, 13]. Each current method has limitations. First, simply reporting a
prediction’s decision path is unhelpful for most models, particularly those based on multiple
trees. Second, the behavior of the heuristic credit allocation has yet to be carefully analyzed;
we show here that it is strongly biased to alter the impact of features based on their tree
depth. Third, since model-agnostic methods rely on post hoc modeling of an arbitrary
function, they can be slow and suffer from sampling variability.
We present TreeExplainer, an explanation method for trees that enables the tractable
computation of optimal local explanations, as defined by desirable properties from game


58
theory. TreeExplainer bridges theory to practice by building on previous model-agnostic
work based on classic game-theoretic Shapley values [121, 172, 184, 47, 188, 87]. It makes
three notable improvements.
1. Exact computation of Shapley value explanations for tree-based models. Classic Shapley
values can be considered “optimal” since, within a large class of approaches, they are
the only way to measure feature importance while maintaining several natural prop
erties from cooperative game theory [121, 87]. Unfortunately, in general these values
can only be approximated since computing them exactly is NP-hard [130], requiring
a summation over all feature subsets. Sampling-based approximations have been pro
posed [184, 121, 47]; however, using them to compute low variance versions of the
results in this paper for even our smallest dataset would consume years of CPU time
(particularly for interaction effects). By focusing specifically on trees, we developed an
algorithm that computes local explanations based on exact Shapley values in polyno
mial time. This provides local explanations with theoretical guarantees of local accuracy
and consistency [121] (Methods).
2. Extending local explanations to directly capture feature interactions. Local explanations
that assign a single number to each input feature, while very intuitive, cannot directly
represent interaction effects. We provide a theoretically grounded way to measure local
interaction effects based on a generalization of Shapley values proposed in game theory
literature [68]. We show that this approach provides valuable insights into a model’s
behavior.
3. Tools for interpreting global model structure based on many local explanations. The
ability to efficiently and exactly compute local explanations using Shapley values across
an entire dataset enables the development of a range of tools to interpret a model’s
global behavior (Figure 4.1B). We show that combining many local explanations lets us
represent global structure while retaining local faithfulness [161] to the original model,


59
which produces detailed and accurate representations of model behavior.
Explaining predictions from tree models is particularly important in medical applications,
where the patterns a model uncovers can be more important than the model’s prediction
performance [174, 122]. To demonstrate TreeExplainer’s value, we use three medical datasets,
which represent three types of loss functions: 1) Mortality, a dataset with 14,407 individuals
and 79 features based on the NHANES I Epidemiologic Followup Study [45], where we model
the risk of death over twenty years of followup. 2) Chronic kidney disease, a dataset that
follows 3,939 chronic kidney disease patients from the Chronic Renal Insufficiency Cohort
study over 10,745 visits, where we use 333 features to classify whether patients will progress to
end-stage renal disease within 4 years. 3) Hospital procedure duration, an electronic medical
record dataset with 147,000 procedures and 2,185 features, where we predict duration of a
patient’s hospital stay for an upcoming procedure.
In this paper, we discuss how the accuracy and interpretability of tree-based models
make them appropriate for many applications. We then describe why these models need
more precise local explanations and how we address that need with TreeExplainer. Next,
we extend local explanations to capture interaction effects. Finally, we demonstrate the
value of explainable AI tools that combine many local explanations from TreeExplainer
(https://github.com/suinleelab/treeexplainer-study).
4.2 Advantages of tree-based models
Tree-based models can be more accurate than neural networks in many applications. While
deep learning models are more appropriate in fields like image recognition, speech recognition,
and natural language processing, tree-based models consistently outperform standard deep
models on tabular-style datasets, where features are individually meaningful and lack strong
multi-scale temporal or spatial structures [35]. The three medical datasets we examine here
all represent tabular-style data. Gradient boosted trees outperform both pure deep learning
and linear regression across all three datasets (Figure 4.3A).


60
Tree-based models can also be more interpretable than linear models due to model
mismatch effects. It is well-known that the bias/variance trade-off in machine learning has
implications for model accuracy. Less well appreciated is that this trade-off also affects
interpretability. Simple high-bias models (such as linear models) seem easy to understand,
but they are sensitive to model mismatch, i.e., where the model’s form does not match its true
relationships in the data [78]. This mismatch can create hard-to-interpret model artifacts.
To illustrate why low-bias models can be more interpretable than high-bias ones, we
compare gradient boosted trees to linear logistic regression using the mortality dataset. We
simulate a binary outcome based on a participant’s age and body mass index (BMI), and we
vary the amount of non-linearity in the simulated relationship (Figure 4.3B). As expected, by
increasing non-linearity, the bias of the linear model causes accuracy to decline (Figure 4.3C).
Perhaps unexpectedly, it also causes interpretability to decline (Figure 4.3D). We know that
the model should depend only on age and BMI, but even a moderate amount of non-linearity
in the true relationship causes the linear model to begin using other irrelevant features
(Figure 4.3D), and the weight placed on these features is driven by complex cancellation
effects that are not readily interpretable. When a linear model depends on cancellation
effects between irrelevant features, the function itself is not complicated, but the meaning of
the features it depends on become subtle: they are no longer being used primarily for their
marginal effects, but rather for their interaction effects. Thus, even when simpler high-bias
models achieve high accuracy, low-bias ones may be preferable, and even more interpretable,
since they are likely to better represent the true data-generating mechanism and depend
more naturally on their input features.
4.3 Tree SHAP
As previously mentioned, Shapley values have been adapted to explain machine learning
algorithms; however, calculating them is NP-hard. By restricting our machine learning
model class to trees, we create tractable algorithms to exactly estimate marginal Shapley
values and approximately estimate conditional Shapley values.


61
In this thesis, we will focus on describing the algorithm to estimate marginal Shapley
values, because it is exact1. As previously mentioned, marginal Shapley values are equivalent
to the average of baseline Shapley values with many baselines. Moving forward, our goal is
to design a tractable algorithm to compute baseline Shapley values.
The brute force approach to estimate baseline Shapley values for a single feature (φi(f, xe, xb))
involves making predictions to compute marginal contributions for each possible subset. If
we assume the computational cost of computing the weight is constant2, then the complexity
of the brute force method is the number of terms in the summation multiplied by the cost of
making a prediction (on the order of the depth of the tree). This gives O((tree depth) × 2d),
where d is the number of features (analogous to the number of players |D|).
Then, in order to compute φi(f, xe, xb) for all features i, we have to re-run the entire
algorithm d times, giving us an overall complexity of
O(d × (tree depth) × 2d) (4.1)
An exponential computational complexity is bad; however, if we constrain f (x) to be a
tree-based model (e.g., XGBoost, decision trees, random forests, etc.), then we can come up
with a polynomial time algorithm to compute φi(f, xe, xb) exactly. Why is this the case?
Well, even for explaining a single feature, the brute force algorithm may consider a particular
path multiple times. However, to compute the Baseline Shapley value for a single feature, it
turns out that we only need to consider each path once (Theorem 1).
Theorem 1. To calculate φi(f, xf , xb), we can calculate attributions for each path from the
root to each leaf. For a given path P , we define NP to be the unique features in the path and
SP to be the unique features in the path that came from xf . Finally, define v to be the value
1This algorithm is my (Hugh Chen) contribution to the paper.
2Which it will be if we memoize k! for k = 1, · · · , |D|.


62
of the path’s leaf. Then, the attribution of the path is:
φP
i (f, xf , xb) =

     
     
0 if i ∈/ NP
W (|SP | − 1, |NP |) × v if i ∈ SP
−W (|SP |, |NP |) × v o.w.
(4.2)
Proof. (Sketch) Treat each path P in the tree from the root to each leaf as a separate model
f P (x) that returns the value of the leaf if that path is traversed by x or zero otherwise. This
results in (# leaf nodes) models that operate on disjoint parts of the input space, implying
that our original model is equal to the summation of all of these path models:
f (x) =
X
P
f P (x) (4.3)
By the additivity of Shapley values,
φi(f, xf , xb) =
X
P
φi(f P , xf , xb) (4.4)
Then, we can simply calculate φi for each path model. Since the path model is zero
everywhere except for the associated path, we arrive to the solution in Theorem 1.
Then, we can design an algorithm which keeps track of features for each path as we
perform a traversal of the tree model. The resultant algorithm checks against a constantly
updating list of current features and calculates the marginal Shapley values with the following
computational complexity:
O((# internal nodes) × (tree depth)) + O((# leaf nodes) × (tree depth) × d)
We can first get rid of the multiplicative (tree depth) factor by using binary arrays to
indicate the presence of features in a given path instead of lists (data structures). Next,


63
we can get rid of the final multiplicative (d) factor by computing the attributions for all
features simultaneously as we traverse the tree by passing “Pos” and “Neg” attributions to
parent nodes (dynamic programming). We can first observe that for each leaf, according to
Theorem 1, there are only two possible values needed to compute the Baseline Shapley value
(Pos and Neg). Then, in Figure 4.2, based on the attributions for x1 we see that these Pos
and Neg terms can be grouped by the left and right subtrees below x1. To generalize, we
make the following observation:
Observation In order to compute the attribution for any feature i it is sufficient to
consider the paths that correspond to each internal node’s children. Then, we can focus
on internal n, where we know that one child is associated with xf child and one child is
associated with xb. The attribution to the node’s feature is:
X
paths P under xf child
PosP +
X
paths P under xb child
NegP (4.5)
The final algorithm, which has a computational complexity of O(#nodes) is:
def dynamic(array xf, array xb, tree T):
phi = [0]∗len(xf)
def recurse(node n, int nc, int sc, array fseen , array bseen):
# Case 1: Leaf
if n.is leaf:
if sc == 0: return((0,0))
else: return ((n.value∗W(sc ,nc−1),−n.value∗W(sc ,nc )))
# Find children associated with xf and xb
xf child = n.left if xf[n.feat] < n.thres else n.right
xb child = n.left if xb[n.feat] < n.thres else n.right
# Case 2: Feature encountered before
if fseen[n.feat] > 0:
return(recurse(xf child ,nc,sc,fseen ,bseen))


64
if bseen[n.feat] > 0:
return(recurse(xb child ,nc,sc,fseen ,bseen))
# Case 3: xf and xb go the same way
if xf child == xb child:
return(recurse(xb child ,nc,sc,fseen ,bseen))
# Case 4: xf and xb don’t go the same way
if xf child != xb child:
fseen[n.feat] += 1
posf,negf = recurse(xf child ,nc+1,sc+1,fseen,bseen)
fseen[n.feat] −= 1; bseen[n.feat] += 1
posb,negb = recurse(xb child ,nc+1,sc ,fseen,bseen)
bseen[n.feat] −= 1
phi[n.feat] += posf+negb
return (( posf +posb , negf + negb ))
recurse(n=T.root ,0,0,[0]∗len(xf),[0]∗len(xf))
4.4 Local explanations for trees
Current local explanations for tree-based models are inconsistent. To our knowledge, only two
tree-specific approaches can quantify a feature’s local importance for an individual prediction.
The first is simply reporting the decision path, which is unhelpful for ensembles of many trees.
The second is an unpublished heuristic approach (proposed by Saabas [167]), which explains
a prediction by following the decision path and attributing changes in the model’s expected
output to each feature along the path. The Saabas method has not been well studied, and
we demonstrate here it is biased to alter the impact of features based on their distance
from a tree’s root. This bias makes Saabas values inconsistent, where increasing a model’s
dependence on a feature may actually decrease that feature’s Saabas value . This is the
opposite of what an effective attribution method should do. We show this difference by


65
examining trees representing multi-way AND functions, for which no feature should have
more credit than another. Yet Saabas values give splits near the root much less credit
than those near the leaves. Consistency is critical for an explanation method since it makes
comparisons among feature importance values meaningful.
Model-agnostic local explanation approaches are slow and variable. While model-agnostic
local explanation approaches can explain tree models, they rely on post hoc modeling of
an arbitrary function and thus can be slow and/or suffer from sampling variability when
applied to models with many input features. To illustrate this, we generate random datasets
of increasing size and then explain (over)fit XGBoost models with 1,000 trees. This runtime
of this experiment shows a linear increase in complexity as the number of features increases;
model-agnostic methods take a significant amount of time to run over these datasets, even
though we allowed for non-trivial estimate variability and used only a moderate number
of features. While often practical for individual explanations, model-agnostic methods can
quickly become impractical for explaining entire datasets.
TreeExplainer provides fast local explanations with guaranteed consistency. It bridges
theory to practice by reducing the complexity of exact Shapley value computation from
exponential to polynomial time. This is important since within the class of additive feature
attribution methods, a class that we have shown contains many previous approaches to local
feature attribution [121], results from game theory imply the Shapley values are the only way
to satisfy three important properties: local accuracy, consistency, and missingness. Local
accuracy (known as efficiency in game theory) states that when approximating the original
model f for a specific input x, the explanation’s attribution values should sum up to the
output f (x). Consistency (known as monotonicity in game theory) states that if a model
changes so that some feature’s contribution increases or stays the same regardless of the other
inputs, that input’s attribution should not decrease. Missingness (null effects and symmetry
in game theory), is a trivial property satisfied by all previous explanation methods.
TreeExplainer enables the exact computation of Shapley values in low order polynomial
time by leveraging the internal structure of tree-based models. Shapley values require a sum


66
mation of terms over all possible feature subsets, TreeExplainer collapses this summation into
a set of calculations specific to each leaf in a tree (Methods). This represents an exponential
complexity improvement over previous exact Shapley methods. To compute the impact of
a specific feature subset during the Shapley value calculation, TreeExplainer uses marginal
expectations over a user-supplied background dataset [87]. But it can also avoid the need for
a user-supplied background dataset by relying only on the path coverage information stored
in the model (which is usually from the training dataset).
Efficiently and exactly computing the Shapley values guarantees that explanations are
always consistent and locally accurate, improving results over previous local explanation
methods in several ways:
• Impartial feature credit assignment regardless of tree depth. In contrast to Saabas
values, Shapley values allocate credit uniformly among all features participating in
multi-way AND operations and thereby avoid inconsistency problems.
• No estimation variability. Since solutions from model-agnostic sampling methods are
approximate, TreeExplainer’s exact explanations eliminate the additional burden of
checking their convergence and accepting a certain amount of noise in the estimates
(other than noise from the choice of a background dataset).
• Strong benchmark performance (Figure 4.4). We designed 15 metrics to comprehen
sively evaluate the performance of local explanation methods; we applied these met
rics to ten different explanation methods across three different model types and three
datasets. Results for the chronic kidney disease dataset, shown in Figure 4.4, demon
strate consistent performance improvements for TreeExplainer.
• Consistency with human intuition. We evaluated how well explanation methods match
human intuition by comparing their outputs with human consensus explanations of 12
scenarios based on simple models. Unlike the heuristic Saabas values, Shapley-value
based explanation methods agree with human intuition in all tested scenarios.


67
TreeExplainer also extends local explanations to measure interaction effects. Tradition
ally, local explanations based on feature attribution assign a single number to each input
feature. The simplicity of this natural representation comes at the cost of conflating main
and interaction effects. While interaction effects between features can be reflected in the
global patterns of many local explanations, their distinction from main effects is lost in each
local explanation (Figure 4.5B-G).
We propose SHAP interaction values as a richer type of local explanation. These values
use the ‘Shapley interaction index’ from game theory to capture local interaction effects.
They follow from generalizations of the original Shapley value properties [68] and allocate
credit not just among each player of a game, but among all pairs of players. The SHAP
interaction values consist of a matrix of feature attributions (interaction effects on the off
diagonal and the remaining effects on the diagonal). By enabling the separate consideration
of interaction effects for individual model predictions, TreeExplainer can uncover significant
patterns that might otherwise be missed.
4.5 Local explanations as building blocks for global insights
Previous approaches to understanding a model globally focused on using simple global ap
proximations [65], finding new interpretable features [99], or quantifying the impact of specific
internal nodes in a deep network [212, 16, 114]. We present methods that combine many
local explanations to provide global insight into a model’s behavior. This lets us retain lo
cal faithfulness to the model while still capturing global patterns, resulting in richer, more
accurate representations of the model’s behavior.
Local model summarization reveals rare high-magnitude effects on mortality risk and
increases feature selection power. Combining local explanations from TreeExplainer across
an entire dataset enhances traditional global representations of feature importance by: (1)
avoiding the inconsistency problems of current methods, (2) increasing the power to detect
true feature dependencies in a dataset, and (3) enabling us to build SHAP summary plots,
which succinctly display the magnitude, prevalence, and direction of a feature’s effect. SHAP


68
summary plots avoid conflating the magnitude and prevalence of an effect into a single
number, and so reveal rare high magnitude effects. Figure 4.5A (right) reveals the direction
of effects, such as men (blue) having a higher mortality risk than women (red); and the
distribution of effect sizes, such as the long right tails of many medical test values. These
long tails mean features with a low global importance can be extremely important for specific
individuals. Interestingly, rare mortality effects always stretch to the right, which implies
there are many ways to die abnormally early when medical measurements are out-of-range,
but not many ways to live abnormally longer.
Local feature dependence reveals both global patterns and individual variability in mor
tality risk and chronic kidney disease. SHAP dependence plots show how a feature’s value (x
axis) impacts the prediction (y-axis) of every sample (each dot) in a dataset (Figure 4.5B and E).
They provide richer information than traditional partial dependence plots. For the mortality
model, SHAP dependence plots reproduce the standard risk inflection point of systolic blood
pressure [75], while also highlighting that the impact of blood pressure on mortality risk
differs for people of different ages (Figure 4.5B). These types of interaction effects show up
as vertical dispersion in SHAP dependence plots.
For the chronic kidney disease model, a dependence plot again clearly reveals a risk
inflection point for systolic blood pressure. However, in this dataset the vertical dispersion
from interaction effects appears to be partially driven by differences in blood urea nitrogen
(Figure 4.5E). Correctly modeling blood pressure risk while retaining interpretabilty is vital
because blood pressure control in select chronic kidney disease (CKD) populations may delay
progression of kidney disease and reduce the risk of cardiovascular events.
Local interactions reveal sex-specific life expectancy changes during aging as well as
inflammation effects in chronic kidney disease. Using SHAP interaction values, we can
decompose the impact of a feature on a specific sample into interaction effects with other
features. This helps us measure global interaction strength as well as decompose SHAP
dependence plots into interaction effects at a local (i.e., per sample) level (Figure 4.5B-D).
In the mortality dataset, plotting the SHAP interaction value between age and sex shows a


69
clear change in the relative risk between men and women over a lifetime (Figure 4.5G). The
largest difference in risk between men and women occurs at age 60; the increased risk for
men could be driven by their increased cardiovascular mortality relative to women near that
age [141]. This pattern is not clearly captured without SHAP interaction values because
being male always confers greater risk of mortality than being female (Figure 4.5A).
In the chronic kidney disease model, we identify an interesting interaction (Figure 4.5F):
high white blood cell counts are more concerning to the model when they are accompanied
by high blood urea nitrogen. This supports the notion that inflammation may interact with
high blood urea nitrogen to hasten kidney function decline [21, 60].
Local model monitoring reveals previously invisible problems with deployed machine
learning models. Using TreeExplainer to explain a model’s loss, instead of a model’s predic
tion, can improve our ability to monitor deployed models. Monitoring models is challenging
because of the many ways relationships between the input and model target can change
post-deployment. Detecting when such changes occur is difficult, so many bugs in machine
learning pipelines go undetected, even in core software at top tech companies [216]. We
demonstrate that local model monitoring helps debug model deployments and identify prob
lematic features (if any) directly by decomposing the loss among the model’s input features.
We simulated a model deployment with the hospital procedure duration dataset using
the first year of data for training and the next three years for deployment. We present three
examples: one intentional error and two previously undiscovered problems. (1) We inten
tionally swapped the labels of operating rooms 6 and 13 partway through the deployment
to mimic a typical feature pipeline bug. The overall loss of the model’s prediction gives
no indication of the bug (Figure 4.6A), whereas the SHAP monitoring plot for the room 6
feature clearly identifies the labeling error (Figure 4.6B). (2) Figure 4.6C shows a spike in
error for the general anesthesia feature shortly after the deployment window begins. This
spike corresponds to a subset of procedures affected by a previously undiscovered tempo
rary electronic medical record configuration problem. (3) Figure 4.6D shows an example
of feature drift over time, not of a processing error. During the training period and early


70
in deployment, using the ‘atrial fibrillation’ feature lowers the loss; however, the feature be
comes gradually less useful over time and eventually degrades the model. We found this drift
was caused by significant changes in atrial fibrillation ablation procedure duration driven by
technology and staffing changes. Current deployment practice monitors both the overall loss
of a model (Figure 4.6A) over time and potentially statistics about input features. Instead,
TreeExplainer lets us directly monitor the impact individual features have on a model’s loss.
Local explanation embeddings reveal population subgroups relevant to mortality risk and
complementary diagnostic indicators in chronic kidney disease. Unsupervised clustering and
dimensionality reduction are widely used to discover patterns characterizing subgroups of
samples (e.g., study participants), such as disease subtypes [198, 179]. These techniques
have two drawbacks: 1) the distance metric does not account for discrepancies among unit
s/meaning of features (e.g., weight vs. age), and 2) an unsupervised approach cannot know
which features are relevant for an outcome of interest and so should be weighted more
strongly. We address both limitations using local explanation embeddings to embed each
sample into a new “explanation space.” Running clustering in this new space will yield a
supervised clustering, where samples are grouped based on their explanations. Supervised
clustering naturally accounts for the differing units of various features, highlighting only the
changes relevant to a particular outcome.
Running hierarchical supervised clustering using the mortality model results in many
groups of people that share a similar mortality risk for similar reasons (Figure 4.7A). This
grouping of samples can reveal high level structure in datasets that would not be revealed
using standard unsupervised clustering and has various applications, from customer segmen
tation, to model debugging, to disease sub-typing. Analogously, we can also run PCA on
local explanation embeddings for chronic kidney disease samples. This uncovers two primary
categories of risk factors that identify unique individuals at risk of end-stage renal disease:
(1) factors based on urine measurements, and (2) factors based on blood measurements (Fig
ure 4.7B-D). This pattern is notable because it continues as we plot more top features. The
separation between blood and urine features is consistent with the fact that clinically these


71
factors should be measured in parallel. This type of insight into the overall structure of
kidney risk is not at all apparent in a standard unsupervised embedding.
4.6 Discussion
The potential impact of local explanations for tree-based machine learning models is widespread.
Explanations can help satisfy transparency requirements, facilitate human/AI collaboration,
and aid model development, debugging, and monitoring.
Tree-based machine learning models are widely used in many regulated domains, such as
healthcare, finance, and public services. Improved interpretability is vital for these applica
tions. In healthcare, the unknowing deployment of “Clever Hans” predictors that depend
on spurious correlations could lead to serious patient harm [110, 152]. In finance, consumer
protection laws require explanations for credit decisions, and no accepted standard exists
for how to produce these for complex tree-based models [83]. In public service applications,
explainability can promote accountability and anti-discrimination policies [49].
Improving human/AI collaboration is critical for applications where explaining machine
learning model predictions can enhance human performance. Such applications include pre
dictive medicine, customer retention, and financial model supervision. Local explanations
enable support agents to predict why the customer they are calling is likely to leave. They
enable doctors to make more informed decisions rather than blindly trust an algorithm’s out
put. With financial model supervision, local explanations help human experts understand
why the model made a specific recommendation for high-risk decisions.
Improving model development, debugging, and monitoring leads to more accurate and
reliable deployments of machine learning systems. Local explanations aid model development
by revealing which features are most informative for specific subsets of samples. They aid
debugging by revealing the global patterns of how a model depends on its input features,
and so enable developers to determine when patterns are unlikely to generalize well. Finally,
they aid model monitoring by enabling the allocation of global accuracy measures among
each model input, significantly increasing the signal-to-noise ratio for detecting problematic


72
data distribution shifts.
In this paper, we identified ways to significantly enhance the interpretability of tree
based models and to broaden the application of local explanation methods. We develop the
first polynomial-time algorithm to compute Shapley values for trees. This algorithm solves
what is in general an NP-hard problem in polynomial time for an important class of value
functions. We present a richer type of local explanation that directly captures interaction
effects. We demonstrate how using local explanation methods to explain model loss enables
a more sensitive and informative method of model monitoring. We offer many tools for
model interpretation that combine local explanations, such as dependence plots, summary
plots, supervised clusterings, and explanation embeddings. We demonstrate that Shapley
based local explanations can improve upon state-of-the-art feature selection for trees. We
identify under-appreciated interpretability problems with simple linear models. And we
compile many varied explainability metrics into a unified open source benchmark, on which
TreeExplainer consistently outperforms other alternatives. Local explanations have a distinct
advantage over global ones. By focusing only on a single sample, they remain more faithful to
the original model. By designing efficient and trustworthy ways to obtain local explanations
for modern tree-based models, we take an important step toward enabling local explanations
to become foundational building blocks for an ever growing number of downstream machine
learning tasks.


73
Datasets
(mortality) (kidney) (hospital)
SHAP values
(local explanations)
# samples
# features # features
Model summarization
Feature dependence
Explanation embeddings
Model monitoring
Interaction effects
...
Combining local explanations from many samples... ...can lead to global model insights
(b)
(a)
Model
Tree Explainer
Mortality risk score = 4
Age = 65
BMI = 40
Blood pressure = 180
Sex = Female
“Black box” model prediction “White box” local explanation
Mortality risk score = 4
Age = 65
BMI = 40
Blood pressure = 180
Sex = Female -2
+3
+0.5
+2.5
Model
TreeExplainer
Figure 4.1: Local explanations based on TreeExplainer enable a wide variety of new ways to understand global model structure. (a) A local explanation based on assigning a numeric measure of credit to each input feature. (b) By combining many local explanations, we can represent global structure while retaining local faithfulness to the original model. We demonstrate this by using three medical datasets to train gradient boosted decision trees and then compute local explanations based on SHapley Additive exPlanation (SHAP) values [121]. Computing local explanations across all samples in a dataset enables development of many tools for understanding global model structure.


74
       
x1
x2
x2
φ1(f, xf , xb) Pos1 + Pos2 + Neg3 + Neg4 φ2(f, xf , xb) Neg1 + Pos2 + Pos3 + Neg4
Figure 4.2: Example to illustrate collapsibility for features. Green paths are associated with xf and red paths are xb.


75
Amount of non-linearity in the data
(b)
(d)
(c)
% of weight assigned
to irrelevant features
15%
10%
5%
0%
Explanability lost
due to bias
Log loss difference
from true model
Accuracy lost
due to bias
Low risk
High risk
NHANES I Mortality (C-statistic)
CRIC Kidney Disease (area under the PR curve)
Hospital Procedure Duration (R2 value)
Gradient Boosted Trees Linear Model Neural Network
0.821 0.813 0.816
0.890 0.871 0.872
0.674 0.595 0.629
**
**
** *
**
(a)
20%
Figure 4.3: Gradient boosted tree models can be more accurate than neural networks and more interpretable than linear models. (a) Gradient boosted tree models outperform both linear models and neural networks on all our medical datasets, where (**) represents a bootstrap retrain P-value < 0.01, and (*) represents a P-value of 0.03. (b-d) Linear models exhibit explanation and accuracy error in the presence of non-linearity. (b) The data generating models we used for the simulation ranged from linear to quadratic along the body mass index (BMI) dimension. (c) Linear logistic regression (red) outperformed gradient boosting (blue) up to a specific amount of non-linearity. Not surprisingly, the linear model’s bias is higher than the gradient boosting model’s, as shown by the steeper slope as we increase non-linearity. (d) As the true function becomes more non-linear, the linear model assigns more credit (coefficient weight) to features that were not used by the data generating model.


76
Gradient Boosted Trees
Random Forest
Decision Tree
*
*
*
TreeExplainer (path dependent) TreeExplainer (interventional)
TreeExplainer (path dependent)
TreeExplainer (interventional)
TreeExplainer (path dependent) TreeExplainer (interventional)
Figure 4.4: Explanation method performance across 15 different evaluation metrics and three classification models in the chronic kidney disease dataset. Each column represents an evaluation metric, and each row represents an explanation method. The scores for each metric are scaled between the minimum and maximum value, and methods are sorted by their average score. TreeExplainer outperforms previous approaches not only by having theoretical guarantees about consistency, but also by exhibiting improved performance across a large set of quantitative metrics that measure explanation quality (Methods). When these experiments were repeated for two synthetic datasets, TreeExplainer remained the top-performing method. Note that, as predicted, Saabas better approximates the Shapley values (and so becomes a better attribution method) as the number of trees increases (Methods). *Since MAPLE models the local gradient of a function, and not the impact of hiding a feature, it tends to perform poorly on these feature importance metrics [153].


77
Mortality model
Global feature importance Local explanation summary
(log relative risk of mortality)
(g)
Male Female
(log relative risk of mortality)
Age (years)
(e)
(log relative risk of ESRD)
Systolic blood pressure (mmHg)
SHAP value for systolic blood pressure
without the age interaction
(log relative risk of mortality)
Systolic blood pressure (mmHg)
(b)
(log relative risk of mortality)
Systolic blood pressure (mmHg)
Age (years)
(log relative risk of mortality)
Systolic blood pressure (mmHg)
Age (years)
Kidney model (f)
(log relative risk of ESRD)
White blood cells (lab value)
Mortality model
Mortality model
+
=
(F/M)
(a)
(c) (d)
Figure 4.5: By combining many local explanations, we can provide rich summaries of both an entire model and individual features. Explanations are based on a gradient boosted decision tree model trained on the mortality dataset. (a) (left) bar-chart of the average SHAP value magnitude, and (right) a set of beeswarm plots, where each dot corresponds to an individual person in the study. The dot’s position on the x-axis shows the impact that feature has on the model’s prediction for that person. When multiple dots land at the same x position, they pile up to show density. (b) SHAP dependence plot of systolic blood pressure vs. its SHAP value in the mortality model. A clear interaction effect with age is visible, which increases the impact of early onset high blood pressure. (c) Using SHAP interaction values to remove the interaction effect of age from the model. (d) Plot of just the interaction effect of systolic blood pressure with age; shows how the effect of systolic blood pressure on mortality risk varies with age. Adding the y-values of C and D produces B. (e) A dependence plot of systolic blood pressure vs. its SHAP value in the kidney model; shows an increase in kidney disease risk at a systolic blood pressure of 125 (which parallels the increase in mortality risk). (f) Plot of the SHAP interaction value of ‘white blood cells’ with ‘blood urea nitrogen’; shows that high white blood cell counts increase the negative risk conferred by high blood urea nitrogen. (g) Plot of the SHAP interaction value of sex vs. age in the mortality model; shows how the differential risk for men and women changes over a lifetime.


78
Sample index (ordered by time)
SHAP loss value of
atrial fibrillation
SHAP loss value of
general anesthesia
SHAP loss value of
in room #6
Smoothed
model squared error
First year of data used for training

(a)
(b)
(c)
(d)
True False
Feature values
2012 2013 2014 2015 2016
Figure 4.6: Monitoring plots reveal problems that would otherwise be invisible in a retrospective hospital machine learning model deployment. (a) The squared error of a hospital duration model averaged over the nearest 1,000 samples. The increase in error after training occurs because the test error is (as expected) higher than the training error. (b) The SHAP value of the model loss for the feature that indicates whether the procedure happens in room 6. A significant change occurs when we intentionally swap the labels of rooms 6 and 13, which is invisible in the overall model loss. (c) The SHAP value of the model loss for the general anesthesia feature; the spike one-third of the way into the data results from previously unrecognized transient data corruption at a hospital. (d) The SHAP value of the model loss for the atrial fibrillation feature. The plot’s upward trend shows feature drift over time (P-value 5.4 × 10−19).


79
Liver/boneproblems25-40yearsold
Underweightwomen25-35yearsold
Menwithliver/boneproblems40-50yearsold
Early-onsethighBP
HighBP40-50yearsold
HighBP50-60yearsold
HighBP+inflammation65-75yearsold
Underweight65-75yearsold
Total log hazard ratio Age Sex Systolic blood pressure White blood cells BMI Sedimentation rate Serum albumin Alkaline phosphatase Cholesterol Physical activity
Inflammation25-35yearsold
High
Low
Feature SHAP values
(normalized per row)
SHAP value of blood creatinine (determines eGFR)
= + + 331
other features
Log odds of end-stage renal disease within 4 years SHAP value of urine protein/creatinine ratio
(B) (C) (D)
(A)
Figure 4.7: Local explanation embeddings support both supervised clustering and interpretable dimensionality reduction. (A) A clustering of mortality study individuals by their local explanation embedding. Columns are patients, and rows are features’ normalized SHAP values. Sorting by a hierarchical clustering reveals population subgroups that have distinct mortality risk factors. (B-D) A local explanation embedding of kidney study visits projected onto two principal components. Local feature attribution values can be viewed as an embedding of the samples into a space where each dimension corresponds to a feature and all axes have the units of the model’s output. The embedding colored by: (B) the predicted log odds of a participant developing end-stage renal disease within 4 years of that visit, (C) the SHAP value of blood creatinine, and (D) the SHAP value of the urine protein/creatinine ratio. Many other features also align with these top two principal components, and an equivalent unsupervised PCA embedding is far less interpretable.


80
Chapter 5
SELF-SUPERVISED LEARNING
5.1 Introduction
Globally, the number of surgical operations performed each year exceeds 300 million [205].
Although surgeries are crucial components of medical care, they have a higher prevalence
of adverse events (i.e., patients harmed as a result of their medical treatment) relative to
other medical specialties. In fact, several international studies have shown rates of adverse
events ranging from 3% to 22% in surgical patients [143, 215, 93]. Fortunately, these studies
also conclude that the majority of adverse events are preventable, indicating a tremendous
opportunity for improvement by predictive models.
The accuracy of such models is largely dependent on the availability of training data. As
of 2014, a large portion (> 40%) of invasive, therapeutic surgeries take place in hospitals
with either medium or small numbers of beds [181, 207]. These smaller institutions may lack
either sufficient data or computational resources to train accurate models. Furthermore, pa
tient privacy considerations mean that large public EHR datasets are unlikely, leaving many
institutions with insufficient resources to train performant models on their own. In the face
of this insufficiency, one natural way to make accurate predictions is transfer learning, which
has already shown success in medical images as well as clinical text [190, 157, 124]. Par
ticularly with the popularization of wearable sensors for health monitoring [125], transfer
learning techniques that train models in one dataset and use them in another are arguably
underexplored for physiological signals, which account for a significant portion of the hun
dreds of petabytes of currently available worldwide health data [162, 147]. One promising
avenue of transfer learning research is deep embedding models which learn to extract gen
eralizable features from images or time series data [36, 127] which improve over traditional


81
domain-specific hand engineered features.
Our approach, PHASE (PHysiologicAl Signal Embeddings), trains deep embedding mod
els on physiological signals to better forecast and facilitate prevention of potentially millions
of adverse surgical outcomes. Furthermore, these models not only improve predictive accu
racy but can be transferred from an institution with plentiful computational resources to
institutions with less. PHASE improves over previous approaches in two important ways:
• PHASE improves predictive accuracy by leveraging deep learning to embed physio
logical signals. Using long-short term memory networks (LSTMs), PHASE embeds
physiological signals prior to forecasting adverse events with a downstream model. We
investigate a number of self-supervised approaches (training with inputs and outputs
derived from the signal data itself) [105] to effectively train embedding models. Our re
sults show that gradient boosted tree (GBT) models trained with features extracted by
self-supervised LSTMs improves accuracy over conventional approaches for forecasting
surgical outcomes that rely on a single model (i.e., predicting adverse outcomes with
an LSTM with raw features or a GBT with raw or hand engineered features).
• PHASE shares models rather than data to address data insufficiency and improves
over alternative methods including GBTs trained with raw features, hand engineered
features, and embeddings jointly learned by a single LSTM. Data insufficiency is espe
cially important for surgical data because protecting patient privacy makes it difficult
to share large amounts of medical data which exacerbates the lack of publicly available
data [104]. By transferring performant models as has been done in medical images and
clinical text [190, 157, 124], scientists can collaborate to improve accuracy of predictive
models without exposing patient data.
In contrast to prior research on transfer learning for physiological signals that focus on a
single medical center’s electroencephalograms (EEGs) [58] or intensive care unit (ICU) stays
[77], we evaluate transfer learning across three distinct medical center data sets (two from


82
Share
Upstream embedding model
Improves prediction
Source hospital
Self-supervised learning
Next five minutes
Previous sixty minutes
Signals are drawn in three ways
PHASE
Model
Forecasting adverse events
PHASE
PHASE
Train
Keep private
PHASE
Target hospital
Reduces cost
PHASE Output: Embeddings
Static Variables (e.g., height)
Dynamic Variables (e.g., blood oxygen)
PHASE Input: Raw Signal Data
(c)
Upstream embedding model
Downstream prediction model
Standard embedding
(b)
(a) PHASE pipeline
PHASE pipeline
Transferred embedding
Fine-tuned embedding
Figure 5.1: (a) PHASE learns models that embed (i.e., extract features from) physiological signals. We concatenate these embeddings with static data to predict adverse events. We describe the model extracting features as an upstream embedding model and the model making the final prediction as the downstream prediction model. (b) PHASE enables researchers at different hospitals to work together without sharing data. Researchers can perform transfer learning where upstream embedding models are trained on data drawn from a source hospital and used to embed signals and make a downstream prediction in data drawn from a target hospital. We show that this approach outperforms conventional deep learning and tree models trained with raw or hand engineered features. In addition, this approach reduces computational cost for users in target hospitals. (c) PHASE comprises LSTM embedding models trained per physiological signal that predict the future of the signal based on the past (self-supervised learning). We train self-supervised embedding models using data drawn in three distinct ways: (1) from the target hospital (standard embedding), (2) from a distinct source hospital (transferred embedding), and (3) from a distinct source hospital and then the target hospital (fine-tuned embedding) (More details in Section 5.2.2).


83
operating rooms and one from an ICU). Furthermore, we focus on evaluating self-supervised
approaches (Figure 5.1) to train embedding models that we validate with feature attributions.
To achieve this, we use data collected by the Anesthesia Information Management System
(AIMS) from two medical centers as well as the Medical Information Mart for Intensive
Care (MIMIC-III) dataset [91]. We utilize fifteen physiological signal variables and six static
variable inputs (variables listed in Section 5.2.1) to forecast five possible outcomes: hypox
emia, hypocapnia, hypotension, hypertension, and phenylephrine administration. We show
in a standard embedding setting, PHASE outperforms a number of conventional approaches
across five outcomes of interest: hypoxemia, hypocapnia, hypotension, hypertension, and
phenylephrine administration. Our results suggest that if the previous state of the art ma
chine learning model (a gradient boosted tree model using hand engineered features [122])
captured 15% of hypoxemic events, PHASE captures approximately 19% of hypoxemic events
based on a fixed recall. In our dataset we observe approximately 2.3 hypoxemic events per
surgery, in the US alone our method could forecast roughly 1 million hypoxemic events that
the previous state of the art model fails to capture (given that there are an annual 10 million
surgeries in the US).
Furthermore, we show that PHASE improves performance in a transferred embedding
setting where LSTM embedding models are trained in one dataset and used to extract fea
tures in a completely unseen dataset. Building upon this finding, we show that fine-tuning
the LSTMs on unseen data leads to faster convergence and improved predictive performance
compared to randomly initialized models across all outcomes. Finally, we validate our mod
els by identifying important variables using state of the art local feature attribution methods
[123]. We interpret our models to validate that the models uncover statistical patterns that
agree with prior literature and demonstrate that models trained using PHASE are explain
able. Importantly, explainability ensures that models are fair, trustworthy, and valuable to
scientific understanding [52]. PHASE takes a step in the direction of allowing scientists to
collaborate on EHR data which is typically accessible by only a single group (data silos [155])
by investigating approaches to train embedding models that generalize to unseen data.


84
5.2 Results
5.2.1 Five perioperative outcomes from three hospital datasets
We are interested in forecasting five outcomes associated with surgical morbidity. The first is
hypoxemia (i.e., low blood oxygen level), an important cause of anesthesia-related morbidity,
resulting in harmful effects on nearly every end organ [106, 56]. The next three outcomes are
hypocapnia (i.e., low blood carbon dioxide), hypotension (i.e., low blood pressure), and hy
pertension (high blood pressure). Negative physiological effects associated with hypocapnia
include reduced cerebral blood flow and reduced cardiac output [154]. Prolonged episodes of
perioperative hypotension are associated with end-organ ischemia as well as assorted other
adverse postoperative complications [116, 27, 89]. In addition, perioperative hypertension
has been tied to increased risk of postoperative intracranial hemorrhage in craniotomies [15]
and end organ dysfunction [200]. Finally, the last outcome of interest is the administration
of phenylephrine, a medication frequently used to treat hypotension during anesthesia ad
ministration [97]. Predicting phenylephrine use lets us further evaluate PHASE because it
represents a clinical decision rather than an aspect of patient physiology as in the prior four
outcomes.
To evaluate our methodology with these outcomes, we utilize data from three different
hospital datasets, summarized in Table 5.1 (Methods Section 5.4.1). In brief, we consider
two operating room datasets from distinct medical centers which we denote as OR0 and OR1.
We also use the publicly available intensive care unit MIMIC-III dataset which we refer to
as ICUM [91]. As inputs, we use fifteen physiological signal variables: SAO2 - Blood oxygen
saturation, ETCO2 - End-tidal carbon dioxide, NIBP[S/M/D] - Non-invasive blood pressure
(systolic, mean, diastolic), FIO2 - Fraction of inspired oxygen, ETSEV/ETSEVO - End-tidal
sevoflurane, ECGRATE - Heart rate from ECG, PEAK - Peak ventilator pressure, PEEP 
Positive end-expiratory pressure, PIP - Peak inspiratory pressure, RESPRATE - Respiration
rate, TEMP1 - Body temperature in addition to six static variables: Height, Weight, ASA
Code, ASA Code Emergency, Gender, and Age. All variables are consistently measured in


85
Dataset OR0 OR1 ICUM
Department OR OR ICU Number of procedures/stays 29,035 28,136 1,669 Static Gender (% female) 57% 38% 44% variables Age (yr) Mean 51.859 48.701 63.956 Age (yr) Std. 16.748 18.419 17.708 Weight (lb) Mean 185.273 181.608 176.662 Weight (lb) Std. 54.042 54.194 55.448 Height (in) Mean 66.913 67.502 66.967 Height (in) Std. 8.268 8.607 6.181 ASA Code Emergency 7.65% 15.31% Adverse Hypoxemia Base Rate 1.09% 2.19% 3.93% outcomes Hypocapnia Base Rate 9.76% 8.06% Hypotension Base Rate 7.44% 3.53% Hypertension Base Rate 1.70% 1.66% Phenylephrine Base Rate 7.23% 9.15% 
Table 5.1: Training set statistics for different data sources. Each outcome has a different number of samples due to missing data.
the operating room datasets, but only SAO2 is consistently measured in the ICU dataset.
Our metric of evaluation is the area under a precision recall curve, otherwise known as
average precision (AP), which is more informative than the area under a receiver operating
curve (ROC AUC) for binary predictions with low base rates [48], as in the outcomes we
consider. In particular, we focus on the percent improvement over using the raw, unpro
cessed physiological signals as an evaluation metric, which is analogous to transfer loss: the
difference between the transfer error and the in-domain baseline error [72]. We additionally
report the absolute value of the AP (and ROC AUC for a subset of results).
5.2.2 Overview of the PHASE framework
PHASE is an approach to embed physiological signals. We consider an embedding framework
using upstream embedding models U that are trained for each physiological signal in a source
hospital data set Hs. We evaluate upstream embedding models with a downstream prediction


86
model D whose inputs are the embedded physiological signals concatenated to static variables
and outputs are adverse surgical outcomes. D is trained in a target hospital data set Ht.
We evaluate our models in three ways (Figure 5.1c): (1) standard embedding where the
source hospital is the same as the target hospital Hs = Ht (Figure 5.2b and Figure 5.2d), (2)
transferred embedding where the source hospital is different to the target hospital Hs ̸= Ht
(Figure 5.2c and Figure 5.2d), and (3) fine-tuned embedding where the upstream embedding
model is first trained to convergence in a different source hospital Hs ̸= Ht and then used to
initialize a model that is trained to convergence in the target hospital Hs = Ht (Figure 5.3).
The modeling decision of per-signal upstream embedding was driven by several advan
tages: (1) we showed that per-signal embedding models produce embeddings that outperform
downstream prediction models trained on the raw signals or hand-engineered signal features
(described in Section 2.4) (2) we found that per-signal embedding models worked better
than a single embedding model trained on all signals jointly in , and (3) we demonstrate
that per-signal embedding models work even in a heterogeneous setting where the variables
available in the target hospital are different to the variables available in the source hospital.
Here, we briefly describe the embeddings: raw, ema, rand, auto, next, min, and hypo in
Figure 5.2a (more details in Methods Section 5.4.2). Raw and ema are not deep learning
models. Instead, raw is the raw signal itself and ema are exponential moving averages and
variance features from Lundberg et al. [122]. The remaining embeddings all use the final
hidden layer of LSTMs trained in a source hospital Hs to embed the signals. The first
embedding is rand, which uses an untrained LSTM with random weights. The second is an
unsupervised approach called auto, which uses an LSTM trained to autoencode the input.
The following two approaches (next and min) are self-supervised: the LSTM outputs are
drawn from the same physiological signal variable as the input, but are taken from different
parts of the signal. Next uses LSTMs trained to predict the next five minutes of a particular
signal; min uses LSTMs trained to predict the minimum of the next five minutes of a
particular signal. The final approach, hypo, is a traditional supervised approach to transfer
learning where the embedding model has the same output as the downstream prediction


87
model (either hypoxemia, hypocapnia, or hypotension).
5.2.3 Comparing approaches to embed physiological signals
As a start, we first compare two popular machine learning models (GBTs and LSTMs)
trained on the raw signal data (i.e., without embedding) concatenated to static patient data.
In this section we will refer to results according to (1) the downstream model type and (2) the
signal embedding type (for instance, GBT raw denotes a gradient boosted tree model trained
with the raw minute by minute signal data). In Figure 5.2b, GBT raw performs comparably
to LSTM raw for hypoxemia and better for hypocapnia and hypotension even though the
LSTM should be more suitable to the time series signal data. Based on prior literature, we
hypothesize that the GBT better captures patterns in the static patient data which is tabular
[123], but the LSTMs better capture patterns in the time series data. In order to leverage
the advantages of both model types, we propose PHASE which utilizes LSTMs to embed
physiological signals and GBTs to perform the final prediction using the extracted features
concatentated to static patient data (Figure 5.1a). In the following sections we primarily
use GBTs as the downstream model and when we refer to our results solely by the signal
data embedding they are assumed to use GBTs as the downstream model (for instance, next
denotes a GBT model trained with next embedded data).
We first evaluate the PHASE methods that include two self-supervised embeddings (next
and min) and a supervised embedding (hypo) in a standard embedding setting where the
source dataset is the same as the target dataset (Figure 5.2b). We train GBT downstream
models on the physiological signal embeddings concatenated to static patient features to see
if the embeddings are more informative than the raw signals. Rand (which serves as a lower
bound) transforms physiological signals in an uninformative manner and makes it harder to
predict the outcomes of interest in comparison to the raw signals. Furthermore, ema and auto
fail to consistently improve or impair performance relative to raw and thus are not viable
features. In contrast, the PHASE methods (next, min, and hypo) consistently yield models
that outperform the alternative approaches across all three outcomes (all p-values < 0.05).


88
raw The last sixty minutes of signal: !"#$%&" ema Exponential moving averages/variances. ' ( ) sec., 1 min., 5 min. rand LSTM with random initialization auto LSTM trained to predict !"#$%&" next LSTM trained to predict !"*+&"*, min LSTM trained to predict -./ !"*+&"*, hypo LSTM trained to predict the target task (e.g., -./ 0123"*+&"*, 4 53) By default embed with source OR data set that matches the target OR data set Embed with a source OR data set that does not match the target OR data set Embed SAO2 using ICUP as a source data set and embed the remaining fourteen signals using the target data set as the source data set
Lundberg et. al. Nature BME ‘18
PHASE
�
M
Test Average Precision [% improvement over GBT (raw)]
b.) Performance of PHASE
Hypoxemia Hypocapnia Hypotension
-10 -5 0 5 10
raw ema rand auto next min hypo
next next
next�M min min
min�M hypo hypo hypoM
�
-10 -5 0 5 10
-10 -5 0 5 10
c.) Performance of PHASE transferred between hospitals
Transfer between data sets
a.) Signal embedding legend
raw
LSTM
GBT
GBT
Hypertension Phenylephrine
d.) Performance of PHASE on additional tasks
raw ema auto
next
min
GBT
min
next
auto�
�
�
Signal data embedding
Downstream model type
PHASE
PHASE
PHASE
-10 -5 0 5 10 -10 -5 0 5 10 (0.241) (0.300) (0.424)
(0.161) (0.227) (0.129)
Epinephrine
0 20 40
Figure 5.2: Comparing the performance of downstream models trained with different embeddings of physiological signals concatenated to static features. We report the average precision (% improvement over GBT model trained with raw signal data, 99% confidence intervals from bootstrapping the test set). We use OR0 and OR1 as target datasets and then aggregate across both by averaging the resultant means and standard errors of the % improvement. (a) The upstream embedding models we use to extract the physiological signal features where raw is the identify function, ema is an exponential moving average, and the rest are LSTMs trained in specific ways. (b) The performance of downstream prediction models for a variety of standard embedding approaches (when the source hospital is the same as the target hospital). We compare combinations of downstream models and embeddings for three adverse surgical outcomes (hypoxemia, hypocapnia, and hypotension). (c) The performance of transferred embedding (next’, nextM , min’, minM , hypo’, and hypoM ) vs. nontransferred (next, min, and hypo) models for the above three adverse outcomes. In the transferred approaches the source hospital is different to the target hospital. (d) Performance of approaches for standard and transferred embedding on additional outcomes: hypertension (high, rather than low, blood pressure) and phenylephrine (doctor action prediction). We do not evaluate hypo embeddings in this setting, because the outcomes are not “hypo” events.


89
In particular, ema is a gradient boosted tree model trained with hand engineered features
(exponential moving average) shown to be on par with practicing anesthesiologists at fore
casting hypoxemia (Lundberg et. al. Nature BME 2018 [122]). PHASE embeddings further
improve over this approach suggesting that PHASE outperforms clinicians for forecasting
hypoxemia by approximately 5% (Figure 5.2b).
In order to see how the choice of embedding model output affects downstream model
performance we can take a closer look at auto, next, min, and hypo. Contrasting PHASE
embeddings to auto suggests that incorporating the future in the source task is crucial (as in
next, min, and hypo). However, while taking the minimum (min) and thresholding (hypo)
make the upstream embedding model’s outcome more similar to the downstream predic
tion model’s outcome, min and hypo embeddings do not consistently improve downstream
prediction performance compared to next.
The previously described results show that PHASE works when forecasting hypoxemia,
hypocapnia, and hypotension; however these outcomes are all associated with low signals
(hence the “hypo” prefix). In order to validate that PHASE performs well for “non-hypo”
outcomes as well, we consider two additional outcomes: hypertension (i.e., high blood pres
sure) and phenylephrine administration (doctor action prediction) (Figure 5.2d). For hyper
tension we empirically demonstrate that next embeddings are better then min embeddings.
This is to be expected because min focuses on the minimum of the future signal, whereas
hypertension is defined as blood pressure being too high and it therefore addresses the max
imum of the future signal. For phenylephrine, both the next and min models improve over
standard approaches. One potential reason is that phenylephrine is typically administered
in response to low blood pressure and thus min models are relevant to phenylephrine admin
istration.
5.2.4 Evaluating upstream embedding models on unseen data
Previously we focused on a standard embedding setting in a single medical center; in this
section, we examine the performance of PHASE when the upstream LSTM embedding models


90
are trained in one dataset but used to embed signals in an unseen dataset (i.e., transferred
embedding setting). We analyze two distinct transfer learning settings where the source
hospital differs to the target hospital (more details in Methods Section 5.4.5). We utilize a
superscript notation (′ and M ) to denote transfer learning. The apostrophe (′) denotes that we
trained LSTMs in one operating room dataset and then fixed them to embed signal variables
and evaluate performance with a downstream GBT model in the other. The superscript
M (M ) denotes that we trained the LSTM for SAO2 in ICUM and the other LSTMs in the
target dataset1.
Training the LSTM embedding models on a source dataset that differs from the target
dataset and using a GBT downstream model (′ and M in Figure 5.2c and Figure 5.2d)
generally outperforms conventional approaches: the LSTM trained on raw data and the
GBT trained on raw or engineered features (LSTM raw, GBT raw, and ema in Figure 5.2b
and Figure 5.2d). The next and min embeddings in the transferred embedding settings
(next′, min′, nextM , minM ) outperform the conventional approaches for all possible outcomes
(Figure 5.2c) including hypertension and phenylephrine (Figure 5.2d). However, for hypo, the
supervised embedding, hypo′ improves over raw embeddings for hypoxemia and hypocapnia,
but actually hurts performance for hypotension. Furthermore the hypoM embedding also
hurts performance for hypoxemia relative to using the raw embedding. This suggests that
the choice of LSTM embedding model output is important and the supervised learning
outcome (hypo′, hypoM ) does not generalize to unseen data as well as the self-supervised
approaches (next′, nextM , min′, minM ).
Comparing the transferred embedding models (′ and M in Figure 5.2c and Figure 5.2d) to
the standard embedding models (next, min, hypo in Figure 5.2c and Figure 5.2d) we see that
the transferred embedding models generally perform comparably to the standard embedding
1MIMIC-III (ICUm) has high rates of missingness for signals except for ECG (which is not directly present in the OR datasets) and SAO2. This means we were able to train an upstream LSTM only for SAO2 from ICUm and we extracted features from the remaining signals using LSTMs trained in the target domain. This result is still meaningful, because it means we can use upstream embedding models trained in different domains synergistically.


91
models even though they are evaluated on previously unseen data. In particular, we see that
the next′, min′, nextM , and minM embeddings perform comparably to their standard, non
transferred counterparts (next and min). It is worth noting that the transferred embeddings
are equally performant for hypocapnia and hypotension; however, slightly reduce downstream
performance for hypoxemia and hypertension, which may be due to differences in the hospital
data sets (e.g., covariate shift). As before, we see that the hypo′ and hypoM embeddings
perform substantially worse than their non-transferred counterpart hypo.
Although transferred PHASE embeddings perform slightly worse in the hypoxemia and
hypertension prediction settings, one important advantage of transferring models is that end
users in the target domain can use them at no additional training cost. Training all upstream
LSTM embedding models for next constituted roughly 66 hours on an NVIDIA GeForce RTX
2080 Ti GPU. Clinicians who lack either computational resources or deep learning expertise
to train their own models from scratch can instead use an off-the-shelf, fixed embedding
model. Given that machine learning is usually not the primary concern of hospital staff,
fixed embedding models are a straightforward way to improve the performance of models
trained on physiological signal data at minimal cost to the end users.
There are two additional considerations for transfer learning: (1) In our results, we focus
on evaluation using GBT downstream models. In order to show that the features we extract
consistently boost performance and are robust to the choice of the downstream model we
replicate our results for a multilayer perceptron (MLP) downstream model. (2) Per-signal
LSTM embedding models outperform a single LSTM embedding model jointly trained with
all signals. However, per-signal embedding models have an additional advantage: they work
even when the variables available in the target hospital do not exactly match the ones in
the source hospital (feature heterogeneity). Per-signal LSTM embedding models work in
heterogeneous settings because end users can pick and choose models that correspond to the
signals available at their institution. In comparison, a model trained on all possible variables
would be unusable on a new hospital dataset with different variables. We show that in
heterogeneous settings where the target hospital has fewer features than the source hospital,


92
GBTs trained with PHASE consistently outperform GBTs trained with the raw signals.
5.2.5 Fine-tuning upstream embedding models improves performance and reduces computa
tional cost
In Section 5.2.4 we discussed that using PHASE embedding models in the transferred em
bedding setting are preferable to the standard embedding setting in terms of training cost;
however, the standard embedding models still showed slightly better performance for hypox
emia and hypertension. Alternatively, we propose a fine-tuned embedding approach where
we assume an end user in the target hospital has been provided a pre-trained embedding
model trained in a distinct source hospital. Fine-tuning posits that deep models initialized
using pre-trained models from a separate domain work better than randomly initialized mod
els [211]. We train PHASE in a fine-tuning setting where upstream embedding models are
trained in an OR target hospital initialized using the weights from the best model from the
other OR hospital data set (detailed setup in Methods Section 5.4.5).
We find that PHASE in the fine-tuned embedding setting boosts performance over both
standard embedding (Section 5.2.3) and transferred embedding (Section 5.2.4) in Figure 5.3b.
We focus on next for the following experiment because it performed and generalized well
across most outcomes in previous sections. In Figure 5.3, we evaluate the convergence and
performance of fine-tuning LSTM embedding models. Figure 5.3a shows the convergence of
fine-tuned models. The top two rows fix OR0 as the target dataset. Dark green lines show
the convergence of a randomly initialized LSTM and light green show the convergence of an
LSTM initialized using weights from the best model in OR1. The bottom two rows show the
analogous plots with OR1 as the target dataset. In Figure 5.3a we see that fine-tuning LSTMs
rather than training them from scratch consistently leads to much faster convergence. In
Figure 5.3b, we see that LSTMs obtained from fine-tuning (nextft) consistently outperform
those trained in a single dataset: standard embeddings (next) and transferred embeddings
(next′). These results indicate that end users can fine-tune PHASE LSTMs to boost perfor
mance at lower computational cost in comparison to training models from scratch. Although


93
fine-tuning is more computationally costly than a pre-trained model (transferred embedding),
the performance gains from fine-tuning are more consistent.
5.2.6 Validating models with local feature attributions
We summarize key variables used by downstream GBT models using summary plots (Fig
ure 5.4). In these plots, each point represents a feature’s importance for a single sample,
with the x-axis showing the feature’s impact on the model’s output and the colors indicates
the feature’s value (attribution method details in Methods Section 5.4.5). We focus on ex
plaining GBT models trained on PHASE next embeddings in terms of each variable because
next embeddings were performant across most of the outcomes we considered. The colors are
the sum of all features associated with a single signal variable (200 extracted features) which
are not naturally interpretable because the embedding values can be arbitrarily positive or
negative based on the embedding models.
Standard approaches to train embedding models would use all signal variables as inputs to
a single model. These approaches are harder to interpret, because each embedding dimension
may be dependent on multiple signals simultaneously. Having per-signal embedding models
as in PHASE allows us to clearly interpret each embedding as being dependent on a single
physiological signal variable.
We validate important variables against prior literature for models trained on next em
beddings for all five outcomes (Figure 5.4). For hypoxemia, the important variables includes
variables logically connected to blood oxygen: SAO2, ETCO2, and FIO2 are all associated
with the respiratory system, while and PIP is tied to mechanical ventilation which is natu
rally linked to blood oxygen [98, 54]. For hypocapnia ETCO2 is logically the most important
feature. Furthermore, using FIO2, RESPRATE, PIP, and TV to forecast hypocapnia makes
sense because these variables all relate to either ventilation or respiration. As one would
expect, for hypotension and hypertension, key variables are generally the three non-invasive
blood pressure measurements: NIBPM, NIBPD, NIBPS. Furthermore, a number of studies
validate the importance of ECGRATE (heart rate measured from ECG signals) to forecast