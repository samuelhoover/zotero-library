130 IEEE SIGNAL PROCESSING LETTERS, VOL. 18, NO. 2, FEBRUARY 2011
Spectrogram Image Feature for Sound Event
Classification in Mismatched Conditions
Jonathan Dennis, Student Member, IEEE, Huy Dat Tran, Member, IEEE, and Haizhou Li, Senior Member, IEEE
Abstract—In this letter, we present a novel feature extraction method for sound event classification, based on the visual signature extracted from the sound’s time-frequency representation. The motivation stems from the fact that spectrograms form recognisable images, that can be identified by a human reader, with perception enhanced by pseudo-coloration of the image. The signal processing in our method is as follows. 1) The spectrogram is normalised into greyscale with a fixed range. 2) The dynamic range is quantized into regions, each of which is then mapped to form a monochrome image. 3) The monochrome images are partitioned into blocks, and the distribution statistics in each block are extracted to form the feature. The robustness of the proposed method comes from the fact that the noise is normally more diffuse than the signal and therefore the effect of the noise is limited to a particular quantization region, leaving the other regions less changed. The method is tested on a database of 60 sound classes containing a mixture of collision, action and characteristic sounds and shows a significant improvement over other methods in mismatched conditions, without the need for noise reduction.
Index Terms—Central moments, nonlinear mapping, sound event classification, spectrogram, support vector machine.
I. INTRODUCTION
T
HE study of non-speech sound recognition is less developed than that of speech and speaker recognition. The literature is largely clustered into a few specific areas, such as music identification [1], security surveillance [2], [3], and meeting room sounds [4]. In addition, the systems used are often based on adaptations of common speech recognition systems (i.e.. cepstral features, HMM classifiers), which may not be appropriate to model the characteristics of sound events. A common problem with both speech and sound classification is that the performance sharply degrades in the presence of noise. The purpose of this letter is to develop an alternative sound classification method, inspired by human visual perception, to particularly address mismatched noise conditions. Visual information was adopted by speech researchers from the very early stages in the 1960s and the spectrogram was used then to analyse the phoneme structure [5]. However, “spectrogram reading” has not become an automatic classification method in speech technology, due to the complicated lexicon
Manuscript received September 21, 2010; revised November 24, 2010; accepted December 06, 2010. Date of publication December 20, 2010; date of current version December 30, 2010. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Murat Saraclar. The authors are with the A STAR Institute for Infocomm Research, Connexis S138632, Singapore (e-mail: stujwd@i2r.a-star.edu.sg; hdtran@i2r.a-star.edu.sg; hli@i2r.a-star.edu.sg). Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org. Digital Object Identifier 10.1109/LSP.2010.2100380
Fig. 1. Detailed steps in the method, where path (1) creates the linear-power spectrogram, (2) is the log-power spectrogram, and (3) is the Cepstrogram.
structures of speech. Unlike speech, with its connected phoneme structures, sound events often have shorter durations but with more distinctive time-frequency representations. Therefore, the visual information from the spectrograms of the sounds should produce a good feature for sound classification. We note that there is little in the literature where visual methods have been used, although one example, in [6], presents an approach for classifying musical sounds based on a simple block-matching technique on the greyscale spectrogram. Here, we develop a nonlinear feature extraction method which first maps the spectrogram into a higher dimensional space, by quantising the dynamic range into different regions, and then extracts the central moments of the partitioned monochrome intensity distributions as the feature. The quantization is seen as a generalization of the pseudo-colormapping procedure in image processing. The robustness of the proposed feature is explained by the fact that the spectrogram of the sound signal is sparse, therefore the diffuse noise intensity is mostly located in the lower-regions of the spectrogram’s dynamic range, while the higher-regions remain dominated by the strongest sound components. As the feature is combined with SVM, where the discriminative components are supposed to get higher weights, the proposed method should be robust.
II. SPECTROGRAM IMAGE FEATURE ALGORITHM
The SIF algorithm is as follows. First, a greyscale spectrogram is generated from the sound signal. This is mapped into several monochrome images, which are then partitioned into 9 9 fixed blocks to capture the spatial distribution of the intensities. Finally, the central moments are calculated from each block and concatenated to form the feature vector.
A. Grey-Scale Spectrogram Generation
The procedure for generating the spectrogram images is shown in Fig. 1. First, the windowed Discrete Fourier Transform is applied to the signal, which is given by
(1)
1070-9908/$26.00 © 2010 IEEE
Authorized licensed use limited to: University of Massachusetts Amherst. Downloaded on March 21,2024 at 15:34:21 UTC from IEEE Xplore. Restrictions apply.


DENNIS et al.: SPECTROGRAM IMAGE FEATURE FOR SOUND EVENT CLASSIFICATION 131
Fig. 2. Clean and noisy images from the “Cymbals” sound class. (a) Linear spectrogram; (b) log spectrogram; (c) cepstrogram; (d) colormaps.
where is the length of the window, is the Hamming window function and corresponds to the frequency , where is the sampling frequency in Hertz. Unlike in conventional methods, here we use either linear or log-power representations to generate the spectrograms:
(2)
The time-frequency matrix is then normalised into a greyscale intensity image, with the range scaled between [0, 1]:
(3)
We note that for the log-scaled representation under clean conditions, as tend towards zero its logarithm may be a highly variable quantity that could affect the normalization in (3). Hence, we employ a thresholding:
(4)
Finally, for the cepstrogram, the standard procedure for producing MFCCs is followed, as shown in path (3) in Fig. 1.
B. Dynamic Range Quantization and Mapping
In this step, the dynamic range of the greyscale spectrogram is quantized into different regions, each of which maps to a monochrome image. We can see that this operation is a generalization of the pseudo-colormapping procedure, where greyscale intensities are quantized into red, green and blue (RGB) monochrome components. Fig. 2(d) shows two example quantizations (the HSV and Jet colormaps) which are commonly used for images. The mapping can be denoted as
(5)
where is a monochrome image, is a nonlinear mapping function and represents the quantization regions. We note that, unlike in image processing, the quantization is not limited to the three regions required in the colormap. However, we found from initial experiments that three was a good tradeoff between the accuracy and computational cost, hence the three-level system is employed in this work.
C. Feature Extraction and Classification
Colour distribution statistics are a commonly used feature in image retrieval, as the color distribution describes the image content by characterising the variation of color intensity [7], [8]. For the sound spectrogram, the block-wise distribution statistics from each monochrome image describe the variation of the
sound intensity in quantized regions over time and frequency and therefore together can characterise the sound. Similarly to the approaches in image retrieval [8], we partition each monochrome image into 9 9 blocks and use the central moments in each block to capture the distribution:
(6)
where is the distribution, is the expectation operator, and is the moment about the mean. Particularly, we use the second and third central moments in our system. It is notable that in preliminary experiments, the classification accuracy was increased when the mean was not used as part of the feature, especially in the case of mismatched conditions. In addition, we found that 9 9 blocks was a good compromise between classification accuracy and feature complexity, hence is employed here. Overall, the final Spectrogram Image Feature (SIF) is a 486 -dimension vector, with two central moments and three quantization regions. For the classification, we employ linear SVM [9], in a OneAgainst-One configuration with the max-wins voting strategy. Although we do not report the results from the conventional Gaussian nonlinear kernel in detail, linear SVM achieved higher accuracy with lower computational cost and hence is selected. We note that if we consider the proposed feature extraction as a nonlinear transform from sample , then the method can be considered as a novel kernel SVM classification with .
III. EXPERIMENTS
A. Sound Event Database
A total of 60 sound categories are selected from the RWCP “Sound Scene Database in Real Acoustical Environments” database [10] to give a selection of collision, action and characteristics sounds. The isolated sound event samples are segmented in a regular fashion although, particularly for short sounds, there is some silence either side of the sound. The frequency spectrum for most sounds is quite sparse, with most of the power contained in only a few frequency bands. For each event, 50 files are randomly selected to form the training pool and another 30 files are selected for the testing pool. The total number of samples for testing and training are 3000 and 1800, respectively, with each experiment repeated in five runs.
B. Noise Conditions
To analyse the robustness of the features, the following noises are added at 20, 10 and 0 dB SNR: “Speech Babble”, “Destroyer Authorized licensed use limited to: University of Massachusetts Amherst. Downloaded on March 21,2024 at 15:34:21 UTC from IEEE Xplore. Restrictions apply.


132 IEEE SIGNAL PROCESSING LETTERS, VOL. 18, NO. 2, FEBRUARY 2011
TABLE I COMPARISON OF CLASSIFICATION ACCURACY FOR ALL METHODS FOR THE “SPEECH BABBLE” NOISE
Control Room,” “Factory Floor 1,” and “Jet Cockpit 1,” obtained from the NOISEX’92 database. All four noises are diffuse, and have most of the energy concentrated in the lower frequencies. The exception is the Jet Cockpit noise, which is diffuse but contains more high frequency components. For each of the methods evaluated, we investigate the classification accuracy in mismatched conditions using only clean samples, without added noise, for training.
C. Baseline Evaluation Methods
To compare the performance of the proposed features, we compared them against conventional methods for SER, using both linear and log-magnitude scales for comparison: 1) MFCC-SVM using frame-averaged features; 2) MFCC-HMM: five states and three Gaussians using HTK1 In all cases, we use a 36-D feature vector: 12 MFCCs with the 0th component excluded, from a 24-filterbank system, plus deltas and accelerations.
D. Spectrogram Image Evaluation Methods
Since we consider several variations in creating the spectrogram image, we investigate the following factors: 1) pseudo-color quantization versus greyscale; 2) linear versus log power; 3) spectral versus cepstral representation. In all cases, the same feature extraction method, described above in Section II-C, is applied. We only report the results of the “HSV” mapping here, as in preliminary experiments, it was found that the classification accuracy was slightly higher than for “Jet.” This could be due to the largest amplitude values becoming ambiguous with the lower ones, caused by the final roll-off in the red channel, as seen in Fig. 2(d).
IV. RESULTS AND DISCUSSION
First, comparing the baseline results in Table I, we can see that MFCC-HMM significantly outperforms MFCC-SVM. This is consistent since all temporal information that is lost in the frame-averaged MFCC-SVM features is fully captured in each HMM. Hence, from here onwards, we only use MFCC-HMM as the baseline. In the rest of this section, we first compare the results of the best performing “HSV” pseudo-color Quantised Spectrogram Image Feature (SIF) with the baseline and then discuss the most important factors leading to its success.
A. Pseudo-Colour Quantised SIF versus Baseline
The results across Tables I & III, show that the linear-power SIF, is comparable to the best performing linear-power MFCC-HMM method in high SNR conditions. However, in more severe mismatched conditions, the SIF significantly outperforms the baseline. The average improvement margin is up to 8%, which is obtained under the “Jet Cockpit” noise
1http://htk.eng.cam.ac.uk/.
condition. Statistically significant improvements are observed in all noise conditions. The log-power SIF is less effective in mismatched conditions, with a worse performance in some of the more severe noise conditions.
B. Pseudo-Colour Quantised SIF versus Greyscale SIF
Here we compare the results obtained for the greyscale and pseudo-color quantized SIFs and analyse the effect of the quantization. It can be seen in Table I that the quantized SIFs, excluding the cepstrogram, outperform the equivalent greyscale features in both clean and mismatched conditions. This indicates that by mapping the greyscale spectrogram into a higher dimensional space, in this case the three RGB quantizations, the separability between sound classes has increased. For the case of mismatched noise, the robustness of the proposed feature can be explained by fact that the noise is normally more diffuse than the sound and therefore the noise intensity is located in low-range regions of the spectrogram image. Hence, the monochrome images mapped from the higher-ranges should be mostly unchanged, despite the presence of the noise. Also, since the proposed feature is combined with the SVM classifier, where the discriminative components should be assigned a higher weighting, it should be more robust in mismatched noise conditions. The effect of the quantization can be shown experimentally. Since the SIF is based on the intensity distribution of the monochrome images, we should compare the distribution distance between clean and noisy samples of the same sound event. A robust feature should have a small distance, indicating that the distributions are similar. Modelling the distributions as Gaussian, we can use the Square Hellinger Distance:
(7)
Since we use central moments as the feature, we are effectively mean-normalizing the distributions. Hence, with , (7) simplifies to a ratio of the standard deviations. Example results are presented in Table II, which show the mean distribution distances across the 9 9 blocks, averaged over 50 samples using the linear-power SIF. Although the distribution distance of the green color, representing the low region of the intensities, is relatively large, the distributions of the other colors are less affected by the noise. We suggest that this, combined with the SVM weighting, allows the dynamic range regions that are most susceptible to noise to be ignored.
C. Spectral versus Cepstral Representations
A question that may be raised, is how the SIF concept applies to the “MFCC image.” Table I shows that while the cepstrogram performs well in clean conditions, the accuracy falls rapidly for mismatched SNRs. The noise causes variations, especially in the lower order cepstral coefficients, causing the cepstrogram image to appear quite different, as shown in Fig. 2(c). It seems Authorized licensed use limited to: University of Massachusetts Amherst. Downloaded on March 21,2024 at 15:34:21 UTC from IEEE Xplore. Restrictions apply.


DENNIS et al.: SPECTROGRAM IMAGE FEATURE FOR SOUND EVENT CLASSIFICATION 133
TABLE II EXAMPLE DISTRIBUTION DISTANCES BETWEEN CLEAN AND 0 dB
TABLE III BASELINE COMPARISON FOR OTHER MISMATCHED NOISE CONDITIONS
that as the DCT transform mixes up the frequency components, this breaks the assumption that particular frequency components will still be visible in the noisy spectrum. Therefore the performance is greatly degraded in mismatched conditions compared to that of the standard SIF.
D. Linear versus Log Power
From the experimental results in Tables I and III, it can be seen that one of main outcomes of our experiments is performance of the linear-power SIF, as a linear representation is not commonly used in conventional methods. It can be seen that although log-power gives a higher classification accuracy for clean conditions, it is the linear-power methods that show an improved performance in mismatched conditions. The log representation is expected to perform better than the linear for clean conditions, since taking log-power reduces the dynamic range revealing the detail from the low power frequencies, which provides better discrimination between sound events. An example of the difference in detail can be seen clearly by comparing the spectrograms in Fig. 2(a)and (b). The linear representation on the other hand, is sparse, with less information visible, as the strongest frequencies are an order of magnitude larger than the surrounding ones. This leads to confusion between the most similar sounds, which is reflected in the lower accuracy in clean conditions. However, in mismatched conditions, the noise affects the log and linear representations differently. Since background noise is diffuse, the power is spread over a wider range of frequencies, compared to the strong peaks of the sounds. In the case of the log scale, the detail of the noise is exposed and causes large changes in the color of the spectrogram images. However, the linear representation does not suffer from this, as the important frequency components remain unchanged and are still much larger than those of the noise.
E. Multiconditional Training
One of most popular methods when dealing with noise is multiconditional training, hence we compare the SIF with the
TABLE IV COMPARISON OF EXPERIMENTS WITH MULTICONDITIONAL TRAINING
MFCC-HMM baseline. Training is carried out with Clean, 20 dB and 10 dB samples using the “Destroyer,” “Factory,” and “Jet” noises. Testing is on samples with “Babble” noise at 20 dB, 10 dB, and also 0 dB. As expected, Table IV shows that MFCC-HMM has greatly improved compared to mismatched conditions. However, it is still worse when compared to our linear-power SIF in mismatched conditions from Table III. In addition, both SIF methods show improvement, although it is the log-power SIF that performs best, unlike in mismatched conditions. It seems that performing multiconditional training is closer to that of matched training and therefore the classification accuracy is improved due to the dynamic range reduction, revealing the detail from the low-power frequencies.
V. CONCLUSION
In this letter, a novel method for sound event classification is proposed, motivated by the visual perception of the spectrogram image. The important point of the method is that since the noise is diffuse, it affects only limited part of the dynamic range, hence the quantization of spectrogram maps the noise intensity to certain regions of the higher dimensional space, leaving the signal intensity unchanged in the other regions. Therefore when combining with a discriminative classifier, like SVM, it yields a very robust classification accuracy in mismatched conditions. We also showed that while the log-power SIF produces a similar classification accuracy to the equivalent MFCC-HMM baseline, the best performance comes from using a linear-power representation, as it is highly robust to noise in mismatched conditions.
REFERENCES
[1] R. Typke, F. Wiering, and R. Veltkamp, “A survey of music information retrieval systems,” in ISMIR, 2005, pp. 153–160. [2] C. Clavel, T. Ehrette, and G. Richard, “Event detection for an audiobased surveillance system,” in IEEE ICME, Amsterdam, The Netherlands, Jul. 2005. [3] L. Gerosa and G. Valenzise et al., “Scream and gunshot detection in noisy environments,” in EURASIP, Poznan, Poland, Sep. 2007. [4] A. Temko and C. Nadeu, “Classification of meeting-room acoustic events with support vector machines and confusion-based clustering,” in Proc. IEEE ICASSP ’05, 2005, pp. 505–508. [5] V. Zue, “Notes on Spectrogram Reading,” Dept. EECS, Mass. Inst. Technol., Cambridge, 1985. [6] G. Yu and J.-J. Slotine, “Audio classification from time-frequency texture,” in Proceedings IEEE ICASSP ’09, Taipei, Taiwan, 2009, pp. 1677–1680. [7] M. J. Swain and D. H. Ballard, “Color indexing,” Int. J. Comput. Vis., vol. 7, no. 1, pp. 11–32, 1991. [8] J. L. Shih and L. H. Chen, “Colour image retrieval based on primitives of colour moments,” Proc. Inst. Elect. Eng., Vis. Image Signal Process., vol. 149, no. 6, pp. 370–376, 2002. [9] G. Fung and O. L. Mangasarian, “Proximal support vector classifiers,” in Proc. Seventh ACM SIGKDD, 2001a, pp. 77–86, ACM. [10] Real World Computing Partnership, RWCP Sound Scene Database in Real Acoustic Environments [Online]. Available: http://tosa.mri.co.jp/ sounddb/indexe.htm
Authorized licensed use limited to: University of Massachusetts Amherst. Downloaded on March 21,2024 at 15:34:21 UTC from IEEE Xplore. Restrictions apply.