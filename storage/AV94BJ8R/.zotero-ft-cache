Contents lists available at ScienceDirect
Information Processing and Management
journal homepage: www.elsevier.com/locate/infoproman
AHAB: Aligning heterogeneous knowledge bases via iterative
blocking
Chen Ling⁎, Gu Weidong, Tian Xiaoxue, Chen Gencai
College of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China
ARTICLE INFO
Keywords:
Heterogeneous knowledge base Alignment Iterative blocking Candidate entity pairs
ABSTRACT
With the development of information extraction, there have been an increasing number of large-scale knowledge bases available in different domains. In recent years, a great deal of approaches have been proposed for large-scale knowledge base alignment. Most of them are based on iterative matching. If a pair of entities has been aligned, their compatible neighbors are selected as candidate entity pairs. The limitation of these methods is that they discover candidate entity pairs depending on aligned relations, which cannot be used for aligning heterogeneous knowledge bases. Only few existing methods focus on aligning heterogeneous knowledge bases, which discover candidate entity pairs just for once by traditional blocking methods. However, the performance of these methods depends on blocking keys heavily, which are hard to select. In this paper, we present an approach for aligning heterogeneous knowledge bases via iterative blocking (AHAB) to improve the discovery and refinement of candidate entity pairs. AHAB iteratively utilizes different relations for blocking, and then matches block pairs based on matched entity pairs. The Cartesian product of unmatched entities in matched block pairs forms candidate entity pairs. By filtering out dissimilar candidate entity pairs, matched entity pairs will be found. The number of matched entity pairs proliferates with iterations, which in turn helps match block pairs in each iteration. Experiments on real-world heterogeneous knowledge bases demonstrate that AHAB is able to yield a competitive performance.
1. Introduction
Knowledge bases organize human knowledge in structural form and are widely used as prior knowledge in applications, e.g., information retrieval (Han, Chen, & Tian, 2018) and question answering (Yin et al., 2016). Thanks to the rapid development of information extraction, more and more knowledge bases have been published online in recent years (Bollacker, Evans, Paritosh, Sturge, & Taylor, 2008; Hoffart, Suchanek, Berberich, & Weikum, 2013; Lehmann, Isele, Jakob, Jentzsch, & Kontokostas, 2015). Although data providers are encouraged to connect their datasets into the LOD (Linked Open Data) Cloud, most datasets are not sufficiently linked to each other. According to Schmachtenberg, Bizer, and Paulheim (2014), 44% of all datasets have no links pointing to at least one other dataset. This raises challenges in Linked Data applications. In addition, the existing links are not of high quality. Since different knowledge bases use different entity terms and the number of entities increases sharply, discovering the owl: sameAs links (i.e., two different URIs referring to the same real-world object) between different knowledge bases is a challenging task, which is known under various names, e.g., knowledge base alignment (Lacoste-Julien et al., 2013), entity resolution (Christophides, Efthymiou, & Stefanidis, 2015), object coreference resolution (Glaser, Jaffri, & Millard, 2009; Hu & Jia, 2015), and instance matching (Castano, Ferrara, Lorusso, & Montanelli, 2008; Rong et al., 2012).
https://doi.org/10.1016/j.ipm.2018.08.006 Received 5 January 2018; Received in revised form 4 June 2018; Accepted 26 August 2018
⁎ Corresponding author. E-mail addresses: lingchen@zju.edu.cn (L. Chen), guweidong@zju.edu.cn (W. Gu), xxtian@zju.edu.cn (X. Tian), chengc@zju.edu.cn (G. Chen).
Information Processing and Management 56 (2019) 1–13
Available online 21 September 2018 0306-4573/ © 2018 Elsevier Ltd. All rights reserved.
T


Considerable investments (Aswani et al., 2006; Aumueller, Do, Massmann, & Rahm, 2005; Hassell, Aleman-Meza, & Arpinar, 2006; Nagy, Vargas-Vera, & Stolarski, 2009; Niu, Rong, Zhang, & Wang, 2011) have been done to automatically discover the owl: sameAs links between different knowledge bases. Most of them are inefficient and cannot deal with large-scale knowledge base alignment, as they need to compare all entity pairs comprehensively. Some other methods based on iterative matching, e.g., SIGMa (Lacoste-Julien et al., 2013), SLINT (Nguyen, Ichise, & Le, 2012), and RIMOM-IM (Shao et al., 2016), are applied for large-scale knowledge base alignment. The idea of these methods is that if a pair of entities has been aligned, their compatible neighbors are selected as candidate entity pairs. Therefore, an entity is just compared with few entities from another knowledge base to improve efficiency. However, these methods mainly align homogeneous knowledge bases, i.e., two knowledge bases with lots of aligned relations and properties. They are ineffective for aligning heterogeneous knowledge bases, i.e., two knowledge bases with few aligned relations and properties, as they would miss some candidate entity pairs. An example is shown in Fig. 1. In KB1 and KB2, the entities “Barack Hussein Obama” and “Obama” have been aligned. Since the relations “bornIn” are matched, the entities “U.S.A.” and “The United States of America” can be selected as a candidate entity pair. However, the relation between “Barack Hussein Obama” and “Michelle LaVaughn Obama” is described as “family” in KB1, while the relation between “Obama” and “Michelle” is described as “wife” in KB2. By the above methods, the entities “Michelle LaVaughn Obama” and “Michelle” cannot be selected as a candidate entity pair. Similarly, the entities “Malia” and “Malia Obama” also cannot be selected as a candidate entity pair. In addition to the above methods based on the graph structure, some other works use the content of knowledge bases to discover candidate entity pairs, e.g., SERIMI (Araujo, Tran, Vries, & Schwabe, 2015). This kind of methods selects candidate entity pairs by blocking, i.e., using the literals of properties as blocking keys to distinguish entities and the discovery of candidate entity pairs does not depend on aligned relations. However, the performance of this kind of methods depends on blocking keys heavily, which are hard to select. As mentioned above, the discovery and refinement of candidate entity pairs are two key points in knowledge base alignment research. In this paper, we propose an approach for aligning heterogeneous knowledge bases via iterative blocking (AHAB), which covers both these two key points. AHAB uses different relations for blocking in different iterations, so that more candidate entity pairs can be discovered. In each iteration, AHAB uses a divide-and-conquer strategy to discover and refine candidate entity pairs: first, for each knowledge base, select a relation randomly to divide the entities of the knowledge base into blocks, which are then matched based on matched entity pairs; next, the Cartesian product of unmatched entities in matched block pairs is treated as candidate entity pairs; finally, refine candidate entity pairs by computing their similarity. By such a design, the number of matched entity pairs proliferates with iterations, which would in turn lead to more matched block pairs. The main contributions of this paper are summarized as follows:
(1) Propose a heterogeneous knowledge base alignment approach AHAB, which iteratively discovers and refines candidate entity pairs. Blocks are constructed within each knowledge base using different relations in different iterations, so AHAB can discover more candidate entity pairs. (2) Present a divide-and-conquer strategy to discover and refine candidate entity pairs in each iteration, in which, based on matched entity pairs, blocks are matched first to discover candidate entity pairs, and then new matched entity pairs are discovered by refining candidate entity pairs. (3) Evaluate the proposed approach on real-world heterogeneous knowledge bases and perform a comparison with other methods. The experimental results show that AHAB is able to yield a competitive performance.
The remainder of this paper is structured as follows. In the next section, we review the related work. Section 3 gives preliminaries and the problem definition. Section 4 introduces the framework and the details of AHAB. Section 5 reports the experimental results. Section 6 concludes and discusses future research directions.
Fig.1. An example of the failure of iterative matching based methods on heterogeneous knowledge base alignment.
L. Chen et al. Information Processing and Management 56 (2019) 1–13
2


2. Related work
This section gives a review of the previous works related to knowledge base alignment, including knowledge base alignment by logical reasoning, single matching, iterative matching, and other related methods.
2.1. Knowledge base alignment by logical reasoning
Knowledge base alignment derives from ontology alignment. Influenced by ontology alignment methods, previous works (Essaid, Martin, Smits, & Yaghlane, 2014; Hogan, Zimmermann, Umbrich, Polleres, & Decker, 2012; Hu & Jia, 2015; Huber, Sztyler, Noessner, & Meilicke, 2011; Isele & Bizer, 2012) in knowledge base alignment use semantic rules to reason equivalent entities. Hogan et al. (2012) first exploit a subset of OWL 2 RL/RDF rules to get novel owl: sameAs links by functional properties and inversefunctional properties. Then, they reason matched entity pairs based on shared inlinks, outlinks, and attribute values. ObjectCoref Hu and Jia (2015) first uses several OWL semantics to get matched entity pairs, which are used to learn the discriminability of property pairs. Then, it uses discriminative property pairs for reasoning. CODI (Huber et al., 2011) utilizes the syntax and transforms the alignment task into the optimization problem. Essaid et al. (2014) use the theory of belief function as a tool for modeling, and introduce a decision process based on a distance measure to identify the most possible matched entities for a given source entity. In addition, some methods (e.g., ASMOV (Jean-Mary, Shironoshita, & Kabuka, 2009 and LN2R (Saïs, Pernelle, & Rousset, 2009)) combine reasoning and string matching techniques, which get equivalent entities by using different similarity measures. One advantage of logical reasoning based methods is that most of matched entity pairs they find are right, i.e., the precision is high. However, this kind of methods may miss a lot of matched entity pairs, so the recall is low. In addition, logical reasoning based methods depend on the correct expression of knowledge bases. Nowadays, a lot of large-scale knowledge bases are automatically constructed and contain some conflicting or wrong information, so logical reasoning based methods are not suitable for aligning these knowledge bases, and single matching based methods appear instead of logical reasoning.
2.2. Knowledge base alignment by single matching
Single matching based methods select limited candidates for each entity, and an entity is just compared with its few candidates. Single matching based methods select candidate entity pairs just once. Blocking (Efthymiou, Stefanidis, & Christophides, 2015) is the common technique used here, which reduces the number of entity pairs to be compared by avoiding apparent nonmatching entity pairs and maintains high matching quality. Some previous works (Yan, Lee, Kan, & Giles, 2007) get effective blocking keys by domain experts, but the required knowledge may be unavailable for different domains. To address this limitation, Song and Heflin (2011) learn a set of predicates through domain-independent unsupervised learning, which are used for candidate selection. Nguyen et al. (2012) first use adaptive filtering in blocking. Then, they jointly use the string similarity and structure similarity to compute the similarity of entity pairs. In addition, to address the increase of the scale of data, Efthymiou, Papadakis, Papastefanatos, Stefanidis, and Palpanas (2017) present three parallelization strategies for parallelizing Meta-blocking using the MapReduce framework. However, the above methods mainly align homogeneous knowledge bases. Schema-agnostic blocking (Papadakis, Alexiou, Papastefanatos, & Koutrika, 2015) is appropriate for heterogeneous knowledge base alignment. It completely disregards any schema information and simply creates one block for every token in the literals of any property. SERIMI (Araujo et al., 2015) also aims to match entities between heterogeneous knowledge bases. It first gets some candidates for each entity by traditional blocking methods, and then refines them by filtering out those entities that do not accord with the class of interest. Instead of comparing the literals of properties, it uses a class-based measure to compute the similarity of entity pairs. Although single matching based methods are of good scalability, the number and quality of candidate entity pairs they find are relatively poor. This is because these methods just use properties to select candidate entity pairs for once and ignore the information of aligned entities. To address this limitation, iterative matching based methods appear.
2.3. Knowledge base alignment by iterative matching
Iterative matching based methods select candidate entity pairs for several times. Whang, Menestrina, Koutrika, Theobald, and Garcia-Molina (2009) propose an iterative blocking framework where blocking is iteratively performed by different blocking keys until no block contains any more matched entities. However, the method just uses properties as blocking keys to select candidate entity pairs and depends on aligned properties, which ignores the information of aligned entities and cannot be used for heterogeneous knowledge bases. SIGMa (Lacoste-Julien et al., 2013) is an iterative greedy algorithm for the alignment of large-scale knowledge bases. It uses aligned entity pairs to discover new candidate entity pairs in each iteration, which can find more candidate entity pairs. When a pair of entities is matched, their compatible neighbors are added to a candidate list, where entity pairs with high similarity score will be compared earlier. LINDA (Böhm, De-Melo, Naumann, & Weikum, 2012) is also an iterative algorithm applied to knowledge base alignment. It mainly focuses on how to run a SIGMa-like algorithm using the MapReduce framework. RIMOM-IM (Shao et al., 2016) is similar to SIGMa and also an iterative matching based method, which proposes a distinctive information based blocking method to reduce the number of candidate entity pairs. However, these methods focus on aligning homogenous knowledge bases, where aligned relations are used for discovering candidate entity pairs. Heterogeneous knowledge bases have less aligned relations, so some matched entity pairs may be missed and the recall would be relatively lower.
L. Chen et al. Information Processing and Management 56 (2019) 1–13
3


2.4. Other related methods
Another direction for knowledge base alignment is using human efforts to improve alignment results (El-Roby & Aboulnaga, 2015; Isele & Bizer, 2013; Wang, Kraska, Franklin, & Feng, 2012). For example, El-Roby and Aboulnaga (2015) use the feedback provided by users on the answers to linked data queries to improve the quality of links by reinforcement learning. Isele and Bizer (2013) propose an approach in which machines automate the generation of link rules and users confirm or decline a number of link candidates. Wang et al. (2012) present a hybrid human-machine approach, in which users only verify the most likely matched links after machines do a preliminary processing over all the data. However, considering the scale of knowledge bases, human involvement would be extremely expensive, and the insufficient training of users might cause noise. Apart from the above works by crowdsourcing, there are also some probabilistic methods. PARIS (Suchanek, Abiteboul, & Senellart, 2011) jointly solves the problems of schema and ontology matching. It defines a function describing the relationship between neighbor nodes, which is used to model the probability of relation, instance, and schema alignment. Some other methods (Albagli, Ben-EliyahuZohary, & Shimony, 2012; Singla & Domingos, 2006) are based on Markov logic, in which knowledge bases are treated as probabilistic graphs. Due to high complexity and the requirement of fine parameter tuning, these methods suffer from low efficiency. In addition, some embedding based methods (Chen, Tian, Yang, & Zaniolo, 2017; Zhu et al., 2017) only use the internal structural information in knowledge bases for knowledge base alignment. They jointly encode both entities and relations into a unified lowdimensional semantic space based on initial matched entity pairs, and then entities can be aligned according to their semantic distances in the joint semantic space. Due to simplicity, these methods are suitable for large-scale knowledge base alignment. However, these methods cannot be used for aligning heterogeneous knowledge bases, as the internal structures are very different.
3. Preliminaries and problem definition
In this section, we first give the definitions of some basic concepts and terms, and then formally define the problem.
Definition 1. (Knowledge Base) A knowledge base KB is a tuple (E, L, R, P, FR, FP), where E, L, R, and P are the sets of entities, literals, relations, and properties, respectively. FR⊆E × R × E is a set of triples that are about relation statements. FP⊆E × P × L is a set of triples that are about property statements.
Note that relations and properties are different. Relations use entities to describe entities, but properties use literals to describe entities. An entity-relation-entity triple describes the relation between two entities, which can help to select candidate entity pairs when a pair of matched entities is discovered. An entity-property-literal triple describes the information of an entity, which can help to compute the similarity of candidate entity pairs. Like both SIGMa (Lacoste-Julien et al., 2013) and SERIMI (Araujo et al., 2015), we also assume that an entity has at most one matched entity in the other knowledge base, i.e., the 1–1 matching constraint.
Definition 2. (Relation Based Block) A relation based block b is a set of subject entities that have the same relation and object entity, i.e., b = { e e , r, e′ ∈ F , e ∈ E, r ∈ R, e′ ∈ E}
i i R i , i = {1, 2, ..., n}. To simplify the description, a relation based block is abbreviated as a block. B is a set of blocks.
For example, there are three triples: (Avatar, Actor, Sam Worthington), (The Debt, Actor, Sam Worthington), and (Righteous Kill, Actor, Al Pacino). Both movies “Avatar” and “The Debt” contain the actor “Sam Worthington”, so they are in a same block. While the movie “Righteous Kill” does not contain the actor “Sam Worthington”, so it belongs to another block. According to the definition, entities in a same block are similar.
Definition 3. (Candidate Entity Pair) A candidate entity pair c is a pair of entities that are similar and have a possibility of matching. C is a set of candidate entity pairs.
Definition 4. (Matched Block Pair / Matched Entity Pair) If the similarity of two blocks is more than the threshold, i.e., simblock(bk, bl) > δb, the two blocks are a matched block pair mb. If the similarity of a candidate entity pair is more than the threshold, i.e., sim(ei, ej) > δe, the two entities are a matched entity pair me. Mb and Me are the sets of matched block pairs and matched entity pairs, respectively.
Definition 5. (Heterogeneous Knowledge Base) If there are few matched relations and properties between two knowledge bases, i.e., ≤
∪ 0.1
Align R R R
(, ) R
12
1 2 and ≤
∪ 0.1
Align P P P
(, ) P
12
1 2 , the two knowledge bases are heterogeneous; otherwise, they are homogeneous.
To distinguish from traditional approaches, AHAB solves the heterogeneous knowledge base alignment problem without depending on aligned relations and properties. If ei ∈ E1, ej ∈ E2, and ei is matched to ej, they should satisfy: = ∈
e argmax sim (e
j eE i
k 2 , ek), where sim(ei, ek) denotes the similarity between ei and ek. Since an entity has at most one matched entity, the alignment problem can be transformed into the following optimization problem:
∑
∑ ≤ ∀∈
∑ ≤ ∀∈
∑≤
∈∈
∈∈
sim e e A
A eE
A eE
AU
max
st
( , )*
. . 1,
1,
e E e E i j ij
e ij j
e ij i
e E e E ij
, (, )
(, ) 2
(, ) 1
, (, )
ij
i
j
ij
12
1 2 (1)
L. Chen et al. Information Processing and Management 56 (2019) 1–13
4


where A is the alignment matrix between E1 and E2. If ei is aligned with ej, A = 1
(i, j) ; otherwise, A = 0
(i, j) . U is the upper-bound of the number of Me. Therefore, the alignment problem can be expressed as: Align (KB , KB ) = {(e , e ) A = 1, e ∈ E , e ∈ E }
1 2 i j (i, j) i 1 j 2 .
4. The method
In this section, we first introduce the framework of AHAB. Next, we show how to discover and refine candidate entity pairs iteratively. Last, we discuss the strategy of reducing the number of comparisons.
4.1. Framework
The framework of AHAB is illustrated in Fig. 2. AHAB is performed in two main steps: discovering candidate entity pairs and refining candidate entity pairs. Discovering candidate entity pairs consists of partitioning the entities of each knowledge base into blocks, matching blocks, and generating candidate entity pairs. Refining candidate entity pairs is to get new matched entity pairs by comparing candidate entity pairs. The two steps run iteratively and alternately until no new matched entity pairs can be discovered. All matched entity pairs that contain initial matched entity pairs and new matched entity pairs are the final results. As shown in Fig. 2, AHAB discovers candidate entity pairs by matched block pairs, and then refines candidate entity pairs to get new matched entity pairs. Since new matched entity pairs are discovered, the similarity between blocks increases and new matched block pairs can be discovered. Therefore, even though relations for blocking have already been used, AHAB can still find new matched block pairs and candidate entity pairs. In AHAB, if a matched block pair has been discovered, it will not appear in later iterations, as the candidate entity pairs generated by it have already been compared carefully. The algorithm of AHAB is simply described in Algorithm 1. A variable, namely newMatchedPair, represents the number of new matched entity pairs in the current iteration. Lines 2 to 11 are called an iteration. For each iteration, there are two steps. First, the candidate entity pairs are selected and refined (lines 4–8). The random function in lines 4 and 5 is used to select a relation randomly for each knowledge base, and the CandidatePairsSelection function in line 6 can get candidate entity pairs by blocking using the selected relations, which is illustrated in Algorithm 2. Second, check if the algorithm ends (lines 9–11). The algorithm ends when no new matched entity pair can be discovered, i.e., newMatchedPairs = 0. Note that in each iteration, AHAB randomly selects one relation from each knowledge base for blocking, so the number of relations hardly affect the overall time complexity of AHAB. Initial matched entity pairs play an important role in AHAB. They serve as matched entity pairs in the first iteration, so the number of them cannot be too small to propagate information. In addition, since false links would lead to more false links, the quality of initial matched entity pairs should be ensured. Therefore, only entity pairs with the same string representation are selected, and the literals of the name property are used as the string representation of an entity, e.g., there is a triple (actor/46619, actor_name, Mary Jo Eustace) in LinkedMDB and “Mary Jo Eustace” is the string representation of the entity “actor/46619′′. However, the name property can have different representations in different knowledge bases, so we manually identify the name property in different knowledge bases. In addition, the 1–1 matching constraint should be satisfied. For example, if entity A has the same string representation as both entities B and C in another knowledge base, neither (A, B) nor (A, C) is selected as an initial matched entity pair.
Fig.2. The Framework of AHAB.
L. Chen et al. Information Processing and Management 56 (2019) 1–13
5


4.2. Discovering candidate entity pairs
Discovering candidate entity pairs is illustrated in Algorithm 2. It aims to use matched entity pairs to discover candidate entity pairs in an iteration. Initially, blocking techniques are used to divide the entities of each knowledge bases into blocks. The entities in one block are more similar than other entities and show some common features. In AHAB, blocking is done in two steps: first, for each knowledge base, select a relation randomly and group different object entities by the selected relation (we assume a triple is in the form of (subject entity, relation, object entity)); second, put all its subject entities into one block for each different object entity. For example, the relation “Actor” is selected for blocking in a knowledge base in an iteration (the type of the subject entities of “Actor” is movie and the type of the object entities of “Actor” is actor). Then, all his/her movies are put into one block for each different actor, e.g., the movies “Titanic”, “Inception”, and “The Revenant”, whose actors include “Leonardo”, would be put into a same block. After blocking, block matching is performed and matched block pairs help get candidate entity pairs. Since blocks can be regarded as sets here, set similarities can be used to measure the similarity of block pairs. The early work (Medin, Goldstone, & Gentner, 1993) shows that the similarity between two sets depends on commonalities and differences, and some traditional set similarity functions (e.g., Jaccard similarity) give different weights to commonalities and differences. In AHAB, we emphasize on commonalities, as the number of matched entity pairs is much smaller than the number of all entities. To decide whether two blocks are matched or not, the common entities are more crucial. Specifically, Eq. (2) is used to measure the similarity between two blocks:
=∩
∩+ ∪− ∩
sim b b b
b βb b
(, ) ( )
block k l
kb
kb kb kb
l
l l l (2)
where the variables bk and bl represent two blocks from different knowledge bases. The matched entities in two blocks are treated as the same elements, which are called “anchors” in some works (Hu, Qu, & Cheng, 2008; Seddiqui & Aono, 2009). bk∩bl and bk∪bl denote the numbers of matched entity pairs and all semantically different entities, respectively. Therefore, bk∩bl and b ∪ − b ∩
kb kb
ll
are the commonalities and differences. βε[0, 1] is a parameter that controls the effect of differences on the similarity between two blocks. If the similarity of two blocks is higher than the threshold δb, the two blocks are matched. The entities that are not matched in each matched block pair form candidate entity pairs. Since a block may match with more than one other block, an entity may belong to more than one candidate entity pair. However, due to the 1–1 matching constraint, if an entity has been matched, AHAB would never discover candidate entity pairs for it later. In essence, the process of discovering candidate entity pairs by matched entity pairs is similarity propagation. The intuition to propagate similarity is similar to a lot of works in ontology matching (Li, Tang, Li, & Luo, 2009). However, they concentrate on the structural similarity of graphs. The graph structures of heterogeneous knowledge bases are quite different, so the structural similarity is low. In AHAB, blocks are applied to propagate similarity. Though there are few aligned relations in heterogeneous knowledge bases, the blocks achieved by blocking can be matched by computing the similarity between blocks and help get candidate entity pairs. An entity will be compared unless it is isolated, i.e., the entity has no relation with other entities.
4.3. Refining candidate entity pairs
Computing the similarity of candidate entity pairs is an important part of alignment algorithms. Most such similarity functions only consider the string representation. For aligning heterogeneous knowledge bases, the similarity between blocks is also useful, as it is an important way to propagate similarity (discussed in Section 4.2). The similarity function is given in Eq. (3):
sim (e , e ) = αsim (e , e ) + (1 − α)sim (b , b )
i j string i j block k l (3)
where ei and ej represent two entities to be compared, bk and bl are the blocks that contain entities ei and ej, respectively, αε[0, 1] is a tradeoff parameter between two contributions. If the similarity of a candidate entity pair is higher than the threshold δe, the two entities are matched. For the string similarity, some essential pre-processing is done first, e.g., removing language flags (@en) and punctuations. Since each string similarity measure has its advantages, e.g., Jaro-Winkler similarity is suitable for short string, AHAB combines different measures using linear weight. Specifically, the string similarity function is given in Eq. (4). Levenshtein distance (Navarro, 2001), Jaro-Winker similarity (Winkler, 2006), Q-Gram similarity (Naumann & Herschel, 2010), and I-SUB similarity (Stoilos, Stamou, & Kollias, 2005) are integrated with equal weight. For the similarity between blocks, simblock(bk, bl) in Eq. (2) is used.
=× +×
+× +×
−
−−
sim e e sim e e sim e e
sim e e sim e e
(, ) (, ) (, )
(, ) (. )
string i j levenshtein i j jaro winker i j
q gram i j i sub i j
1 4
1 4 1 4
1
4 (4)
4.4. Reducing the number of comparisons
In order to prevent the loss of some candidate entity pairs when dealing with heterogeneous knowledge bases, AHAB tries to explore larger search space to discover candidate entity pairs, which may bring some unnecessary comparisons. This section introduces some restrictions to reduce the number of comparisons.
L. Chen et al. Information Processing and Management 56 (2019) 1–13
6


In AHAB, the most time-consuming step is to get matched block pairs, which requires a maximum of |B1| × |B2| comparisons in each iteration. Since |B1| and |B2| are not small, AHAB reduces the number of comparisons by reducing B1 and B2 to B1* and B2*. Then, B1* and B2* are used instead of B1 and B2 in the lines 2 and 3 of Algorithm 2. In addition, we need to find B1* and B2* at low cost, so that the consumed time is less than the saved time by replacing B1 and B2 with B1* and B2*. Therefore, a greedy algorithm is used to avoid some unimportant blocks that are less likely to lead to matched entity pairs. The procedure to obtain B* is summarized in Algorithm 3. Algorithm 3 reduces the number of comparisons in three steps. First, the number of entities in each block should be less than a threshold. If a block is too large, AHAB would get a large number of candidate entity pairs. In addition, in different iterations, an entity may be distributed in disparate blocks, which can increase the comparison chance of the entity. Second, the percentage of matched entities in each block should exceed a threshold. In AHAB, it is hard for the blocks with less matched entities to get matched blocks, so they can be ignored. Third, if all entities in a block have already been matched, it is not necessary to match this block. The time complexity of Algorithm 3 is O(|B|). In each iteration of AHAB, it will take |B1| × |B2| comparisons between different blocks to get matched block pairs. Thus, it takes O ( B + B + B * × B * )
1 2 1 2 and O(|B1| × |B2|) to get matched block pairs in an iteration with and without Algorithm 3, respectively. In Section 5.3, we will compare the precision, recall, F-measure, and execution time of AHAB with and without Algorithm 3.
5. Experiments
In this section, we first describe the experimental settings, and then show the experimental results to demonstrate the effectiveness of AHAB.
5.1. Experimental settings
Datasets: DBpedia-LinkedMDB and YAGO-IMDB are used as the dataset pairs to test AHAB. Their detailed descriptions are: DBpedia1 (Lehmann et al., 2015) contains structured data from Wikipedia, and LinkedMDB2 Hassanzadeh and Consens (2009) contains information from several major movie web resources. In the following experiments, we use DBpedia-2014 (containing over 33 million triples, about 8 million entities, over 1100 relations, and about 200 properties) and LinkedMDB-latest (containing over 6 million triples, about 5 million entities, over 150 relations, and about 50 properties), and 14 relations and 4 properties are matched between them in advance. These two knowledge bases are real-world datasets in the LOD Cloud, so they can be obtained easily and are already linked. However, they are of large scale and linked automatically, so these links are not of high quality, i.e., these links are not all correct, and it is unreasonable to use these links as ground truth directly. Therefore, we manually label links as ground truth between partial entities in both knowledge bases. We first select the movie entities whose release date was in 2001 from both knowledge bases. Then, we select the movie entity pairs whose literal similarity of the name property is bigger than 0.6 as possible links. Finally, we manually label 347 links as ground truth from more than 6000 possible links. YAGO3 (Hoffart et al., 2013) is a large semantic knowledge base derived from Wikipedia, WordNet, and GeoNames. It is available in the form of triples from its website. IMDB4 is a large popular online database that stores information about movies. It is given as a list of text files, and there are different files for different categories, e.g., actors, directors, etc. Suchanek et al. (2011) transform these text files in a fairly straightforward manner into a collection of triples, and provide the ground truth links between YAGO and IMDB. In the following experiments, we use YAGO3-core (containing over 19 million triples, about 3 million entities, about 40 relations, and over 15 properties) and IMDB-PARIS (containing over 20 million triples, about 5 million entities, about 20 relations, and over 5 properties), and 5 relations and 2 properties are matched between them in advance. Some pre-processing is done for these knowledge bases. First, we remove the irrelevant information. For example, there are some owl: sameAs links in LinkedMDB, which are useless for knowledge base alignment, and we remove them. Second, we unify the format of date, number, name, etc. Evaluation Metrics: We evaluate the performance by comparing the discovered matched entity pairs to the ground truth. Specifically, the quality of discovered links is evaluated using precision = ∩
PM
M
eG
e , recall = ∩
RM
G
e G , and F-measure = +
F PR
PR
2 , where G is the ground truth. We also measure the execution time of different parameter settings and methods. Note that the execution time does not include the time to get initial matched entity pairs. Parameter Setting: In Eq. (2), βε[0, 1] is a parameter that controls the effect of differences on the similarity between two blocks, and we set β = 0.5 in the experiments. In Eq. (3), α ∈ [0, 1] is the weight of string similarity in entity similarity, and we set α = 0.6 in the experiments, as we slightly stress the importance of string similarity. The string representations of two matched entities are rarely very different, so string similarity is more direct and reliable to represent entity similarity. Stopping criterion: AHAB ends when no new matched entity pair is discovered in an iteration. Execution Environment: AHAB is implemented in Java, runs with a single thread, and uses SPARQL for query. In addition, we also re-implemented the compared approaches in Java with a single thread and optimized their meta-parameters. All experimental results are obtained on a machine running Ubuntu 14.04, with Intel Pentium G3220 and 24G memory.
1 DBpedia http://wiki.dbpedia.org/ 2 LinkedMDB http://www.cs.toronto.edu/∼oktie/linkedmdb/ 3 YAGO http://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/ 4 IMDB http://www.imdb.com/
L. Chen et al. Information Processing and Management 56 (2019) 1–13
7


5.2. Experiment 1: The effects of parameters
In this section, we study the effects of parameters δe and δb using DBpedia-LinkedMDB to demonstrate whether AHAB is sensitive to these parameters, and select the best parameters for the other experiments. The parameter δe is used to determine whether two entities are matched. If it is too low (e.g., less than 0.55), some semantically unequal entity pairs may be matched and the precision decreases obviously. Otherwise (e.g., more than 0.75), some matched entity pairs may be missed and the recall decreases obviously. Similarly, the parameter δb is used to determine whether two blocks are matched. Due to the omission of candidate entity pairs, the recall will decrease fast if it is too high (e.g., more than 0.3). Therefore, we select parameters δe and δb in the ranges [0.55, 0.75] and [0.05, 0.3], respectively. For simplicity, we do not consider the strategy of block set reduction. With the variation of parameters δe and δb, the precision, recall, and F-measure are shown in Fig. 3. We can observe that, when the parameter δb increases, i.e., the number of matched block pairs decreases, the precision keeps high, the recall and F-measure drop a little. This is because less matched block pairs lead to less candidate entity pairs, so the number of matched entity pairs also decreases. It also implies that most of matched entity pairs are discovered in matched block pairs with high similarity. Since the F-measure keeps more than 0.9, AHAB is stable with slight changes on the parameter δb. In addition, Fig. 3 also convinces that AHAB is robust with slight changes on the parameter δe. In the following experiments, we set δ = 0.65
e and δ = 0.2
b , as they get the best F-measure.
5.3. Experiment 2: The effects of block set reduction
In this section, we study the effects of parameters δ1 and δ2 using DBpedia-LinkedMDB, which are used to avoid unnecessary comparisons between blocks, and select the best parameters for the other experiments. We mainly concern the precision, recall, Fmeasure, and execution time. In order to study the effect of the parameter δ1, we run AHAB with δ = 10
1 , 20, 30, 40, 50, 60, 10000, and δ = 0.3
2 . The results are shown in Table 1. The parameter δ1 represents the upper bound of the number of entities in one block. Generally speaking, the number of entities in one block is less than 60, so we range the parameter δ1 from 10 to 60. δ = 10000
1 means that we do not restrict the number of entities. A larger δ1 leads to the faster speed of similarity propagation, which causes more block pairs to be discovered, so more matched entity pairs are discovered with more execution time. We can observe that, when δ = 50
1 , the result is stable with a good performance, but the execution time decreases by 21% compared to that of δ = 10000
1 . It demonstrates most blocks that can discover matched entity pairs contain less than 50 entities, so we set δ = 50
1 by default in the following experiments, by which the efficiency can be improved and the performance keeps high. In order to study the effect of the parameter δ2, We run AHAB with δ = 0
2 , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, and δ = 50
1 . Table 2 shows the results. The parameter δ2 represents the lower bound of the percentage of matched entities in one block, i.e., the blocks whose percentage of matched entities is lower than δ2 would be ignored. If the parameter δ2 is too high (e.g., more than 0.6), the recall drops rapidly, so we adjust the parameter δ2 from 0 to 0.6. The parameter δ2 controls the number of blocks to match with each other. In general, the increase of the parameter δ2 will result in a reduction of candidate entity pairs and execution time. From Table 2, it can be seen that when the parameter δ2 increases from 0 to 0.3, the F-measure drops a little, but the execution time reduces by 46%. It
Fig.3. Precision, Recall, and F-measure with the Variation of δe and δb.
L. Chen et al. Information Processing and Management 56 (2019) 1–13
8


indicates that most matched entity pairs can be detected in the block pairs whose similarity is higher than 0.3, so we set δ = 0.3
2 in the following experiments, by which time can be saved without performance loss.
5.4. Experiment 3: The performance of AHAB with iterations
In this section, we study the performance of AHAB with iterations using DBpedia-LinkedMDB and YAGO-IMDB. The results are shown in Fig. 4. The iteration 0 corresponds to the performance of initial matched entity pairs. The following tendencies can be discerned:
(1) AHAB can converge after a few iteration. Generally, the precision just decreases a little, but the recall and F-measure increase obviously with iterations. The improvement of the recall indicates that due to the continuous discovery of matched entity pairs, some hidden candidate entity pairs are discovered. (2) AHAB is robust to the errors in initial matched entity pairs. To ensure that the number of initial matched entity pairs is enough to run AHAB iteratively, we get them loosely (described in Section 4.1), which may result in some errors. However, the precision keeps high with iterations, which indicates that AHAB can control the propagation of errors, as the similarity of candidate entity pairs is well calculated to filter out false entity pairs.
5.5. Experiment 4: Comparison with other methods
In this section, we compare AHAB with other methods (i.e., SIGMa (Lacoste-Julien et al., 2013 and SERIMI (Araujo et al., 2015)) using DBpedia-LinkedMDB and YAGO-IMDB. The detailed descriptions of the compared methods are: SIGMa introduces iterative matching by using the greedy algorithm. It finds matched entity pairs in multiple loops, i.e., only one pair of matched entities is discovered, which is then used as the seed to discover other matched entity pairs based on aligned relations. Due to its simple and greedy nature, it can match large-scale knowledge bases. The main difference with AHAB is that it propagates similarity by the graph structure, but AHAB uses blocks to propagate similarity. In the experiment setting of SIGMa, we use the same initial matched entity pairs as AHAB. SERIMI can solve different kinds of alignment tasks, i.e., it uses the strategy of direct matching when the knowledge bases are homogeneous, and the strategy of class-based matching when the knowledge bases are heterogeneous. When it deals with heterogeneous knowledge bases, the main difference with AHAB is that SERIMI does not use the idea of iteration. In the lines 4 and 5 of Algorithm 1, AHAB randomly selects the relations, so the results of AHAB are varying. Therefore, to statistically measure the significance of performance difference, one sample t-tests at 95% significance level are conducted between these methods. The results are shown in Table 3, from which following tendencies can be discerned:
(1) Compared to SERIMI, AHAB has a significantly higher recall, which indicates that AHAB discovers more candidate entity pairs. SERIMI selects candidates by comparing the literals of properties. However, it is hard to get a satisfactory blocking criterion, so SERIMI may miss some candidate entity pairs. AHAB introduces the idea of iteration and explores more search space, so more candidate entity pairs can be discovered in later iterations.
Table 1
Precision, Recall, F-measure and Execution Time (minutes) with the Variation of δ1.
δ1 Precision Recall F-measure Execution time
10 0.986 0.839 0.907 22 20 0.987 0.902 0.943 27 30 0.979 0.925 0.951 29 40 0.979 0.928 0.953 32 50 0.979 0.931 0.954 33 60 0.979 0.931 0.954 37 10,000 0.979 0.937 0.957 42
Table 2
Precision, Recall, F-measure and Execution Time (minutes) with the Variation of δ2.
δ2 Precision Recall F-measure Execution time
0 0.982 0.937 0.959 61 0.1 0.979 0.931 0.954 41 0.2 0.979 0.931 0.954 38 0.3 0.979 0.931 0.954 33 0.4 0.979 0.925 0.951 28 0.5 0.975 0.899 0.936 24 0.6 0.977 0.873 0.922 13
L. Chen et al. Information Processing and Management 56 (2019) 1–13
9


(2) The recall of AHAB is significantly higher than that of SIGMa, which indicates that AHAB can discover more candidate entity pairs. AHAB assumes that if two blocks are matched, the unmatched entities from the block pair generate candidate entity pairs. However, SIGMa assumes that if two entities are matched and they have aligned relations, their corresponding entities are regarded as candidate entity pairs. The assumption of AHAB is less strict and does not depend on aligned relations, so more candidate entity pairs can be discovered by AHAB, and the recall improves when aligned relations are few. (3) The precision of AHAB is higher than that of the compared methods. It indicates the similarity of most entity pairs is better captured. Traditional approaches use the string similarity and structure similarity to model the similarity of entity pairs. The basic assumption is that the more neighbor entity pairs are matched, the similarity of the entity pair is higher. However, there are few aligned relations in heterogeneous knowledge bases, i.e., matched neighbor entity pairs are sparse. So the set similarity is used in AHAB, and the results demonstrate the set similarity is more suitable for heterogeneous knowledge base alignment. (4) The execution time of AHAB is slower than that of the compared methods. That is because AHAB aligns knowledge bases with iterative blocking, and it can find more candidate entity pairs than the compared methods. Therefore, AHAB is slower but its other performance is significantly high.
Fig.4. Precision, Recall, and F-measure with Iterations.
Table 3
The Comparison of Different Methods (mean ± std); * Indicates AHAB is Statistically Superior to the Compared Method (one sample t-tests at the 5% level).
Dataset Method Precision Recall F-measure Execution time(minutes)
SERIMI 0.947* 0.821* 0.880* 24 DBpedia-LinkedMDB SIGMa 0.963* 0.911* 0.936* 18 AHAB 0.977 ± 0.002 0.929 ± 0.003 0.952 ± 0.002 33 SERIMI 0.938* 0.809* 0.869* 22 YAGO-IMDB SIGMa 0.974* 0.848* 0.907* 15 AHAB 0.978 ± 0.001 0.872 ± 0.002 0.922 ± 0.001 28
L. Chen et al. Information Processing and Management 56 (2019) 1–13
10


Error analysis: Examining some errors made by AHAB, the following types of matching errors can be observed: first, isolated entities, i.e., entities that do not have any relation with other entities, are usually not selected as candidate entity pairs, so some matched entity pairs between isolated entities would be missed and the recall is negatively influenced. It indicates that the propagation of similarity can hardly influence isolated entities. Second, we get some false matched entity pairs that have similar or even same string representations, which suggest that the parameter α can be further tuned to decrease the weight of string similarity. Finally, some false entity pairs appear in initial matched entity pairs and AHAB cannot rectify them. However, getting initial matched entity pairs with too strict rules may cause the missing of some candidate entity pairs and the decrease of recall.
Algorithm 1
AHAB(R1, R2, KB1, KB2, Me0).
Input: Two sets of relations, two knowledge bases, and initial matched entity pairs Me0 Output: The final matched entity pairs Me 1 M =M
e e0
1 while true do
1 newMatchedPairs = 0 1 ri = random(R1) 1 rj = random(R2) 1 C = CandidatePairsSelection(ri, rj, KB1, KB2, Me) 1 Me′ = refinement(C) 1 add Me′ to Me
1 += ′
newMatchedPairs Me 1 if newMatchedPairs = = 0 then 1 return Me
Algorithm 2
CandidatePairsSelection(r1, r2, KB1, KB2, Me).
Input: Two relations, two knowledge bases, and the set of matched entity pairs Me Output: The set of candidate entity pairs C 1 // generating matched block pairs 1 B1 = blocking(r1, KB1) 1 B2 = blocking(r2, KB2) 1 Mb = ∅
1 for bi ∈ B1 do 1 for bj ∈ B2 do
1 if simblock(bi, bj) ≥ δb then 1 add bi, bj to Mb 1 // generating candidate entity pairs 1 for mb ∈ Mb do 1 for ei ∈ b1 do
1 if ei is already matched to other entity then 1 continue 1 for ej ∈ b2 do
1 if ej is already matched to other entity then 1 continue
1 add < ei, ej > to C 1 return C
Algorithm 3
BlockSetReduction(B).
Input: The original set of block B Output: The tidy set of block B* 1 B* = ∅
1 for bi ∈ B do 1 if |bi| > δ1 then 1 continue
1 // bi′ denotes the number of matched entities in the block
1 if ′ < δ
bi
bi 2 then
1 continue 1 if b ′ = = b
i i then 1 continue 1 add bi to B* 1 return B*
L. Chen et al. Information Processing and Management 56 (2019) 1–13
11


6. Conclusions and future work
In this paper, we present an approach to align heterogeneous knowledge bases, which enlarges search space so that more candidate entity pairs can be discovered. AHAB iteratively utilizes different relations for blocking, and candidate entity pairs are generated from matched block pairs. We conduct experiments on real-world heterogeneous knowledge bases to evaluate the effectiveness of AHAB, and the results show AHAB gets better precision, recall, and F-measure than other methods when two large-scale knowledge bases have few aligned relations and properties. Currently, AHAB performs relatively poor when there are a lot of isolated entities in knowledge bases. It is hard to discover these entities as candidate entity pairs by matched entity pairs. A possible approach is to divide all entities into isolated and non-isolated entities, and only use the string representation to find matched entities for isolated entities. Also, due to the lower similarity between blocks at the starting stage, thresholds (e.g., δe and δb) can be designed to be adjustable, i.e., starting with relatively low values and increasing gradually with iterations. In addition, AHAB never detects and amends errors, which would arouse conflicting information, and inconsistent information detection can be employed to redress these errors.
Acknowledgments
This work was funded by China Knowledge Centre for Engineering Sciences and Technology(No. CKCEST-2014-1-5), the National Key Research and Development Program of China (2018YFB0505000).
Supplementary materials
Supplementary material associated with this article can be found, in the online version, at doi:10.1016/j.ipm.2018.08.006.
References
Albagli, S., Ben-Eliyahu-Zohary, R., & Shimony, S. E. (2012). Markov network based ontology matching. Journal of Computer and System Sciences, 78, 105–118. Araujo, S., Tran, D. T., Vries, A. P., & Schwabe, D. (2015). SERIMI: Class-based matching for instance matching across heterogeneous datasets. IEEE Transactions on Knowledge and Data Engineering, 27, 1397–1440.
Aswani, N., Bontcheva, K., & Cunningham, H. (2006). Mining information for instance unification. Proceedings of the 5th international semantic web conference (pp. 329342). . Aumueller, D., Do, H. H., Massmann, S., & Rahm, E. (2005). Schema and ontology matching with COMA++. Proceedings of the 24th ACM SIGMOD international conference on management of data (pp. 906–908). .
Böhm, C., De-Melo, G., Naumann, F., & Weikum, G. (2012). LINDA: Distributed web-of-data-scale entity matching. Proceedings of the 21st ACM international conference on information and knowledge management (pp. 2104–2108). .
Bollacker, K., Evans, C., Paritosh, P., Sturge, T., & Taylor, J. (2008). Freebase: A collaboratively created graph database for structuring human knowledge. Proceedings of the 27th ACM SIGMOD international conference on management of data (pp. 1247–1250). .
Castano, S., Ferrara, A., Lorusso, D., & Montanelli, S. (2008). On the ontology instance matching problem. Proceedings of the 19th international conference on database and expert systems (pp. 180–184). .
Chen, M., Tian, Y., Yang, M., & Zaniolo, C. (2017). Multilingual knowledge graph embeddings for cross-lingual knowledge alignment. Proceedings of the 26th international joint conference on artificial intelligence (pp. 1511–1517). .
Christophides, V., Efthymiou, V., & Stefanidis, K. (2015). Entity resolution in the web of data. Proceedings of synthesis lectures on the semantic web: Theory and technology (pp. 203–204). . Efthymiou, V., Papadakis, G., Papastefanatos, G., Stefanidis, K., & Palpanas, T. (2017). Parallel meta-blocking for scaling entity resolution over big heterogeneous data. Information Systems, 65, 137–157.
Efthymiou, V., Stefanidis, K., & Christophides, V. (2015). Big data entity resolution: From highly to somehow similar entity descriptions in the web. Proceedings of IEEE international conference on big data (pp. 401–410). .
El-Roby, A., & Aboulnaga, A. (2015). ALEX: Automatic link exploration in linked data. Proceedings of the 34th ACM SIGMOD international conference on management of data (pp. 1839–1853). . Essaid, A., Martin, A., Smits, G., & Yaghlane, B. B. (2014). Uncertainty in ontology matching: A decision rule-based approach. Information Processing & Management, 442, 46–55. Glaser, H., Jaffri, A., & Millard, I. (2009). Managing co-reference on the semantic web. Proceedings of the workshop on linked data on the web (pp. 1–6). . Han, B., Chen, L., & Tian, X. (2018). Knowledge based collection selection for distributed information retrieval. Information Processing & Management, 54, 116–128. Hassanzadeh, O., & Consens, M. P. (2009). Linked movie data base. Proceedings of the workshop on linked data on the web (pp. 1–5). . Hassell, J., Aleman-Meza, B., & Arpinar, I. B. (2006). Ontology-driven automatic entity disambiguation in unstructured text. Proceedings of the 5th international semantic web conference (pp. 44–57). .
Hoffart, J., Suchanek, F. M., Berberich, K., & Weikum, G. (2013). YAGO2: A spatially and temporally enhanced knowledge base from wikipedia. Artificial Intelligence, 194, 28–61. Hogan, A., Zimmermann, A., Umbrich, J., Polleres, A., & Decker, S. (2012). Scalable and distributed methods for entity matching, consolidation and disambiguation over linked data corpora. Web semantics: Science, services and agents on the world wide web. 10. Web semantics: Science, services and agents on the world wide web (pp. 76–110). Hu, W., & Jia, C. (2015). A bootstrapping Approach to entity linkage on the semantic web. Web semantics: Science, services and agents on the world wide web. 34. Web semantics: Science, services and agents on the world wide web (pp. 1–12).
Hu, W., Qu, Y., & Cheng, G. (2008). Matching large ontologies: A divide-and-conquer approach. Data & Knowledge Engineering, 67, 140–160. Huber, J., Sztyler, T., Noessner, J., & Meilicke, C. (2011). CODI: Combinatorial optimization for data integration results for OAEI 2011. Proceedings of the 6th international conference on ontology matching (pp. 134–141). .
Isele, R., & Bizer, C. (2012). Learning expressive linkage rules using genetic programming. Proceedings of the 38th VLDB endowment (pp. 1638–1649). . Isele, R., & Bizer, C. (2013). Active learning of expressive linkage rules using genetic programming. Web semantics: Science, services and agents on the world wide web. 23. Web semantics: Science, services and agents on the world wide web (pp. 2–15).
Jean-Mary, Y. R., Shironoshita, E. P., & Kabuka, M. R. (2009). Ontology matching with semantic verification. Web semantics: Science, services and agents on the world wide web. 7. Web semantics: Science, services and agents on the world wide web (pp. 235–251).
Lacoste-Julien, S., Palla, K., Davies, A., Kasneci, G., Graepel, T., & Ghahramani, Z. (2013). Sigma: simple greedy matching for aligning large knowledge bases.
L. Chen et al. Information Processing and Management 56 (2019) 1–13
12


Proceedings of the 19th ACM SIGKDD international conference on knowledge discovery and data mining (pp. 572–580). .
Lehmann, J., Isele, R., Jakob, M., Jentzsch, A., & Kontokostas, D. (2015). DBpedia: A large-scale, multilingual knowledge base extracted from wikipedia. Semantic Web. 6. Semantic Web (pp. 167–195).
Li, J., Tang, J., Li, Y., & Luo, Q. (2009). RiMOM: A dynamic multistrategy ontology alignment framework. IEEE Transactions on Knowledge and Data Engineering, 21, 1218–1232. Medin, D. L., Goldstone, R. L., & Gentner, D. (1993). Respects for similarity. Psychological Review, 100, 254–278. Nagy, M., Vargas-Vera, M., & Stolarski, P. (2009). Dssim results for OAEI 2009. Proceedings of the international workshop on ontology matching 1-1. Naumann, F., & Herschel, M. (2010). An introduction to duplicate detection. Synthesis Lectures on Data Management, 2, 1–87. Navarro, G. (2001). A guided tour to approximate string matching. ACM Computing Surveys, 33, 31–88. Nguyen, K., Ichise, R., & Le, B. (2012). SLINT: A schema-independent linked data interlinking system. Proceedings of the 7th international conference on ontology matching (pp. 1–12). . Niu, X., Rong, S., Zhang, Y., & Wang, H. (2011). Zhishi.links results for OAEI 2011. Proceedings of the 6th international conference on ontology matching (pp. 220–227). . Papadakis, G., Alexiou, G., Papastefanatos, G., & Koutrika, G. (2015). Schema-agnostic vs schema-based configurations for blocking methods on homogeneous data. Proceedings of the 41st VLDB endowment (pp. 312–323). .
Rong, S., Niu, X., Xiang, E., Wang, H., Yang, Q., & Yu, Y. (2012). A machine learning approach for instance matching based on similarity metrics. Proceedings of the 11th international semantic web conference (pp. 460–475). .
Saïs, F., Pernelle, N., & Rousset, M. C. (2009). Combining a logical and a numerical method for data reconciliation. Journal on Data Semantics, 12, 66–94. Schmachtenberg, M., Bizer, C., & Paulheim, H. (2014). Adoption of the linked data best practices in different topical domains. Proceedings of the 3rd international semantic web conference (pp. 245–260). .
Seddiqui, M. H., & Aono, M. (2009). An efficient and scalable algorithm for segmented alignment of ontologies of arbitrary size. Web semantics: Science, services and agents on the world wide web. 7. Web semantics: Science, services and agents on the world wide web (pp. 344–356).
Shao, C., Hu, L., Li, J., Wang, Z., Chung, T., & Xia, J. (2016). RiMOM-IM: A novel iterative framework for instance matching. Journal of Computer Science and Technology, 31, 185–197.
Singla, P., & Domingos, P. (2006). Entity resolution with markov logic. Proceedings of the 6th international conference on data mining (pp. 572–582). . Song, D., & Heflin, J. (2011). Automatically generating data linkages using a domain-independent candidate selection approach. Proceedings of the 10th international semantic web conference (pp. 649–664). .
Stoilos, G., Stamou, G., & Kollias, S. (2005). A string metric for ontology alignment. Proceedings of international semantic web conference (pp. 624–637). . Suchanek, F. M., Abiteboul, S., & Senellart, P. (2011). Paris: Probabilistic alignment of relations, instances, and schema. Proceedings of the 37th VLDB endowment (pp. 157–168). . Wang, J., Kraska, T., Franklin, M. J., & Feng, J. (2012). Crowder: Crowdsourcing entity resolution. Proceedings of the 38th VLDB endowment (pp. 1483–1494). . Whang, S. E., Menestrina, D., Koutrika, G., Theobald, M., & Garcia-Molina, H. (2009). Entity resolution with iterative blocking. Proceedings of the 28th ACM SIGMOD international conference on management of data (pp. 219–232). .
Winkler, W. E. (2006). Overview of record linkage and current research directions. Bureau of the Census, 25, 603–623. Yan, S., Lee, D., Kan, M. Y., & Giles, C. L. (2007). Adaptive sorted neighborhood methods for efficient record linkage. Proceedings of the 7th IEEE-CS joint conference on digital libraries (pp. 185–194). .
Yin, J., Jiang, X., Lu, Z., Shang, L., Li, H., & Li, X. (2016). Neural generative question answering. Proceedings of international joint conference on artificial intelligence (pp. 2972–2978). . Zhu, H., Xie, R., Liu, Z., & Sun, M. (2017). Iterative entity alignment via joint knowledge embeddings. Proceedings of the 26th international joint conference on artificial intelligence (pp. 4258–4264). .
L. Chen et al. Information Processing and Management 56 (2019) 1–13
13