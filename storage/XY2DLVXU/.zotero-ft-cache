IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 5, NO. 2 , MARCH IY94 157
Learning Long-Term Dependencies with Gradient Descent is Difficult
Yoshua Bengio, Patrice Simard, and Paolo Frasconi, Student Member, IEEE
Abstract- Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.
I. INTRODUCTION
E ARE INTERESTED IN training recurrent neural
W
networks to map input sequences to output sequences, for applications in sequence recognition, production, or timeseries prediction. All of the above applications require a system that will store and update context information; i.e., information computed from the past inputs and useful to produce desired outputs. Recurrent neural networks are well suited for those tasks because they have an intemal state that can represent context information. The cycles in the graph of a recurrent network allow it to keep information about past inputs for an amount of time that is not fixed a priori, but rather depends on its weights and on the input data. In contrast, static networks (i.e., with no recurrent connection), even if they include delays (such as time delay neural networks [15]), have a finite impulse response and can’t store a bit of information for an indefinite time. A recurrent network whose inputs are not fixed but rather constitute an input sequence can be used to transform an input sequence into an output sequence while taking into account contextual information in a flexible way. We restrict our attention here to discrete-time systems. Learning algorithms used for recurrent networks are usually based on computing the gradient of a cost function with respect to the weights of the network [22],[21]. For example, the backpropagation through time algorithm [221 is a generalization of back-propagation for static networks in which one stores the activations of the units while going forward in time. The backward phase is also backward in time and recursively uses these activations to compute the required gradients. Other algorithms, such as the forward propagation algorithms [141, [23], are much more computationally expensive (for
Manuscript received April 21, 1993; revised December 21, 1993. Y. Bengio is with Universite de Montreal (Dept. IRO), Montreal. Canada,
P. Simard is with AT&T Bell Laboratories, Holmdel, NJ. P. Frasconi is with Universith di Firenze (Dip. Sistemi e Informatica), Italy. IEEE Log Number 9215775.
and with AT&T Bell Laboratories. NJ.
a fully connected recurrent network) but are local in time; i.e., they can be applied in an on-line fashion, producing a partial gradient after each time step. Another algorithm was proposed [lo], [181 for training constrained recurrent networks in which dynamic neurons-with a single feedback to themselves-have only incoming connections from the input layer. It is local in time like the forward propagation algorithms and it requires computation only proportional to the number of weights, like the back-propagation through time algorithm. Unfortunately, the networks it can deal with have limited storage capabilities for dealing with general sequences [7], thus limiting their representational power. A task displays long-term dependencies if prediction of the desired output at time t depends on input presented at an earlier time 7 << t. Although recurrent networks can in many instances outperform static networks [4], they appear more difficult to train optimally. Earlier experiments indicated that their parameters settle in sub-optimal solutions that take into account short-term dependencies but not longterm dependencies [5]. Similar results were obtained by Mozer [19]. It was found that back-propagation was not sufficiently powerful to discover contingencies spanning long temporal intervals. In this paper, we present experimental and theoretical results in order to further the understanding of this problem. For comparison and evaluation purposes, we now list three basic requirements for a parametric dynamical system that can learn to store relevant state information. We require the following: 1) That the system be able to store information for an arbitrary duration. 2) That the system be resistant to noise (i.e., fluctuations of the inputs that are random or irrelevant to predicting a correct output). 3) That the system parameters be trainable (in reasonable time). Throughout this paper, the long-term storage of definite bits of information into the state variables of the dynamic system is referred to as information latching. A formalization of this concept, based on hyperbolic attractors, is given in Section
The paper is divided into five sections. In Section I1 we present a minimal task that can be solved only if the system satisfies the above conditions. We then present a recurrent network candidate solution and negative experimental results indicating that gradient descent is not appropriate even for such a simple problem. The theoretical results of Section IV show that either such a system is stable and resistant to noise
IV-A.
3045-9227/94$04.00 0 1994 IEEE
Authorized licensed use limited to: University of Massachusetts Amherst. Downloaded on March 21,2024 at 15:29:41 UTC from IEEE Xplore. Restrictions apply.


158 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 5 . NO. 2, MARCH 1994
or, altematively, it is efficiently trainable by gradient descent, but not both. The analysis shows that when trying to satisfy conditions 1) and 2) above, the magnitude of the derivative of the state of a dynamical system at time t with respect to the state at time 0 decreases exponentially as t increases. We show how this makes the back-propagation algorithm (and gradient descent in general) inefficient for learning of long term dependencies in the input/output sequence, hence failing condition 3) for sufficiently long sequences. Finally, in Section V, based on the analysis of the previous sections, new algorithms and approaches are proposed and compared to variants of back-propagation and simulated annealing. These algorithms are evaluated on simple tasks on which the span of the input/output dependencies can be controlled.
11. MINIMALTASKILLUSTRATING THE PROBLEM
The following minimal task is designed as a test that must necessarily be passed in order to satisfy the three conditions enumerated above. A parametric system is trained to classify two different sets of sequences of length T .For each sequence ~ 1 , ... ,UT the class C ( u 1 , . .. ,U T ) E ( 0 , l ) depends only on the first L values of the external input:
C ( U 1 , . ..,UT)= C ( U 1 , . . . ,U L ) .
We suppose L fixed and allow sequences of arbitrary length T >> L. The system should provide an answer at the end of each sequence. Thus, the problem can be solved only if the dynamic system is able to store information about the initial input values for an arbitrary duration. This is the simplest form of long-term computation that one may ask a recurrent network to carry out. The values U L + ~ ,. . . ,UT are irrelevant for determining the class of the sequences. However, they may affect the evolution of the dynamic system and eventually erase the intemally stored information about the initial values of the input. Thus the system must latch information robustly, i.e., in such a way that it cannot be easily deleted by events that are unrelated to the classification criterion. We assume here that for each sequence, ut is zero-mean Gaussian noise for t > L. The third required condition is leamability. There are two different computational aspects involved in this task. First, it is necessary to process ~ 1. .,.I U L in order to extract some information about the class; i.e., to perform classification. Second, it is necessary to store such information into a subset of the state variables (let us call them latching state variables) of the dynamic system, for an arbitrary duration. For this task, the computation of the class does not require accessing latching state variables. Hence the latching state variables do not need to affect the evolution of the other state variables. Therefore, a simple solution to this task may use a latching subsystem, fed by a subsystem that computes information about the class. We are interested in assessing learning capabilities on this latching problem independently on a particular set of training sequences; i.e. in a way that is independent of the specific problem of classifying u 1 , . . . ,U L . Therefore we will focus here only on the latching subsystem. In order to train any module feeding the latching subsystem, the learning algorithm
should be able to transmit error information (such as gradient) to such a module. An important question is thus whether the leaming algorithm can propagate error information to a module that feeds the latching subsystem and detects the events leading to latching. Hence, instead of feeding a recurrent network with the input sequences defined as above we use only the latching subsystem as a test system and we reformulate our minimal task as follows. The test system has one input ht and one output zt (at each discrete time step t).The initial inputs ht, for t 5 L, are values which can be tuned by the learning algorithm (e.g., gradient descent) whereas ht is Gaussian noise for L < t 5 T . The connection weights of the test system are also trainable parameters. Optimization is based on the cost function
where p is an index over the training sequences and d p is a target of $0.8 for sequence of class 1 and -0.8 for sequences of class 0. In this formulation, ht (t = 1,.. . I L ) represent the result of the computation that extracts the class information. Learning ht directly is an easier task than computing it as a parametric function ht(ut,6) of the original input sequence and learning the parameters 19.In fact, the error derivatives (as used by backpropagation through time) are the same as if ht were obtained as a parametric function of ut. Thus, if ht cannot be directly trained as parameters in the test system (because of vanishing gradient), they clearly cannot be trained as a parametric function of the input sequence in a system that uses a trainable module to feed a latching subsystem. The ability of leaming the free input values h l , . .. ILL is a measure of the effectiveness of the gradient of error information that would be propagated further back if the test system were connected to the output of another module.
111. SIMPLERECURRENTNETWORCKANDIDATSEOLUTION
We performed experiments on this minimal task with a single recurrent neuron, as shown in Fig. l(a). Two types of trajectories are considered for this test system, for the two classes (IC = 0, k = 1):
z: = f ( @ )= tanh(a:)
.o- 1 
0 - a0 - 0.
l/f’(0)
a : = w f ( a : - l ) + h : t = l . . . T (1)
If w > = 1, then the autonomous dynamic of this neuron has two attractors ?E > 0 and -5 that depend on the value of the weight w [7], [8] (they can be easily obtained as non zero intersections of the curve 5 = tanh(a) with the line z = a / w ) .Assuming that the initial state at t = 0 is :CO = -?E, it can be shown [8] that there exists a value h* > 0 of the input such that 1) xt maintains its sign if lhtl < h* Vt , and,
2) there exists a finite number of steps L1 such that z ~ >, 5 if ht > h* V t 5 L1. A symmetric case occurs for 20 = -?E. h* increases with w. For fixed w , the transient length L1 decreases with Ihtl. Thus the recurrent neuron of Fig. l(a) can robustly latch one bit of information, represented by the sign of its
Authorized licensed use limited to: University of Massachusetts Amherst. Downloaded on March 21,2024 at 15:29:41 UTC from IEEE Xplore. Restrictions apply.


BENGIO er al.: LEARNING LONG-TERM DEPENDENCIES WITH GRADIENT DESCENT IS DIFFICULT 159
Fig. 1. (a) Latching recurrent neuron. (b) Sample input to the recurrent neuron. The trainable values (marked in bold) have been tuned by one of the successfully learning simulations.
lhl
Freq .
WO
Fig. 2. Experimental results for the minimal problem. (a) Density of training convergence with respect to the initial weight (iv) and the noise variance .s (white high density), with L = 3 and T = 20. (b) Frequency of training convergence with respect to the sequence length T , (with noise variance .$ = 0 . 2 , and initial weight WO = 1.25).
activation. Storing is accomplished by keeping a large input (i.e., larger than h* in absolute value) for a long enough time. Small noisy inputs (i.e., smaller than h* in absolute value) cannot change the sign of the activation of the neuron, even if applied for an arbitrary long time. This robustness essentially depends on the nonlinearity. The recurrent weight w is also trainable. The solution for T >> L requires w > 1 to produce two stable attractors C and -3.Larger U) correspond to larger critical value /I* and, consequently, more robustness against noise. The trainable input values must bring the state of the neuron towards C or -T in order to robustly latch a bit of information against the input noise. For example, this can be accomplished by adapting, for t = 1,... L , hi 2 H and 11; 5 - H , where
H > h* controls the transient duration towards one of the two attractors. In Fig. l(b) we show two sample sequences that feed the recurrent neuron. As stated in Section 11, h: are trainable for t 5 L and samples from a Gaussian distribution with mean 0 and variance .Y for t > L. The values of ht for t 5 L were initialized to small uniform random values before starting training. A set of simulations were camed out to evaluate the effectiveness of back-propagation (through time) on this simple task. In a first experiment we investigated the effect of the noise variance s and of different initial values 7ug for the self loop weight (see also [3]). A density plot of convergence is shown in Fig. 2(a), averaged over 18 runs for each of the selected pairs (wols ) . It can be seen that convergence becomes very unlikely for large noise variance or small initial values of w.L = 3 and T = 20 were chosen in these experiments.
In Fig. 2(b), we show instead the effect of varying T , keeping fixed s = 0.2 and WO = 1.25. In this case the task consists in learning only the input parameters I L ~ .As explained in Section 11, if the learning algorithm is unable to properly tune the inputs ht, then it will not be able to leam what should trigger latching in a more complicated situation. Solving this task is a minimum requirement for being able to transmit error information backward, towards modules feeding the latch unit. When T becomes large it is extremely difficult to attain convergence. These experimental results show that even in the very simple situation where we want to robustly latch on one bit of information about the input, gradient descent on the output error fails for long-term input/output dependencies, for most initial parameter values.
Iv. LEARNINTGO LATCHWITH DYNAMICASLYSTEMS
In this section, we attempt to understand better why leaming even simple long-term dependencies with gradient descent appears to be so difficult. We discuss the general case of a realtime recognizer based on a parametric dynamical system. We find that the conditions under which a recurrent network can robustly store information (in a way defined below, i.e., with hyperbolic attractors) yield a problem of vanishing gradients that can make leaming very difficult. We consider a non-autonomous discrete-time system with additive inputs:
(2)
at = M ( U - 1 ) + Ut
and the corresponding autonomous dynamics
where M is a nonlinear map, and at and ut are mvectors representing respectively the system state and the extemal input at time t. To simplify the analysis presented in this section, we consider only a system with additive inputs. However, a dynamic system with non-additive inputs, e.g., at = N ( a t - l ! ut--l),can be transformed into one with additive inputs by introducing additional state variables and corresponding inputs. Suppose at E R" and ut E R'". The new system is defined by
the additive inputs dynamics ai = N ' ( a ; - l ) + U: where
(14 = ( a t ,?iits)a n + 7ri-vector state, and the first 71 elements of U: = ( 0 , ~ E~ )R n f mare 0. The new map N' can be defined in terms of the old map N as follows: N ' ( a - l ) = ( N ( u t - l , y t - l ) , O ) , with 7r1, zeroes for the last elements of N ' ( ) .Hence we have yt = u t . Note that a system with additive inputs with a map of the form of N I ( ) can be transformed back into an equivalent system with non-additive inputs. Hence without loss of generality we can use the model in ( 2 ) . In the next subsection, we show that only two conditions can arise when using hyperbolic attractors to latch bits of information. Either the system is very sensitive to noise, or the derivatives of the cost at time t with respect to the system activations (LO converge exponentially to 0 as t increases. This situation is the essential reason for the difficulty in using gradient descent to train a dynamical system to capture long-term dependencies in the input/output sequences.
Authorized licensed use limited to: University of Massachusetts Amherst. Downloaded on March 21,2024 at 15:29:41 UTC from IEEE Xplore. Restrictions apply.


160 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 5 , NO. 2, MARCH 1994
A. Analysis
In order to latch a bit of state information one wants to restrict the values of the system activity at to a subset S of its domain. In this way, it will be possible to later interpret at in at least two ways: inside S and outside S. To make sure that ut remains in such a region, the system dynamics can be chosen such that this region is the basin of attraction of an attractor (or of an attractor in a sub-manifold or subspace of u t ’ s domain). To “erase” that bit of information, the inputs may push the system activity at out of this basin of attraction and possibly into another one. In this section, we show that if the attractor is hyperbolic (or can be transformed into one; e.g.,
a stable periodic attractor), then the derivatives 2 quickly
vanish as t increases. Unfortunately, when these gradients vanish, training becomes very difficult because the influence of short-term dependencies dominates in the weights gradient. Definition I : A set of points E is said to be invariant under a map M if E = M ( E ) .
Definition 2: A hyperbolic attractor is a set of points X invariant under the differentiable map M , such that Vo, E X , all eigenvalues of M’(w) are less than 1 in absolute value. An attractor X may contain a single point (fixed point attractor), a finite number of points (periodic attractor), or an infinite number of points (chaotic attractor). Note that a stable and attracting fixed point is hyperbolic for the map M , whereas a stable and attracting periodic attractor of period 1 for the map M is hyperbolic for the map M‘. For a recurrent net, the kind of attractor depends on the weight matrix. In particular, for a network defined by at = W tanh(at-l) +ut , if W is symmetric and its minimum eigenvalue is greater than -I,then the attractors are all fixed points [17]. On the other hand, if IWI < 1 or if the system is linear and stable, the system has a single fixed point attractor at the origin.
Definition 3: The basin of attt-uction of an attractor X is the set / j ( X ) of points U converging to X under the map M ;
i.e., / j ( X ) = { u : Ve, I1,Ix:E x s.t. IIM‘(a) - :cII < t}. Dqfinitioiz 4: We call I‘(X), the reduced attracting set of a hyperbolic attractor X , the set of points y in the basin of attraction of X , such that V l 2 1, all the eigenvalues of ( M ‘ ) ’ ( g )are less than I . Note that by definition, for a hyperbolic attractor X , x’ C
Definition 5: A system is robustly latched at time t o to X, one of several hyperbolic attractors, if at,, is in the reduced attracting set of X under a map M defining the autonomous system dynamics. For the case of non-autonomous dynamics, it remains robustly latched to X as long as the inputs ut are such that ut E T(X) for t > to. Let us now see why it is more robust to store a bit of information by keeping at in T‘(X),the reduced attracting set of X . Theorem I : Assume 3: is a point of RrLsuch that there exists an open sphere U ( z )centered on 5 for which IM’(z)l > 1 for all z E U(:[;).Then there exist y E U ( z ) such that \IM(.c)- A i l ( y ) ( ( > ((:c - yI(.
qx’) c ij(x).
Proof: See the Appendix. This theorem implies that for a hyperbolic attractor X , if
B
IM’I>l
Domain of a, . (b) 
Fig. 3. Basin of attraction (3).reduced attracting set (r)of an attractor X.
Ball of uncertainty grows exponentially (a) outside I?, but is bounded (b)
inside r.
a0 is in B ( X ) but not in I‘(X), then the size of a ball of uncertainty around a0 will grow exponentially as t increases, as illustrated in Fig. 3(a). Therefore, small perturbations in the input could push the trajectory towards another (possibly wrong) basin of attraction. This means that the system will not be resistant to input noise. What we call input noise here may be simply components of the inputs that are not relevant to predict the correct future outputs. In contrast, the following results show that if no is in r ( X ) ,at is guaranteed to remain within a certain distance of X when the input noise is bounded. Definition 6: A map M is contracting on a set D if 3 Q E
[O. 1) such that I M ( z )- M(y)I I allx - yII Vz,y E D.
Theorem 2: Let M be a differentiable mapping on a convex set D.If Vx E D ,IM’(z)l < 1,then M is contracting on D.
A crucial element in this analysis is to identify the conditions in which one can robustly latch information with an attractor. Theorem 3: Suppose the system is robustly latched to X , starting in state ao, and the inputs ut are such that for all
t > 0, IIull < bt, where bt = (1 - &)d. Let at be the autonomous trajectory obtained by starting at a0 and no input U . Also suppose Vv E D t , IM’(y)I < At < 1, where Dt is a ball of radius d around at. Then at remains inside a ball of radius d around lit, and this ball intersects X when t + CO.
Proof: See [20].
Prory? See the Appendix. The above results justify the term “robust” in our definition of robustly latched system: as long as at remains in the reduced attracting set r ( X ) of a hyperbolic attractor X , a bound on the inputs can be found that guarantees at to remain within a certain distance of some point in X, as illustrated in Fig. 3(b). The smaller IM’(y)l is in the region around at, the looser the bound bt is on the inputs, meaning that the system is more robust to input noise. On the other hand, outside r ( X )but in B ( X ) ,M is not contracting, it is expanding; i.e., the size of a ball of uncertainty grows exponentially with time.
Authorized licensed use limited to: University of Massachusetts Amherst. Downloaded on March 21,2024 at 15:29:41 UTC from IEEE Xplore. Restrictions apply.


BENGIO er al.: LEARNING LONG-TERM DEPENDENCIES WITH GRADIENT DESCENT IS DIFFICULT 161
We now show the consequences of robust latching; i.e., vanishing gradient. Theorem 4: If the input ut is such that a system remains
robustly latched on attractor X after time 0, then 2 -+ 0
as t -+ os. Proof: See the Appendix. The results in this section thus show that when storing one or more bit of information in a way that is resistant to noise, the gradient with respect to past events rapidly becomes very small in comparison to the gradient with respect to recent events. In the next section we discuss how that makes gradient descent on parameter space (e.g., the weights of a network) inefficient.
B. Effect on the Weight Gradient
Let us consider the effect of vanishing gradients on the derivatives of a cost Ct at time t with respect to parameters of a dynamical system, say a recurrent neural network with weights W :
Suppose we are in the condition in which the network has
robustly latched. Hence for a term with 7 << t , I e I 4 0.
This term tends to become very small in companson to terms for which 7- is close to t. This means that even though there might exist a change in W that would allow a, to jump to another (better) basin of attraction, the gradient of the cost with respect to W does not reflect that possibility. This is because the effect of a small change in W would be felt mostly on the near past (7 close to t ) . Let us see an example of how this result hampers training a system that requires robust latching of information. Consider for example a system made of two sub-systems A and B with the output of A being fed to the input of B. Suppose that any good solution to the leaming problem requires B storing information about events detected by A at time 0, with the output of B at a later distant time T used to compute an error, as in our minimal problem defined in Section 11. If B has not been trained enough to be able to store information for a long time, then gradients of the error at T with respect to the output of A at time 0 are very small, since B doesn’t latch and the outputs of A at time 0 have very little influence on the error at time T . On the other hand, as soon as B is trained enough to reliably store information for a long time, the right gradients can propagate; but because they quickly vanish to very small values, training A is very difficult (depending of the size of T and the amount of noise between 0 and T).
V. ALTERNATIVEAPPROACHES
The above section helped us understand better why training a recurrent network to leam long range input/output dependencies is a hard problem. Gradient-based methods appear inadequate for this kind of problem. We need to consider alternative systems and optimization methods that give acceptable results even when the criterion function is not smooth and has long plateaus. In this section we consider several altemative
optimization algorithms for this purpose, and compare them to two variants of back-propagation. One way to help in the training of recurrent networks is to set their connectivity and initial weights (and even constraints on the weights) using prior knowledge. For example, this is accomplished in [8] and [1 11using prior rules and sequentiality constraints. In fact, the results in this paper strongly suggest that when such prior knowledge is given, it should be used, since the leaming problem itself is so difficult. However, there are many instances where many long-term input/output dependencies are unknown and have to be leamed from examples.
A. Simulated Annealing
Global search methods such as simulated annealing can be applied to such problems, but they are generally very slow. We implemented the simulated annealing algorithm presented in [6] for optimizing functions of continuous variables. This is a “batch leaming” algorithm (updating parameters after all examples of the training set have been seen). It performs a cycle of random moves, each along one coordinate (parameter) direction. Each point is accepted or rejected according to the Metropolis criterion [ 131. New points are selected according to a uniform distribution inside a hyperrectangle around the last point. The dimensions of the hyperrectangle are updated in order to maintain the average percentage of accepted moves at about one-half of the total number of moves. After a certain number of cycles, the temperature is reduced by a constant multiplicative factor (0.85 in the experiments). Training stops when some acceptable value of the cost function is attained, when learning gets “stuck,”’ or if a maximum number of function evaluations is performed. A “function evaluation” corresponds to performing a single pass through the network, for one input sequence.
B. Multi-Grid Rundom Search
This simple algorithm is similar to the simulated annealing algorithm. Like simulated annealing, it tries random points. However, if the main problem with the learning tasks was plateaus (rather than local minima), an algorithm that accepts only points that reduce the error could be more efficient. This algorithm has this property. It performs a (uniform) random search in a hyperrectangle around the current (best) point. When a better point is found, it reduces the size of the hyperrectangle (by a factor of 0.9 in the experiments) and re-centers it around the new point. The stopping criterion is the same as for simulated annealing.
C. Time-Weighted Pseudo-Newton Optimization
The pseudo-Newton algorithm [2] for neural networks has the advantage of re-scaling the leaming rate of each weight dynamically to match the curvature of the energy function with respect to that weight. This is of interest because adjusting the leaming rate could potentially circumvent the problem of
‘When the cost value on the last S,points does not change by more than 6 (a small constant) and these values are all within f of the current optimal cost value found by the algorithm. In the experiments, 6 = 0.001 and .V, = -1.
Authorized licensed use limited to: University of Massachusetts Amherst. Downloaded on March 21,2024 at 15:29:41 UTC from IEEE Xplore. Restrictions apply.


162
Ayi =
IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 5, NO. 2, MARCH 1994
2 if xi < 0 and zi + Azi 2 0
- 2 if z; 2 0 and zi + Azi < 0 0 otherwise
(8)
vanishing gradient. The pseudo-Newton algorithm computes a diagonal approximation to the Hessian matrix (second derivatives of the cost with respect to the parameters) and updates parameters according to the following on-line rule:
where witwhere Aw;(p) is the update for weight w ; after pattern p has been presented, C ( p ) is the cost for pattern p , and and 71 are small positive constants. This amounts to computing a local learning rate for each parameter by using the inverse of the second derivative with respect to each parameter as a normalizing factor. When 1 ' ~ ~ )isl small, the curvature is small (around the current value of w ) in the direction corresponding to the wi axis. Hence a larger step can be taken in that direction. This algorithm was tested in the experiments described in Section V-E. It consistently performs better than standard back-propagation, but still fails more and more as we increase the span of input/output dependencies. This algorithm and our theoretical results in Section 1V inspired the following time-weighted pseudo-Newton algorithm. The basic idea is to consider the unfolding of the recurrent network in time, and each instantiation of a weight (at different times) as a separate variable, albeit with the constraint that these now separate variables should be equal. To simplify the problem, we consider here a cost C ( p )that depends on the output of the network at the final time step of sequence p . Hence the weight update for w;can be computed as follows:
where wit is the instantiation for time t of parameter 711;. In this way, each (temporal) contribution to Auii ( p ) is weighted by the inverse curvature with respect to wit, the instantiation of parameter wi at time t.* The reader may compare the above equation with (4), where all the temporal contributions are uniformly summed. Consequently, updating tu according to (6) does not actually follow the gradient [but neither would following (5)].Instead, several gradient contributions are weighted using second derivatives, in order to make faster moves in the flatter directions. As for the pseudo-Newton algorithm of [ 2 ] , we prefer using a diagonal approximation of the Hessian that is cheap to compute and guaranteed to be positive. 71 is a global learning rate (0.01 in our experiments). The constant p is introduced to prevent Aiii
from becoming very large (when I"iEt)iis very small). However, we found that much better performance can be attained with the recurrent networks when LL is adapted online. This prevents the maximum Aw from being greater than a certain upper bound (0.3 in the experiments) or smaller than a certain lower bound (0.001 in the experiments). The constant ,u is updated with a "momentum" term (0.8 in the experiments), in order to prevent it from decreasing too rapidly when the first and second derivatives vary widely from sequence to sequence
'The idea of using second derivatives in this way was inspired from discussions with L. Bottou.
and have very small magnitude (for example when the norm of the weight matrix IWI is less than 1).
Authorized licensed use limited to: University of Massachusetts Amherst. Downloaded on March 21,2024 at 15:29:41 UTC from IEEE Xplore. Restrictions apply.


BENGIO er al.: LEARNING LONG-TERMDEPENDENCIES WITH GRADIENT DESCENT IS DIFFICULT 163
and from this equation we can compute the desired variation Axz(Ayz)of z, when the desired variation of y, is Ay,:
6 - x,
/U otherwise
if Ay, = 2 Axz = - E - x, if Ay, = -2 (9)
where t is a positive constant. We now denote by Ayz and Axz the desired changes in y, and x, respectively. Let C be a cost function on our system when a certain pattern (sequence)
is presented. A “pseudo-gradient’’ E should reflect the influence of a change of x, on the cost C. In our experiment
we set to & if Ayz # 0 and 0 otherwise. To use the “pseudo gradient” we must insure that Ay, is in(2, -2,0} since Az:,(Ay,) is not defined for other values. This is achieved using a stochastic process. Let’s assume that there exist two constants M I N and M A X such that the error signal = gz to be backpropagated is a real number satisfying MI%J 5 gz 5 M A X . We define the stochastic function Ay, = S(g2),which maps g, to {-2,2} as follows.
Ax,
Provided that -2 < M I N and M A X < 2, it is easy to show that the expectation of S ( g i ) is exactly gi, even though
S(gi) can only take two values (if 1g;I > 2 the resulting expected value will be -2 or +2). Furthermore the sum of this “pseudo gradient” over several patterns quickly converges to the sum of the continuous valued gi’s. The non-linear threshold unit can be used in combination with any other differentiable elements that backpropagate the gradient in the usual fashion. The important point is that when a non-linear threshold unit is connected to itself in a loop with a positive gain, two stable fixed points are induced. The “pseudo gradient” along this loop doesn’t vanish with time, which is the essential reason for using discrete units. This pseudo-gradient doesn’t vanish along the loop, as can be observed by repetitively applying (8) and (9) and noting that if the pseudo-gradient is large enough in magnitude then it is always propagated. This approach is in no way optimal and many other discrete error propagationalgorithms are possible. Another very promising approach for instance is the trainable discrete flipflop unit [3] which also preserves error information in time. Our only claim here is that discrete propagation of error offers interesting solutions to the vanishing gradient problem in recurrent network. Our preliminary results on toy problems (see next subsection and [3]) confirm this hypothesis.
E. Experimental Results
Experiments were performed to evaluate various altemative optimization approaches on problems on which one can increase the temporal span of input/output dependencies. Of course, when it is possible, first training on shorter sequences helps a lot, but in many problems no such “short-term” version of the problem is available. Hence a goal of these experiments was to measure how these algorithms can perform when it is
FinalClassificalionEncf,LalchProblem I Seqwnce Presenlations,Latch Problem
.In*, 1W.l lull UI 1m1 11.1 Y.l U, mb
1111.
T nan T
l.llLl
FiMlClassificationError,2-Sequence Problem #SequencePnsenlalions,Z-Sequnw Problem
]-.”,,,
”:I- a,”.,
F i M l ClasslieationError, Parity Problem
l r I SequencePrae8nbUons,ParityProblem
“u.l-f--
Fig. 4. Comparative simulation results for: 0 standard back-propagation, v
pseudo-Newton, A time-weighted pseudo-Newton, 0discrete error propa
gation, multi-grid random search, 0 simulated annealing. The horizontal axis (T)represents maximum sequence length. On the left, the vertical axis
represents classification error after training; on the right, the number of sequence presentations to reach a stopping criterion.
not possible to train using sequences with equivalent shortterm dependencies. Experiments were performed with and without input noise (uniformly distributed in [-0.2,0.2]) and varying the length of the input/output sequences. The criteria by which the performance of these algorithms were measured are (I) the average classification error at the end of training, i.e., after the stopping criterion has been met (when either some allowed number of function evaluations has been performed or the task has been learned), (2) the average number of function evaluations needed to reach the stopping criterion. Experiments were performed on three problems: the Latch problem, the 2-Sequence problem, and the Parity problem. For each of these problems, a suitable architecture was chosen and all algorithms were used to search in the resulting parameter space (except that the discrete error propagation algorithm used hard threshold neurons instead of symmetric sigmoids). Initial parameters of the networks were randomly generated for each trial (uniformly between -0.5 and 0.5). The choice of inputs and the noise for each training sequence was also randomly generated for each trial. The same initial conditions and training set were used with each of the algorithms (at a given trial). For each trial, a training set was generated with sequences whose length is uniformly distributed between T / 2 and T .The number T (maximum sequence length) is displayed in Fig. 4. The tasks all involved a single input and a single output at each time step. 1 )Latch Problem: The Latch problem is the same as described above in Section 111. Here we considered only three adaptive parameters: the self-loop weight w, the initial input value u1 for “positive” sequences (with positive final target),
Authorized licensed use limited to: University of Massachusetts Amherst. Downloaded on March 21,2024 at 15:29:41 UTC from IEEE Xplore. Restrictions apply.


I64 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 5 . NO. 2. MARCH 1994
and the initial input value uo for “negative” sequences (with negative final target). The network thus had only one unit. 2) 2-Sequence Problem: The 2-Sequence problem is the following: to classify an input sequence as one of two sequences when given the first N elements of this sequence. N varies from pattern to pattem and noise may be added to the input. Hence the network can’t rely on the last particular values it saw. Instead, early on, it must recognize subsequences belonging to one of the two classes and store that information (or update it if conflicting information arrives) until its output is read out (which may be at any time but is done only once per sequence). These initial key subsequences were randomly generated from a uniform distribution in [ - I , I]. In all experiments we used a fully connected recurrent network with five units and no bias (one of the units received external additive input; i.e., the network has 25 free param
eters). .
3) Parity Problem: The Parity problem consists in producing the parity of an input sequence of 1’s and -1’s (i.e., a 1 should be produced in output if and only if the number of 1’s in the input is odd). The target is only given at the end of the sequence. The length of the sequence may vary and the input may be noisy. It is a difficult problem that has local minima (like the XOR problem), and that appears more and more difficult for longer sequences. Most local optimization algorithms tend to get stuck in a local minimum for many initial values of the parameters. The minimal size network that we implemented has 7 free parameters and 2 units ( 2 inputs connected to 1 hidden and 1 output units). Although it requires less parameters than the 2-Sequence problem, it is a more difficult learning problem. The results displayed in Fig. 4 can be summarized as follows: 1) Although simulated annealing performed well on all problems, it requires an order of magnitude more training time than all the other algorithms. This is not surprising since it is global search algorithm. The multigrid algorithm is faster but fails on the Parity problem, probably because of local minima. It is also interesting to note that on the Latch problem with simulated annealing, training time increases with sequence length. Although the best solution is the same for all sequence lengths, the error surface for longer sequences could be more difficult to search, even for simulated annealing. 2) The discrete error propagation algorithm performed reasonably well on all the problems and sequence lengths, and was the only one with simulated annealing that could solve the Parity problem. Because it performs an on-line local search it is, however, much faster than simulated annealing. It seems to be more robust to local minima than the multi-grid random search. 3) The pseudo-Newton back-propagation algorithm consistently performs better than the standard backpropagation. However, both see their performance worsen when the temporal span of input/output dependencies increasing. 4) The time-weighted pseudo-Newton algorithm appears to perform better than the other two variants of back
propagation but its performance also appears to worsen with increasing sequence length.
VI. CONCLUSION
Recurrent networks are very powerful in their ability to represent context, often outperforming static networks [4]. However, we have presented theoretical and experimental evidence showing that gradient descent of an error criterion may be inadequate to train them for tasks involving long-term dependencies. Assuming hyperbolic attractors are used to store state information, we found that either the system would not be robust to input noise or would not be efficiently trainable by gradient descent when long-term context is required. Note that the theoretical results presented in this paper hold for any error criterion and not only for the mean square error criterion. Two simple generalizations are obtained as follows. As mentioned in the analysis section, a periodic attractor can be transformed into a fixed point by subsampling time with the period of the attractor. Hence, if the corresponding fixed point is stable, it is also hyperbolic and our results hold in that case as well. Another interesting case is the situation in which the system doesn’t remain long near an attractor, but rather, jumps rapidly from one stable (hyperbolic) attractor to another. This would arise for example if the continuous dynamics can be made to correspond to the discrete dynamics of a deterministic finite-state automaton. In that case, our results hold as well since the norm of Jacobian of the map derivatives near each
of the attractor is less than one ( = eM’(at-1)). What
remains to be shown is that similar problems occur with chaotic attractors; i.e., that either the gradients vanish or the system is not robust to input noise. It is interesting to note that related problems of vanishing gradient may occur in deep feedforward networks (since a recurrent network unfolded in time is just a very deep feedforward network with shared weights). The result presented here does not mean that it is impossible to train a recurrent network on a particular task. It says that gradient descent becomes increasingly inefficient when the temporal span of the dependencies increases. Furthermore, for a given problem, there are sometimes ways to help the training by setting the network connectivity and initial weights (and even constraints on the weights) using prior knowledge (e.g., [8], [ I 11). For some tasks, it is also possible to present a variety of examples of the input/output dependencies, including shortterm dependencies that are sufficient to infer similar but longer-term dependencies. For example, in the Latch problem or the Parity problem, if we start by training with short sequences, the system rapidly settles in the correct region of parameter space. A better understanding of this problem has driven us to design alternative algorithms, such as the time-weighted pseudoNewton and the discrete error propagation algorithms. In the first case, we consider the instantiation of the weights at different times as different variables and consider the curvature of the cost function for these variables. This information is used to weight the gradient contributions for the different times in such a way as to make larger steps in directions where the cost
Authorized licensed use limited to: University of Massachusetts Amherst. Downloaded on March 21,2024 at 15:29:41 UTC from IEEE Xplore. Restrictions apply.


BENGIO et al.: LEARNING LONG-TERM DEPENDENCIES WITH GRADIENT DESCENT IS DIFFICULT I65
function is flatter. The discrete error propagation algorithm propagates error information through a mixture of discrete and continuous elements. The gradient is locally quantized with a stochastic decision rule that appears to help the algorithm in locally searching for solutions and getting out of local minima. We have compared these algorithms with standard optimization algorithms on toy tasks on which the temporal span of the input/output dependencies could be controlled. The very preliminary results we obtained are encouraging and suggest that there may be ways to reconcile leaming with storing. Good solutions to the challenge presented here to learning long-term dependencies with dynamical systems such as recurrent networks may have implications for many types of applications for learning systems; e.g., in language-related problems, for which long-term dependencies are essential in order to make correct decisions.
VII. APPENDIX
A. Proof of Theorem 1
By hypothesis and definition of norm, 3 U s.t. [lull = 1 and llM’(1~)u11> 1. The Taylor expansion of M at IC for small value of X is:
M(IC + Xu) = M ( z )+ M’(IC)Xu+ O(llXu112). (11)
Since ~ ( z i)s an open set, 3 x s.t. llO(llX~11~)11<
A( IIM’(z)uJ(- 1)and IC +Xu E U ( x ) .Letting y = IC+ Xu we
can write l I ~ ( y )- ~ ( z-) M’(x)Xull = llO(llXz~11~)11< XllM‘(x)uII - X or -IIM(y) - M ( z ) - M’(z)Xull +I~M’(Ic)X>UAJ.( This implies using the triangle inequality I I M ( Y ) - M(IC)ll > = llz - Y11.0
B . Proof of Theorem 3
= { a : llkt - all < p t } in which we are sure to find at, where kt gives the trajectory of the autonomous system. Let us suppose that at time t , p t < d (this is certainly true at time 0, when 60= ao). By Lagrange’s mean value theorem and convexity
of Dt, 32 E Dt S.t. llM($)- M(y))II l M ’ ( z ) l l l ~-~ ~ 1 1 ,but
IM’(z)I < At by hypothesis. Then by the contraction theorem
[20] we have pt+l 5 Xtd + bt. Now by hypothesis we have
bt = (1 - Xt)d, so pt+l < d. The conclusion of the theorem is then obtained since Et E Dt by our construction above and kt converges to X for t -+ 00. 0
Let us denote by pt the radius of the “uncertainty” ball
C. Proof of Theorem 4
By hypothesis and definitions 4 and 2,
for r > 0, hence 2 -+ 0 as t -+ 00 0.
One could however ask what happens when at remains near the boundary between two basins:
Lemma: Suppose that for t > 0, a0 and ut are such that at remains on the boundary between two basins of attraction for attractors X1 and X z , and there exists an infinitesimal change in a0 yielding the state into either X I or X2 and remaining there. Then
It appears that the hypotheses of this lemma will rarely be satisfied, for two reasons. First, the system evolves in discrete time, making it improbable to obtain at precisely on the boundary surface. Second, in order to stay on that surface, say S ( a t ) = 0, ut must satisfy the equation S ( M ( a t - l ) + u t ) = 0. Hence the submanifold of values of ut in R” that satisfy this
equation has dimension m - 1, thus having null measure.
D.Generalization to a Projection of the State
The results obtained so far can be generalized to the case when a projection Pat of the state at converges to an attractor under a map M . This would be the case, for example, when a subset of the hidden units in a recurrent network participate directly in the dynamics of a stable attractor. Let P and R be orthogonal projection matrices such that
at = P+zt + R+yt
zt = Pat;yt = Rat (12) PR+ = 0;RP+ = 0
where A+ denotes the right pseudo-inverse of A; i.e. AA+ = I . Suppose M is such that P can be chosen so that zt converges to an attractor 2 with the dynamics zt = Mp(zt--l) =
P t M ( P f z t + R+yt) for any yt. Then we can specialize all the previous definitions, lemmas, and theorems to the subspace spanned by P . When we conclude with these results that
%
-+ 0 , we can infer that 2 -+ R+%R ; i.e., that the derivatives of at with respect to a0 depend only on the projection of a on the subspace Ra. Hence the influence of changes in the projection of a on the subspace P a is ignored in the computation of the gradient with respect to W ,even though non-infinitesimal changes in P a could yield very different results (i.e., jumping into a different basin of attraction). Although training can now proceed in some directions, the effect of parameters that influence detecting and storing events for the long-term or switching between stable states is still not taken very much into account.
REFERENCES
P. L. Bartlett, and T. Downs, “Using Random Weights to train Multilayer Networks of Hard-Limiting Units.” ZEEE Transactions on Neural Networks, vol. 3, no. 2, 1992, pp. 202-210. S. Becker and Y. Le Cun, “Improving the convergence of backpropagation learning with second order methods,” Proceedings of rhe 1988 Connectionist Models Summer School, Touretzky, Hinton, and Sejnowski, Eds. San Matteo, CA: Morgan Kaufmann, pp. 29-37. Y. Bengio, P. Frasconi, P. Simard, “The problem of leaming longterm dependencies in recurrent networks,” invited paper at the IEEE International Conference on Neural Networks 1993, San Francisco, IEEE Press, pp. 1 1 83-1 188, Y. Bengio, “Artificial neural networks and their application to sequence recognition,” Ph.D. thesis, McGill University, 1991, Montreal, Quebec, Canada.
Authorized licensed use limited to: University of Massachusetts Amherst. Downloaded on March 21,2024 at 15:29:41 UTC from IEEE Xplore. Restrictions apply.


166 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 5, NO. 2. MARCH 1994
[5] Y. Bengio, R. De Mori, G. Flammia, and R. Kompe, “Global optimization of a neural network-Hidden Markov model hybrid,” IEEE Transactions on Neural Networks, vol. 3, no. 2, 1992, pp, 252-259.
[6] A. Corana, M. Marchesi, C. Martini, and S. Ridella, “Minimizing multimodal functions of continuous variables with the simulated annealing algorithm,” ACM Transactions on Mathematical Software, vol. 13, no. 13, Sept. 1987, pp. 262-280. [7] P. Frasconi, M. Gori, and G. Soda, “Local Feedback Multilayered Networks,” Neural Computation 4( I), 1992, pp. 120-130. [8] P. Frasconi, M. Gori, M. Maggini, and G. Soda, “Unified integration of explicit rules and learning by example in recurrent networks,” IEEE Trans. on Knowledge and Data Engineering, in press.
[9] R. J. Gaynier, and T. Downs, “A method of training multi-layer networks with heaviside characteristics using internal representations,” IEEE International Conference on Neural Networks 1993, San Francisco, pp. 1812-1817. [lo] M. Gori, Y. Bengio, and R. De Mori, “BPS: a learning algorithm for capturing the dynamic nature of speech,” Proc. IEEE Int. Joint Con$ on Neural Nehvorks, Washington, DC, 1989, pp. II.417-11.424. [ l l ] C. L. Giles and C. W. Omlin, “Inserting rules into recurrent neural networks,” in Neural Networks for Signal Processing II, Proceedings of the 1992 IEEE workshop, Kung, Fallside, Sorenson, and Kamm, Eds. IEEE Press, pp. 13-22. [12] T. Grossman, R. Meir, and E. Domany, “Learning by choice of internal representation,” Neural Informarion Processing Systems 1, D. S. Touretzky, Ed. pp. 73-80. [I31 S. Kirkpatrick, C. D. Gelatt, M. P. Vecchi, “Optimization by simulated annealing,” Science 220, pp.67 1-680, 4598 (May 1983). [I41 G. Kuhn, “A first look at phonetic discrimination using connectionist models with recurrent links.” CCRP - IDA SCIMP working paper No.4/87, Institute for Defense Analysis, Princeton, NJ, 1987. [I51 K. J. Lang and G. E. Hinton, “The development of the Time-Delay Neural Network architecture for speech recognition,” Technical Report CMU-CS-88- 152, Carnegie-Mellon University, 1988. [16] Y. Le Cun, “Learning processes in an asymmetric threshold network,” in Disordered systems and biological organization,E. Bienenstock, F. Fogelman-SouliC,and G. Weisbuch, Eds. Les Houches, France: Springer-Verlag, 1986, pp. 233-240. [17] C. M. Marcus, F. R. Waugh, and R. M. Westervelt, “Nonlinear dynamics and stability of analog neural networks,” Physica D 51 (special issue), pp. 234247, 1991. [18] M. C. Mozer, “A focused back-propagation algorithm for temporal pattern recognition,” Complex Systems, 3, pp. 349-391, 1989. [19] M. C. Mozer, “Induction of multiscale temporal structure,” in Advances in Neural Information Processing Systems 4, Moody, Hanson, and Lippman, Eds. San Matteo, CA: Morgan Kaufmann, 1992, pp. 275-282. [20] J. M. Ortega and W. C. Rheinboldt, Iterative Solution of Non-linear Equations in Several Variables and Systems of Equations, New York: Academic Press, 1960. [21] R. Rohwer, “The ‘moving targets’ training algorithm,” Advances in
Neural Information Processing Systems 2, Touretzky, Ed. San Matteo, CA: Morgan Kaufmann, 1990, pp. 558-565. [22] D. E. Rumelhart, G. E. Hinton and R. J. Williams, “Learning internal representation by error propagation,” in Parallel Distributed Processing, D. E. Rumelhart and 3.L. McClelland, Eds. Cambridge, MA: MIT Press, Bradford Books, vol. I , 1986, pp. 318-362. [23] R. J. Williams and D. Zipser “A learning algorithm for continuously running fully recurrent neural networks,” Neural Computation, I , 1989, pp. 270-280.
Yoshua Bengio received the Ph.D. degree in Computer Science from McGill University, Canada in 1991. After two post-doctoral years, one at M.I.T. and one at Bell Laboratories, he is currently assistant professor in the Departement d’Informatique et Recherche Op6rationnelle (computer science) at the Universiti de MontrCal. His research interests include learning algorithms for neural networks, recurrent networks, speech recognition, hidden Markov models, and handwriting recognition.
Patrice Simard grew up in Paris, France, where he studied mathematics and physics. He received the bachelor’s degree in Electrical Engineering (1986) from I’UniversitC de MontrCal, Canada, and the Ph.D. in Computer Science (1991) from the University of Rochester, NY. He is now working at AT&T Bell Laboratories in the Adaptive Systems Research Department. His scientific interests are in learning and generalization.
Paolo Frasconi (S ’91) received the degree in Electronic Engineering from the Universitl di Firenze, Italy in 1990. Since 1991 he is with the Dipartimento di Sistemi e Informatica, Firenze, where he is currently a Ph.D. candidate in Computer Science. In 1992 he was a visiting scholar in Department of Brain and Cognitive Science at M.I.T. His research interests include neural networks, speech recognition, and parallel computing.
Authorized licensed use limited to: University of Massachusetts Amherst. Downloaded on March 21,2024 at 15:29:41 UTC from IEEE Xplore. Restrictions apply.