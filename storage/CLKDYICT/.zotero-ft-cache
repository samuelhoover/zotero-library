Recent advances in machine learning towards multiscale soft materials design
Nicholas E Jackson1,2,3, Michael A Webb1,3 and Juan J de Pablo1,2
Abstract
The multiscale design of soft materials requires an ensemble of computational techniques spanning quantum-chemistry to molecular dynamics to continuum modeling. The recent emergence of machine-learning (ML) and modern optimization algorithms has accelerated material property prediction, as well as stimulated the development of hybrid ML/molecular modeling methodologies capable of providing physical insights unobtainable from purely physics-based modeling and intuition. Such hybrid techniques also have important ramifications for the ML-enhanced interpretation of results from simulations and experiments alike. Leveraging ML techniques for the design of chemical or morphological structures based on a target property or functionality represents an exciting goal for the general area of soft materials, including polymers, liquid crystals, colloids, or biomolecules, to name a few representative classes of systems. Here, we provide a perspective on recent work using ML techniques of relevance for the multiscale design of soft materials and outline potential future directions of interest to the soft materials community.
Addresses
1 Institute for Molecular Engineering, University of Chicago, Chicago, IL 60615, USA 2 Institute for Molecular Engineering, Argonne National Laboratory, Lemont, IL 06349, USA 3 Equal contribution.
Current Opinion in Chemical Engineering 2019, 23:106‚Äì114
This review comes from a themed issue on Frontiers of Chemical Engineering
Edited by Jim Pfaendtner, Randall Q Snurr and Veronique Van Speybroeck
For a complete overview see the Issue and the Editorial
Available online 28th April 2019
https://doi.org/10.1016/j.coche.2019.03.005
2211-3398/aÃÉ 2019 Elsevier Ltd. All rights reserved.
Introduction
A primary challenge for soft materials design is the strong coupling of disparate length and timescales that mediates materials behavior; molecular degradation is influenced by macroscopic environment, electronic and ionic transport are controlled by mesoscale morphology, and biological
functionality is inherently hierarchical. This confluence of length and timescales requires the integration of an array of theoretical, computational, and data-centric techniques to fully characterize material performance. Design efforts could be greatly enhanced not only by predictive models that capture multiscale structure-function relationships in existing materials but also by generative design techniques that identify new candidate soft matter chemistries and morphologies for target applications.
In the past decade, machine-learning (ML) approaches to molecular modeling have transformed the landscape of materials research and engineering. The success of ML applied to ‚Äúhard‚Äù materials motivates the question of why ML has not similarly affected soft materials research. As noted by Ferguson [1], this is principally attributable to (i) the qualitative diversity of soft matter systems and the variety of modeling techniques (quantum-chemistry to finite element methods) required, (ii) the importance of entropic effects and competing kinetic/thermodynamic states for a given material, and (iii) emergent many-body effects that are ubiquitous in soft materials. Soft materials research is further complicated by a lack of obvious order parameters, as they are often amorphous or disordered, and their behavior is often governed by a competition between multiple weak interactions, thereby rendering any design endeavor particularly susceptible to small perturbations. Thus, by comparison to many inorganic systems or hard materials for which the structure is known or well defined, relevant structural motifs for soft materials, such as polymers and colloids, are often unknown and may depend on processing conditions/history. Consequently, no single ML approach suits the entire soft matter community simultaneously, and solutions must be tailored to specific research needs. This fact has naturally slowed the adoption of ML in soft matter, though there are no fundamental physical attributes impeding its integration.
In this article, we discuss the emergence of ML algorithms of interest for soft materials modeling. In particular, we focus on developments related to (i) molecular property prediction and chemical discovery and (ii) multiscale simulation and analysis. As the focus of this article is on the application of ML for the molecular modeling of soft materials, we do not address promising work relevant to the soft matter community in the areas of closed-loop discovery, experimental automation and
Available online at www.sciencedirect.com
ScienceDirect
Current Opinion in Chemical Engineering 2019, 23:106‚Äì114 www.sciencedirect.com


optimization, or reaction prediction, nor do we extensively review technical details of the various ML algorithms employed. Finally, due to space limitations, this perspective cannot comprehensively review all impactful work in the field. Instead, included or highlighted works are intended to showcase illustrative advances relevant to soft materials design, based on our own biases. Readers are thus encouraged to further explore the articles, the references therein and beyond to gain a broader and complete perspective.
Molecular property prediction and chemical discovery in soft materials Molecular property prediction
An overarching goal for modeling soft materials concerns the prediction of physical observables derived solely from a molecule‚Äôs chemical structure. In this regard, efforts have traditionally been split between two approaches: (i) first-principles modeling and (ii) the statistical learning of physicochemical properties via mathematical representations of molecular structures, with no reliance on fundamental physical equations. While both paradigms have been widely adopted by the soft material community, they experience known limitations pertaining to accuracy and/or computational cost. Recent breakthroughs in featurization approaches and ML algorithms promise to enable the statistical learning of first-principles-derived physical properties at a fraction of the conventional computational costs.
Effective representation of molecules as inputs to ML algorithms is a prerequisite to the success of molecular property predictions. While specifying Cartesian coordinates yields a unique solution of the Schr  Ãàodinger equation, such coordinates are inadequate as inputs for ML algorithms because they lack appropriate invariances to translation, rotation, and permutation of like atoms. Consequently, features based on radial symmetry functions, Coulomb matrices, or other vector-based schemes are often used to encode structural and chemical properties [2‚Äì4]. While some descriptors use sorting techniques to satisfy input requirements, other featurization approaches explicitly incorporate critical invariances and symmetries, such as the Smooth Overlap of Atomic Positions (SOAP) [5] for learning multipole moments and polarizabilities [6]. For soft matter applications, work combining SOAP (for featurization) and Gaussian Process Regression (as the ML algorithm) to estimate relative energies of organic crystals as well as intermolecular electronic couplings relevant to charge transport [7] is particularly intriguing. It will be interesting to see if SOAP can be extended to broadly characterize the properties of soft materials, given the ubiquity of disorder and heterogeneity in such systems. Given the proliferation of available molecular feature representations, the work of Imbalzano et al. guiding the selection of optimal
feature representations based on the intrinsic structure of the underlying data is highly relevant for soft materials researchers looking to incorporate ML into their modeling efforts [3]. Moreover, the importance of selecting effective featurization approaches is underscored by the work of Pronobis, which showed how combining a novel 2-body featurization approach with a linear ML algorithm yielded comparable performance to other advanced non-linear ML algorithms [8].
The use of molecular graphs to represent chemical structures has also enabled many recent, high-performing ML methods [9,10]. The MoleculeNet database of Wu and coworkers highlights the predictive performance of graph-based featurizations and their associated artificial neural network (ANN) ML algorithms on tasks relevant to soft materials [11]. In hundreds of prediction tasks across multiple length and time scale phenomena, graphbased representations demonstrated state-of-the-art performance, which the authors attributed to the ability of graph-based ML algorithms to extract molecular information via convolutions over subgraphs of the molecular environment. Further generalizing this graph-based paradigm is the work by Schutt et al. developing the deep tensor neural network (DTNN) approach [12]. This technique utilizes convolutions over molecular environments defined by distance matrices and nuclear charges to predict a variety of molecular properties and has recently been adapted to handle periodic boundary conditions [13]. Interestingly, recent work by Gilmer et al. illustrates how a spectrum of graph-convolutional networks may be cast into a single common framework (Message Passing Neural Networks) with broad applicability [14].
To conclude this section, we highlight two works underscoring the role of data in the application of ML to the modeling of soft materials. First is the work of Faber, which illustrates that many ML methods have surpassed actual ab-initio methods (relative to the exact result) in terms of predictive accuracy [15]. This suggests that ML models could further improve predictive accuracy if large quantities of high quality data (e.g. explicitly electron correlated or experimental) were available. With this in mind, the aforementioned MoleculeNet framework warrants further emphasis [11]. For MoleculeNet, researchers identified 800 different prediction tasks spanning quantum-mechanical physical properties to macroscopic physiological effects, with efforts to extend the scope of this database in the future. Clearly, standardized databases and algorithm implementations such as MoleculeNet will be key to future advancement of ML in soft materials modeling.
Generative molecular and morphological design
Coinciding with developments in molecular property prediction have been advancements applying ML algorithms for materials design. Since intuition-based
Recent advances in machine learning Jackson, Webb and de Pablo 107
www.sciencedirect.com Current Opinion in Chemical Engineering 2019, 23:106‚Äì114


design is notoriously difficult given the complexities underlying soft materials, generative design methods for both molecular and morphological structure discovery are of tremendous interest to the soft matter community.
Early work on the design problem for soft materials applied high-throughput virtual screening to identify promising organic light-emitting diode molecules, which were subsequently synthesized, tested, and confirmed as high-performance materials [16]. More recently, variational autoencoders (VAE) have been used to convert discrete molecular representations into a continuous latent space upon which gradient optimization
algorithms can be applied [17], and promise to be a powerful avenue for the future design of soft materials (see Figure 1). Other design strategies include generative adversarial networks (GAN) [18] and reinforcement learning techniques [19], which have been used to guide the structural evolution of organic compounds towards particular soft material applications. Future advances in generative design will require solving the graph isomorphism and generation problem, such that emerging graph-based feature representations may be effectively combined with generative algorithms. A review by Sanchez-Lengeling and Aspuru-Guzik discussing the current state of generative materials design algorithms is strongly suggested for this topic [20].
For soft matter applications, designing condensed-phase and multi-molecular properties (self-assembly, for example) is a challenging but necessary task. Early work in this direction includes that by Mannodi-Kanakkithodi et al., which combines ML predictions of condensed phase dielectric constants with genetic algorithms to design new polymeric repeat units [21]. Work by multiple groups have instantiated databases of computed and experimental polymer properties that should greatly facilitate future design of polymeric materials [22‚Äì25]. In another example, Long and Ferguson utilized diffusion maps (an unsupervised ML approach) along with enhanced sampling tools to obtain low-dimensional free-energy landscape representations for patchy particle self-assembly [26]. Then, they leveraged covariance matrix adaptation-evolution strategy (CMA-ES) to optimize properties of patchy colloidal particles, such that they would assemble into targeted structural motifs. These examples illustrate the potential of using ML techniques for designing soft materials.
Machine learning for soft material simulation and analysis Machine-learning potentials
Accurately representing complex energetic interactions is critical to predictive materials modeling and limits design efforts. Traditionally, particle-based simulations
108 Frontiers of Chemical Engineering
Figure 1
A diagram illustrating the use of a variational autoencoder (VAE) for generative molecular design. (Left) The encoder enables one to convert a discrete molecular representation (in this case a SMILES string) into a vector in a latent space for a continuous molecular representation; the decoder enables conversion from a point in the latent space back to a discrete molecule. (Right) When jointly trained with a property prediction task, gradient-based optimization can be used to navigate to regions of latent space to high values of a desired property f(z), and then the latent space points can be converted back to target molecules in a generative design framework. Reprinted with permission from Ref. [17] at https:// pubs.acs.org/doi/10.1021/acscentsci.7b00572, Copyright American Chemical Society 2018.
Current Opinion in Chemical Engineering 2019, 23:106‚Äì114 www.sciencedirect.com


of soft matter are facilitated by fitting analytical functions that approximate the Born-Oppenheimer potential energy surface (PES); the fitting is based, at least in part, on electronic-structure calculations. The accuracy and transferability of this approach is restricted by particular functional forms and can be limiting or constraining. Meanwhile, the prospects of utilizing purely firstprinciples simulations of soft materials is glum, given the lengthscales and timescales required for examining macromolecular properties and the computational expense of evaluating the electronic structure on-the-fly. Recent advances that leverage ML in force-field development may be key for simulating soft matter with greater accuracy and efficiency.
ML-derived force fields, or machine-learning potentials (MLPs), can provide accuracy commensurate with the electronic structure method used to generate training data at significantly reduced cost [27,28]. Already, MLPs have been effectively used to study a number of systems of varying complexity, ranging from small-to-intermediately sized molecules and molecular clusters to electrolyte solutions and interfaces [28‚Äì32]. An attractive feature of MLPs is that they sidestep limitations imposed by choosing particular analytical functions and further enable a description of effects ‚Äî reactivity, polarizability, many-body interactions ‚Äî that are either neglected or complexly described in traditional force fields. Moreover, when trained directly to ab initio data, they can be effectively combined with path-integral methods to include nuclear quantum effects in MD simulations [30,32,33]. However, certain challenges remain for the proliferation of MLPs to studies of soft matter.
One issue relates to efficiently treating systems with rich chemical complexity. Just as in chemical discovery, MLPs require definition of appropriate input features or descriptors for the atomic environment; this choice impacts both simulation efficiency and accuracy [3,27]. Commonly used atom-centered symmetry functions scale poorly with increasing number of distinct elements, since separate sets of radial functions are required for every pair and triplet of elements. Recently, Gastegger et al. [34] utilized element-dependent weighting functions in their symmetry functions to implicitly account for local chemistry, thereby limiting the number of required basis functions and enhancing simulation efficiency without shortchanging accuracy.
Relatedly, descriptors in MLPs tend to be intrinsically local and limited to approximate short-range interaction energies. While perhaps reasonable for small molecules or clusters, this is likely problematic in condensed-phase and interfacial simulations with macromolecules. One appealing approach is to augment short-range MLPs with long-range physics, as demonstrated by Yao et al. in their hybrid TensorMol-0.1 approach, which couples a
short-range ANN potential with long-range electrostatic
and van der Waals physics [35]. Similarly, Bereau and coworkers have prescribed a strategy that uses ML to parameterize a handful of transferable, global parameters in a set of physics-based potentials representing noncovalent interactions [36].
These latter two works [35,36] are also in pursuit of MLPs that are transferable across chemical composition. In part, such efforts are motivated by another key challenge, which is related to dataset construction. Namely, effective parameterization of a high-quality MLP depends intimately on both the amount of training data produced as well as the chemical and configurational biases of that data; recent works have approached these issues in a variety of ways. Smith et al. have utilized an active learning approach to directly generate requisite data across chemical space [37] and improve the performance of their earlier extensible ANN potential [38]. Similarly, Botu and Ramprasad have provided an adaptive ML framework that might be used to guide dataset generation over configuration space [39]. Moreover, Herr et al. have shown how metadynamics can be used to train better and more transferable MLPs compared to those trained using configurations produced from unbiased MD or sampling of normal-mode motions [40]. Meanwhile, Chmiela and coworkers have achieved highly accurate, flexible molecular force fields using a limited amount (hundreds of conformations) of training data by incorporating physical symmetries in their gradient-domain ML approaches [33,41]. Ultimately, if such approaches allow high-quality results to be obtained from limited data, then higher-quality electronic structure methods can be employed to improve the overall accuracy of future simulations.
Approaches to coarse-grained modeling
Coarse-grained (CG) modeling is an essential component of soft matter research. The considerable success of ML techniques in the realms of electronic structure prediction and atomistic simulation is encouraging for applications to CG modeling, and application of ML approaches that facilitate CG simulation are now emerging.
Just as in ab initio simulations, MLPs have the potential to improve the accuracy, efficiency, and transferability of CG models. Building on their earlier work for ab initio simulations [30], Zhang and colleagues introduced a framework for developing many-body CG potentials based on ANN and used it to develop a CG water model that exhibited excellent agreement with ab initio simula
tions of DFT water [42]. Application of this or similar approaches for CG simulation of polymers or systems with more complex chemical topology is an exciting possibility. Using a different strategy, Lemke and Peter trained a ANN to predict conformational free energies of oligopeptides by creating a classification problem
Recent advances in machine learning Jackson, Webb and de Pablo 109
www.sciencedirect.com Current Opinion in Chemical Engineering 2019, 23:106‚Äì114


between real conformations (obtained from atomistic simulation) and fake conformations (obtained from an arbitrary distribution) [43]. By employing a convolutional ANN architecture, their trained ANN model could be applied to longer oligopeptides; such an approach will be useful in future polymer-related work, since training data will be restricted to simulations of oligomers that are generally shorter than intended simulation targets.
Interestingly, developing MLPs for CG simulation may require more data compared to parameterizing an atomistic model. For Zhang et al., a trajectory of 15 ns was used for training the water CG MLP [42]. Because such a trajectory is computationally demanding for direct ab initio simulations, they first parameterized an accurate MLP for atomistic water, and this model generated the requisite data for training the CG model. Somewhat relatedly, the training data of Lemke and Peter was necessarily limited to shorter oligopeptides due to difficulties of obtaining converged MD data for longer oligopeptides, although including longer oligopeptides in the training data would have likely improved transferability of the model. Both cases suggest that simulation approaches that proceed through a hierarchy of system representations may be fruitfully applied for CG model development. The advent of methods that generate CG representations in an automated and systematic fashion should be useful to such efforts [44‚Äì46]. In addition,
training approaches that exploit symmetries [33,41] may help to reduce the amount of required data.
There are also opportunities to exploit ML algorithms in a multi-scale simulation setting, without analogy to atomistic simulation. The work by Jackson et al. provides one such example, in which the conformationally
dependent electronic structure (HOMO levels, dimer couplings, etc.) for model conjugated materials is predicted directly from CG degrees of freedom using an ANN [47] (see Figure 2). Combined with efficient CG simulations that can predict the morphological character of systems, such an approach enables characterization of the bulk electronic properties of photoactive materials. Continued development of multiscale simulation strategies that exploit ML at CG resolutions will be crucial to soft matter design efforts.
Enhanced simulation and analysis
Efficient exploration of configuration space and characterization of free energy landscapes is a critical concern in both atomistic and coarse-grained simulation of soft materials, making enhanced sampling methods [48] essential in many research efforts. Recent ML-enhanced approaches demonstrate great promise in improving upon existing enhanced sampling methods. Adaptive biasing techniques based on ANN ‚Äî namely, the ANN sampling technique of Sidky et al. [49] and the Force-biasing Using Neural Networks technique of Guo, Sevgen, and coworkers [50] ‚Äî have been shown to explore complex free energy landscapes in less time than conventional approaches. For a representative Gaussian landscape, the acceleration provided by the FUNN sampling technique amounts to an approximately two orders of magnitude increase over a traditional adaptive force-biasing approach [50]. Similarly, Zhang et al. utilized reinforcement learning to train a deep ANN representation of a free energy surface, which is also used to bias the simulation in regions of high certainty [51]. Already demonstrated for simple polymers and biomolecular systems, these methods hold obvious potential to explore the complex free energy landscapes in soft matter simulation. Moreover,
110 Frontiers of Chemical Engineering
Figure 2
A schematic for the ANN electronic coarse-graining method [47]. In the method, an ANN is trained to learn the configuration-dependent electronic structure of a model conjugated polymer directly from coarse-grained degrees of freedom, rather than atomistic features.
Current Opinion in Chemical Engineering 2019, 23:106‚Äì114 www.sciencedirect.com


these methods enjoy synergy with the development of MLPs, which are well served by enhanced sampling methods to improve the quality and coverage of training data (Figure 3).
While the aforementioned sampling techniques use defined collective variables (CVs)[48], there is growing use of both unsupervised and supervised ML techniques for CV discovery or identification of effective descriptors of complex system behavior [26,52‚Äì55]. Unsupervised ML techniques, such as clustering techniques, have long been used in the development of Markov state models to analyze and identify important states and transitions in dynamical systems [56]. Jadrich and coworkers utilized principal component analysis to detect phase transitions in a variety of model systems [53,54]. Meanwhile, Schoenholz and coworkers utilized a support vector machine to identify a local structural metric, ‚Äúsoftness‚Äù, that strongly correlates with observed glassy dynamics [52]. In addition, Sultan and Pande have recently addressed how one might initially identify good reaction coordinates or CVs based on local sampling of initial and final states using either support vector machines or logistic regression [57]. Nonlinear manifold learning techniques have also been applied for the purpose of CV discovery, with work by Guo et al. utilizing diffusion maps to extract the collective motions underlying nucleosome dynamics [58]. Finally, advancements that interleave the process of CV discovery and enhanced sampling have begun to emerge. In this direction, Chen et al. have demonstrated how autoencoders can be used for on-the-fly nonlinear CV discovery and employed directly in enhanced sampling (based on the existence of differentiable mappings of the molecular coordinates) [59]. Similarly, a recently developed method by Ribeiro and colleagues, termed RAVE, iteratively employs a deep learning framework of variational autoencoders to simultaneously identify and enhance sampling for a CV [60]. Given the challenges associated with identifying appropriate CVs for soft matter systems in tandem with rugged free energy landscapes, these ML methods provide new powerful approaches to studying complex systems.
In closing, it is important to highlight emerging work in which ML and optimization strategies are used in concert with molecular simulation to enhance the interpretation of experimental data. The origin of this simulation paradigm reaches back two decades, with examples spanning the use of neutron scattering and molecular simulation to iteratively derive atomistic water models [61], to the application of NMR-derived biasing constraints applied to molecular simulations of proteins [62]. Now, recent advances in datadriven ML and optimization techniques, in tandem with ever-improving experimental approaches, promise to combine in significant breakthroughs for understanding soft materials across lengthscales and timescales. Work by Khaira et al. for example, has developed an optimization
strategy that couples a theoretically-informed CG (TICG) model with CMA-ES to iteratively derive molecular simulation parameters that best reproduce experimental critical dimension SAXS data [63]. Jiang et al. used evolutionary optimization to minimize the difference between simulated and experimental trajectories to iteratively derive charge measurements of dielectric particles in electrostatically charged granular matter ‚Äî details that would be
Recent advances in machine learning Jackson, Webb and de Pablo 111
Figure 3
A schematic illustrating the ANN adaptive biasing method. During the sampling loop, the collective variables h1 and h2 are monitored to construct input and output training data for a ANN. In the training loop, an ANN with pre-defined architectures is trained using a Bayesian regularization procedure to minimize the squared error of the ANN output F^√∞h1; h2√û and the free energy estimate F√∞h1; h2√û. The biasing potential for the next iteration is then defined based on the ANN output as fi√æ1√∞h√û 1‚ÅÑ4 F^√∞h√û. Reprinted from Ref. [49], with the permission of AIP Publishing.
www.sciencedirect.com Current Opinion in Chemical Engineering 2019, 23:106‚Äì114


unobtainable in the absence of this modeling paradigm [64]. Finally, a set of techniques exist by which to explicitly bias simulated ensembles with experimental data [65], and we anticipate rapid development in the application of ML to the enhancement of these strategies.
Conclusions and outlook
Recent years have seen growing utilization of ML techniques for computational materials discovery. When properly applied, these methods can enable efficient identification and characterization of promising novel materials. However, the intrinsically hierarchical nature of soft materials design requires simulation over multiple spatiotemporal scales, as well as effective descriptions of complex many-body interactions underpinning emergent phenomena, which have slowed progress relative to other materials classes. Recent developments exploiting ML techniques suggest that we are poised to embrace such challenges and enhance capabilities of designing soft materials.
For data-driven property prediction and chemical discovery, the existing and considerable work on molecular descriptors must be extended beyond the domain of conventional molecular length and timescales, such that they can be fruitfully applied to multiscale macromolecular and soft matter systems. Critical to this effort will be the development and utilization of soft material databases, [11,22,23] such that generative design frameworks enabled by ML methods might be exploited with a success similar to that observed for hard materials and small molecules. Decisions on the constitution and variety of such databases must be carefully deliberated to derive maximum utility for the soft materials community. In the realm of simulation enhancement and analysis, developments in MLPs continue to make accurate atomistic simulations more affordable, and comparable MLP methodologies for CG modeling must be developed such that multiscale simulations of similar accuracy may be leveraged. The development of ML methodologies at specific lengthscales and timescales, in addition to those that link more conventional simulation efforts across spatiotemporal scales, will be key to this effort. Parallel to enhancements in the accuracy and expediency of molecular modeling will be advances in the analysis and interpretation of complex systems for which conventional approaches and the capabilities of human intuition are insufficient. Both supervised and unsupervised techniques have begun to emerge in this regard, but methodologies that effectively address the implicit hierarchies in soft materials will be critical for rational design. Finally, the synthesis of molecular simulation and ML for the enhanced interpretation of experimental data is an emerging frontier, which may prove most useful to the soft matter community due to both the ubiquity of molecular modeling techniques, as well as the diversity of soft materials and their relevant experimental
characterizations. In all of these cases, researchers will need to continually evaluate what is the best representation of data, which ML algorithm is most suitable for the application, and how extensible is the approach. Synthesis of all these advancements and continued innovation are promising towards the goal of multiscale soft materials design.
Conflicts of interest statement Nothing declared.
Acknowledgements
This work was supported by the Department of Energy, Office of Science, Basic Energy Sciences, Division of Materials Science and Engineering. The development of the SSAGES (github.com/MICCoM/SSAGES-public) and COPSS (bitbucket.org/COPSS/copss-polarization-public.git) software in which some of the methods discussed have been implemented was supported by the Midwest Integrated Center for Computational Materials (MICCoM) funded by the Department of Energy, Basic Energy Sicences, Division of Materials Science and Engineering. NEJ was supported by the Maria Goeppert named fellowship from Argonne National Laboratory.
References and recommended reading
Papers of particular interest, published within the period of review, have been highlighted as:
 of special interest  of outstanding interest
1. Ferguson AL: Machine learning and data science in soft materials engineering. J Phys: Condens Matter 2018, 30:043002.
14. Gilmer, J., Schoenholz, S.S., Riley, P.F., Vinyals, O., Dahl, G.E., Neural Message Passing for Quantum Chemistry, arXiv:1704.01212.
2. Hansen K, Montavon G, Biegler F, Fazli S, Rupp M, Scheffler M, von Lilienfeld OA, Tkatchenko A, Mu Ãà ller KR: Assessment and validation of machine learning methods for predicting molecular atomization energies. J Chem Theory Comput 2013, 9:3404-3419.
3. Imbalzano G, Anelli A, Giofre ÃÅ D, Klees S, Behler J, Ceriotti M: Automatic selection of atomic fingerprints and reference configurations for machine-learning potentials. J Chem Phys 2018, 148:241730.
4. Hansen K, Montavon G, Biegler F, Ramakrishnan R, Proonobis W, von Lilienfeld OA, Tkatchenko A, Mu Ãà ller KR: Machine learning predictions of molecular properties: accurate many-body potentials and nonlocality in chemical space. J Phys Chem Lett 2015, 6:2326-2331.
5. Bart  ÃÅok AP, Kondor R, Csa ÃÅ nyi G: On representing chemical environments. Phys Rev B: Condens Matter Mater Phys 2013, 87:184115.
6. Grisafi A, Wilkins DM, Csa ÃÅ nyi G, Ceriotti M: Symmetry-adapted machine learning for tensorial properties of atomistic systems. Phys Rev Lett 2018, 120:036002.
7. Musil F, De S, Yang J, Campbell JE, Day GM, Ceriotti M: Machine learning for the structure-energy-property landscapes of molecular crystals. Chem Sci 2018, 9:1289-1300.
8. Pronobis W, Tkatchenko A, Mu Ãà ller K-R: Many-body descriptors for predicting molecular properties with machine learning: analysis of pairwise and three-body interactions in molecules. J Chem Theory Comput 2018, 14:2991-3003.
9. Duvenaud DK, Maclaurin D, Iparraguirre J, Bombarell R, Hirzel T, Aspuru-Guzik A, Adams RP: Convolutional networks on graphs for learning molecular fingerprints. Adv Neural Inform Process Syst 2015, 28:2224-2232.
112 Frontiers of Chemical Engineering
Current Opinion in Chemical Engineering 2019, 23:106‚Äì114 www.sciencedirect.com


10. Kearnes S, McCloskey K, Berndl M, Pande V, Riley P: Molecular graph convolutions: moving beyond fingerprints. J Comput Aided Mol Des 2016, 30:595-608.
11. Wu Z, Ramsundar B, Feinberg EN, Gomes J, Geniesse C, Pappu AS, Leswing K, Pande V: MoleculeNet: a benchmark for molecular machine learning. Chem Sci 2018, 9:513-530.
12. Sch  Ãàutt KT, Arbabzadah F, Chmiela S, M  Ãàuller KR, Tkatchenko A: Quantum-chemical insights from deep tensor neural networks. Nat Commun 2017, 8:13890.
13. Sch  Ãàutt KT, Sauceda HE, Kindermans P-J, Tkatchenko A, M  Ãàuller KR: Schnet ‚Äî a deep learning architecture for molecules and materials. J Chem Phys 2018, 148:241722.
15. Faber FA, Hutchison L, Huang B, Gilmer J, Schoenholz SS, Dahl GE, Vinyals O, Kearnes S, Riley PF, Von Lilienfeld OA: Prediction errors of molecular machine learning models lower than hybrid DFT error. J Chem Theory Comput 2017, 13:5255-5264.
16. 
Go ÃÅ mez-Bombarelli R, Aguilera-Iparraguirre J, Hirzel TD, Duvenaud D, Maclaurin D, Blood-Forsythe MA, Chae HS, Einzinger M, Ha DG, Wu T, Markopoulos G et al.: Design of efficient molecular organic light-emitting diodes by a highthroughput virtual screening and experimental approach. Nat Mater 2016, 15:1120-1127. This article is a critical demonstration of the efficacy of designing soft materials using a hybrid simulation‚Äìcomputation‚Äìexperimental approach.
17. 
Go ÃÅ mez-Bombarelli R, Wei JN, Duvenaud D, Herna ÃÅ ndezLobato JM, Sa ÃÅ nchez-Lengeling B, Sheberla D, AguileraIparraguirre J, Hirzel TD, Adams RP, Aspuru-Guzik A: Automatic chemical design using a data-driven continuous representation of molecules. ACS Central Sci 2018, 4:268-276. This article advances the variational autoencoder technique for molecular design. This methodology should provide useful for the generative design of soft matter chemistries and morphologies, where the latent space of molecules can be organized across multiple lengthscale phenomena.
18. Sanchez-Lengeling, B., Outeiral, C., Guimaraes, G.L., AspuruGuzik, A., Optimizing distributions over molecular space. An objective-reinforced generative adversarial network for inversedesign chemistry (ORGANIC), ChemRxiv 5309668.
19. Popova M, Isayev O, Tropsha A: Deep reinforcement learning for de novo drug design. Sci Adv 2018, 4:eaap7885.
20. Sanchez-Lengeling B, Aspuru-Guzik A: Inverse molecular design using machine learning: generative models for matter engineering. Science 2018, 361(6400):360-365.
21. 
Mannodi-Kanakkithodi A, Pilania G, Huan TD, Lookman T, Ramprasad R: Machine learning strategy for accelerated design of polymer dielectrics. Sci Rep 2016, 6:20952.
This article demonstrates the potential for generative machine learning algorithms to rationally design new polymer chemistries based on firstprinciples quality calculations.
22. Kim C, Chandrasekaran A, Huan TD, Das D, Ramprasad R: Polymer genome: a data-powered polymer informatics platform for property predictions. J Phys Chem C 2018, 122:17575-17585.
23. Polymer property predictor and database, 2019. http://pppdb. uchicago.edu.
24. Audus DJ, de Pablo JJ: Polymer informatics: opportunities and challenges. ACS Macro Lett 2017, 6(10):1078-1082.
25. Tchoua RB, Qin J, Audus DJ, Chard K, Foster IT, de Pablo J: Blending education and polymer science: semiautomated creation of a thermodynamic property database. J Chem Educ 2016, 93:1561-1568.
26. 
Long AW, Ferguson AL: Rational design of patchy colloids: via landscape engineering. Mol Syst Des Eng 2018, 3:49-65.
This article is an excellent demonstration of the application of machinelearning to design self-assembly pathways in soft materials.
27. Behler J: Perspective: machine learning potentials for atomistic simulations. J Chem Phys 2016, 145:170901.
28. Behler J: First principles neural network potentials for reactive simulations of large molecular and condensed systems. Angew Chem Int Ed 2017, 56:12828-12840.
29. Huan TD, Batra R, Chapman J, Krishnan S, Chen L, Ramprasad R: A universal strategy for the creation of machine learningbased atomistic force fields. NPJ Comput Mater 2017, 3:37.
30. Zhang L, Han J, Wang H, Car R, Weinan E: Deep potential
molecular dynamics: a scalable model with the accuracy of quantum mechanics. Phys Rev Lett 2018, 120:143001.
31. Nguyen TT, Sze ÃÅ kely E, Imbalzano G, Behler J, Csa ÃÅ nyi G, Ceriotti M, G  Ãàotz AW, Paesani F: Comparison of permutationally invariant polynomials, neural networks, and Gaussian approximation potentials in representing water interactions through manybody expansions. J Chem Phys 2018, 148:241725.
32. Hellstr  Ãàom M, Ceriotti M, Behler J: Nuclear quantum effects in sodium hydroxide solutions from neural network molecular dynamics simulations. J Phys Chem B 2018, 122 (44):10158-10171.
33. Chmiela S, Tkatchenko A, Sauceda HE, Poltavsky I, Schu Ãà tt KT, M  Ãàuller KR: Machine learning of accurate energy-conserving molecular force fields. Sci Adv 2017, 3:e160301.
34. Gastegger M, Schwiedrzik L, Bittermann M, Berzsenyi F,
Marquetand P: WACSF ‚Äî weighted atom-centered symmetry functions as descriptors in machine learning potentials. J Chem Phys 2018, 148:241709.
35. 
Yao K, Herr JE, Toth DW, McKintyre R, Parkhill J: The TensorMol0.1 model chemistry: a neural network augmented with longrange physics. Chem Sci 2018, 9(8):2261-2269.
This article is an impressive demonstration of a hybrid machine-learning/ physics based model for molecular potential energy surfaces that should be critical for the design of soft materials in the condensed phase.
36. 
Bereau T, DiStasio RA, Tkatchenko A, von Lilienfeld OA: Noncovalent interactions across organic and biological subsets of chemical space: physics-based potentials parametrized from machine learning. J Chem Phys 2018, 148(24):241706.
This work provides a nice illustration of how one might approach the problem of transferable force fields via the parameterization of physicsbased potentials ‚Äúonce and for all‚Äù using machine learning techniques.
37. 
Smith JS, Nebgen B, Lubbers N, Isayev O, Roitberg AE: Less is more: sampling chemical space with active learning. J Chem Phys 2018, 148. This work demonstrates the utility of active learning applied to the development of machine-learning models for soft materials, specifically the development of machine-learning potentials.
38. Smith JS, Isayev O, Roitberg AE: ANI-1: an extensible neural network potential with DFT accuracy at force field computational cost. Chem Sci 2017, 8(4):3192-3203.
39. Botu V, Ramprasad R: Adaptive machine learning framework to accelerate ab initio molecular dynamics. Int J Quantum Chem 2015, 115(16):1074-1083.
40. Herr JE, Yao K, Toth DW, McKintyre R, Parkhill J: Metadynamics for training neural network model chemistries: a competitive assessment. J Chem Phys 2018, 148:241710.
41. 
Chmiela S, Sauceda HE, Mu Ãà ller KR, Tkatchenko A: Towards exact molecular dynamics simulations with machine-learned force fields. Nat Commun 2018, 9:3887. This work represents a promising route towards the development of sub kcal/mol MLP via the explicit incorporation of physical symmetries to reduce the requisite size of training datasets.
42. 
Zhang L, Han J, Wang H, Car RWE: DeePCG: constructing
coarse-grained models via deep neural networks. J Chem Phys 2018, 149(3):034101. Here, authors construct a framework for using MLPs in CG modeling applications. The parameterization of the CG model is facilitated by a previously parameterized atomistic MLP, hinting at the viability of future hierarchical simulation approaches.
43. Lemke T, Peter C: Neural network based prediction of conformational free energies ‚Äî a new route toward coarsegrained simulation models. J Chem Theory Comput 2017, 13 (12):6213-6221.
44. Bereau T, Kremer K: Automated parametrization of the coarsegrained martini force field for small organic molecules. J Chem Theory Comput 2015, 11:2783-2791.
Recent advances in machine learning Jackson, Webb and de Pablo 113
www.sciencedirect.com Current Opinion in Chemical Engineering 2019, 23:106‚Äì114


45. Chakraborty M, Xu C, White AD: Encoding and selecting coarsegrain mapping operators with hierarchical graphs. J Chem Phys 2018, 149:134106.
46. Webb MA, Delannoy J-YP, de Pablo JJ: Graph-based approach to systematic molecular coarse-graining. J Chem Theory Comput 2019, 15:1199-1208.
47. 
Jackson NE, Bowen AS, Antony LW, Webb MA, Vishwanath V, de Pablo JJ: Electronic structure at coarse-grained resolutions via supervised machine learning. Sci Adv 2019, 5:eaav1190.
This demonstrates the application of machine-learning techniques to solve multiscale modeling problems without good solutions in conventional physics, specifically the coarse-graining of configuration-dependent electronic structure in soft materials.
48. Sidky H, Col  ÃÅon YJ, Helfferich J, Sikora BJ, Bezik C, Chu W, Giberti F, Guo AZ, Jiang X, Lequieu J, Li J et al.: Ssages: software suite for advanced general ensemble simulations. J Chem Phys 2018, 148:044104.
49. 
Sidky H, Whitmer JK: Learning free energy landscapes using artificial neural networks. J Chem Phys 2018, 148:104111.
This work provides a novel approach to enhanced sampling using ANN, architectures in an adaptive biasing scheme. Enhanced sampling tools are widely employed in soft matter simulations, and this method seemingly outperforms conventional approaches.
50. Guo AZ, Sevgen E, Sidky H, Whitmer JK, Hubbell JA, de Pablo JJ: Adaptive enhanced sampling by force-biasing using neural networks. J Chem Phys 2018, 148:134108.
51. Zhang L, Wang HWE: Reinforced dynamics for enhanced sampling in large atomic and molecular systems. J Chem Phys 2018, 148(12):124113.
52. Schoenholz SS, Cubuk ED, Sussman DM, Kaxiras E, Liu AJ: A structural approach to relaxation in glassy liquids. Nat Phys 2016, 12:469-471.
53. Jadrich RB, Lindquist BA, Truskett TM: Unsupervised machine learning for detection of phase transitions in off-lattice systems. i. Foundations. J Chem Phys 2018, 149(19):194109.
54. Jadrich RB, Lindquist BA, Pi neros WD, Banerjee D, Truskett TM: Unsupervised machine learning for detection of phase
transitions in off-lattice systems. ii. Applications. J Chem Phys 2018, 149(19):194110.
55. Wehmeyer C, Noe ÃÅ F: Time-lagged autoencoders: deep learning of slow collective variables for molecular kinetics. J Chem Phys 2019, 148.
56. Husic BE, Pande VS: Markov state models: from an art to a science. J Am Chem Soc 2018, 140:2386-2396.
57. Sultan MM, Pande VS: Automated design of collective variables using supervised machine learning. J Chem Phys 2018, 149.
58. Guo A, Lequieu J, de Pablo JJ: Extracting collective motions underlying nucleosome dynamics via nonlinear manifold learning. J Chem Phys 2019, 150:054902.
59. Chen W, Tan AR, Ferguson AL: Collective variable discovery and enhanced sampling using autoencoders: innovations in network architecture and error function design. J Chem Phys 2018, 149:072312.
60. Ribeiro JML, Bravo P, Wang Y, Tiwary P: Reweighted
autoencoded variational Bayes for enhanced sampling (RAVE). J Chem Phys 2018, 149:072301.
61. Soper AK: Computer simulation as a tool for the interpretation of total scattering data from glasses and liquids. Mol Simul 2012, 38(14-15):1171-1185.
62. Wang Y, Camilloni C, Kim J, Vendruscolo M, Gao J, Veglia G: Characterization of protein kinase a free energy landscape by NMR-restrained metadynamics. Biophys J 2017, 112:50a.
63. Khaira G, Doxastakis M, Bowen A, Ren J, Suh HS, Segal-Peretz T, Chen X, Zhou C, Hannon AF, Ferrier NJ, Vishwanath V et al.: Derivation of multiple covarying material and process parameters using physics-based modeling of X-ray data. Macromolecules 2017, 50:7783-7793.
64. Jiang X, Li J, Lee V, Jaeger HM, Heinonen OG, de Pablo JJ: Evolutionary strategy for inverse charge measurements of dielectric particles. J Chem Phys 2018, 148:234302.
65. Pitera JW, Chodera JD: On the use of experimental
observations to bias simulated ensembles. J Chem Theory Comput 2012, 8:3445-3451.
114 Frontiers of Chemical Engineering
Current Opinion in Chemical Engineering 2019, 23:106‚Äì114 www.sciencedirect.com