doi.org/10.1002/ijch.202100016
What’s Left for a Computational Chemist To Do in the Age
of Machine Learning?
Heather J. Kulik*[a]
Dedicated to Kenny Lipkowitz on the occasion of his 70th birthday.
Abstract: Machine learning (ML) has become a central focus of the computational chemistry community. I will first discuss my personal history in the field. Then I will provide a broader view of how this resurgence in ML interest echoes and advances upon earlier efforts. Although numerous changes have brought about this latest wave, one of the most significant is the increased accuracy and efficiency of low-cost methods (e. g., density functional theory or DFT) that have made it possible to generate large data sets for ML
models. ML has also been used to bypass, guide, or improve DFT. The field of computational chemistry thus finds itself at a crossroads as ML both augments and supersedes traditional efforts. I will present what I believe the role of the computational chemist will be in this evolving landscape, with specific focus on my experience in the development of autonomous workflows in computational materials discovery for open-shell transition-metal chemistry.
Keywords: computational chemistry · density functional calculations · machine learning · transition metals · materials discovery
1. Introduction: Let’s Start at the Beginning
“I was taught that the way of progress was neither swift nor easy.” – Marie Curie It would be challenging for me to speak broadly on the history of computational chemistry, a field that was well established[1] before I entered grade school. Nevertheless, I can speak to my own experience coming into computational chemistry in the mid-2000s, only five or so years after Kohn and Pople received the 1998 Nobel Prize for their developments in the field.[2] My journey to computational chemistry arose out of a series of failures that ultimately illuminated my path. First, I failed to get a job as a practicing chemical engineer directly after completing my undergraduate degree at the Cooper Union. Encouraged to enter graduate school as a “plan B”, I was at first captivated by the promise of protein crystallography. I learned in my biochemistry course that knowing the atomic structure of a protein would reveal its function, as exemplified by the selectivity filter on the potassium ion channel[3] (Figure 1). Indeed, down the street at Rockefeller University, Roderick MacKinnon received the Nobel prize in 2003 for having solved this very structure.[4] After spending a summer at Rockefeller in the Darst lab failing to be suitably proficient with my sterile technique to convince anyone I could become an X-ray crystallographer, I eventually realized that what had attracted me was the idea of looking at the image of where the electrons and atoms were. The rest of biochemistry, such as protein expression, which seemed to involve staring at or shaking beakers in the hopes that cells were doing something magical, completely bored me. In the mid-2000s, X-ray crystallographers would spend a
very long time (years, even!) attempting to express and crystallize proteins with very little luck in producing a crystal structure.[5] Yet, through discussions with my undergraduate quantum mechanics teacher, Robert Topper, plus a little bit of convincing myself that computers were better research tools than pipettes for someone as clumsy as I was, I came to realize that computational chemistry was a way to look at the electron density and know what the atoms were doing all of the time, not just once every five years. Years before this, in grade school, I had been captivated by computers and their promise in the early days of the internet, but it did not occur to me that I might carry out chemical modeling on them until much later. I mention all of these points to highlight how many career paths can be a consequence of chance and a response to tides of excitement that build around us. For the true trailblazers and nonconformists out there, this may not be the case. For the rest of us, we are a product of our environment. In my case, it was the culmination of my own clumsiness, love of computers, the growing excitement around me about building understanding of structure-function relationships, and the increasing computing power that made it possible to routinely model electronic structure, including of biological systems, at the time I was entering graduate school. Taking a step forward fifteen years from the start of my research career, young researchers today enter the field of
[a] Prof. Dr. H. J. Kulik
Department of Chemical Engineering, Massachusetts Institute of Technology, 77 Massachusetts Ave Rm 66–464, Cambridge, MA 02139 (USA) E-mail: hjkulik@mit.edu
Essays
www.ijc.wiley-vch.de
Isr. J. Chem. 2022, 62, e202100016 (1 of 13) © 2021 Wiley-VCH GmbH


computational chemistry with a distinct backdrop. Artificial intelligence pervades their everyday lives, whether it is Spotify and Netflix recommendations, text autocomplete on their phones, or Alexa or Siri digital assistants. The changing landscape of computer hardware, especially including everimproving graphics cards, plays a dual role here. On one hand, it has enabled the training of these deep machine learning (ML) models. On the other, architectural advances have transformed bread-and-butter computational chemistry, both in its own right and as a tool for generating training data for ML models. This transformation occurred thanks to computational chemists who creatively adopted and redesigned algorithms to leverage hardware advances.[6] For example, the classical
molecular dynamics simulations of proteins that my own students routinely carry out in a day would have required months when I was a student.[6a] First-principles calculations with density functional theory (DFT) of moderate to large (ca. hundreds to thousands) of atoms can be carried out in hours where they would have taken weeks.[6b,c] Methods more accurate than DFT are now routinely available, such as local variants of correlated wavefunction theory (WFT),[6d] making them amenable to study of systems dozens of atoms in size where only a handful would have been possible. Thus, my students and young researchers entering graduate school are experiencing an environment transformed from the one in which I first came to know “computational chemistry”. Physics-based approaches that were emerging when I was a student are now almost mundane, and machine learning and artificial intelligence appear poised to dominate what we think of when we talk about carrying out chemistry on a computer. My postdoc advisor, Todd Martínez, has said about this change, “a factor of ten should change your life.” And indeed, this factor of ten-and-then-some has changed all of our lives. In moments like these, I feel both proud and scared to be a part of this subfield: proud to have chosen such a fast-moving field that reinvents itself every decade but apprehensive for what the future may bring and whether I will be up to the challenge of helping to continue computational chemistry’s transformation over the next thirty years. In the rest of this Rosarium, I hope to bring you my personal view of the lay of the land in this field, first by reviewing the historical development of traditional computational chemistry and its relationship to parallel developments in machine learning. Then, I describe some anecdotes that highlight the unique position in which machine-learningaccelerated computational chemistry finds itself, both as a tool for illuminating other subfields and as a fundamental science in and of itself. Next, I outline what I view as some of our chief accomplishments and what I believe the “holy grail” is ultimately for my research area. Finally, I conclude with my outlook and make a few conjectures about what the next 2030 years could look like for computational chemistry in the age of big data and machine learning.
Heather J. Kulik was born and raised in New Jersey. She received her B.E. in Chemical Engineering from The Cooper Union for the Advancement of Science and Art in New York City in 2004. She obtained her Ph.D. in Materials Science and Engineering at MIT in the group of Nicola Marzari in 2009. She completed postdocs in the group of Felice Lightstone at Lawrence Livermore (2010) and Todd J. Martínez at Stanford (2010–2013). She joined MIT as a faculty member in the Department of Chemical Engineering in 2013, where she is now an associate professor. Her research interests focus on electronic structure theory and machine learning for transition-metal complex design as well as multiscale modeling for enzyme catalysis. Her work has been recognized by a Burroughs Wellcome Fund Career Award at the Scientific Interface, Office of Naval Research Young Investigator Award, DARPA Young Faculty Award and Director’s Fellowship, AAAS Marion Milligan Mason Award, NSF CAREER Award, and Sloan Fellowship in Chemistry, among others.
Figure 1. Cartoon depiction of a crystal structure of a potassium ion channel (PDB ID: 1BL8[3]) with potassium ions and water molecules shown as purple and red spheres, respectively. The selectivity filter that coordinates the potassium ions is shown as sticks with oxygen in red and nitrogen in dark blue.
Essays
Isr. J. Chem. 2022, 62, e202100016 (2 of 13) © 2021 Wiley-VCH GmbH
18695868, 2022, 1-2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ijch.202100016 by University Of Massachusetts, Wiley Online Library on [21/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License


2. A Brief History of Computational Chemistry and Machine Learning
“A factor of ten should change your life.” – Todd J. Martínez
2.1 Computational Chemistry and Machine Learning: Strange Bedfellows
Major advances in computational chemistry with physicsbased modeling in the past 20–30 years deserve special mention because they have largely powered and motivated the next wave of machine-learning-based development in computational chemistry. A compelling paradigm for thinking about computational chemistry is that there is a cost/accuracy tradeoff (Figure 2). By systematically incorporating more careful estimations of electron correlation, e. g., through more determinants in correlated wavefunction theory or through more sophisticated rungs on “Jacob’s ladder” in DFT, we increase the accuracy of our predictions while increasing the cost. If we are patient enough, we can thus approach the exact solution to the Schrödinger equation, getting the right answer for the right reason. While a generalized gradient approximation hybrid functional in DFT can be used to estimate thermochemistry at a computational cost tractable for many molecules both large and small, steeper-scaling coupled cluster theory can be used to selectively reach chemical accuracy (ca. 1 kcal/mol within
experiment) needed to predict reaction outcomes or mechanisms a priori on a computer. A combination of algorithmic developments and increases in computing power has shifted these principles to give us even more bang for our buck. For example, in correlated methods, rank reduction[7] has advantageously reduced the scaling of methods such as MP2 to the point where its cost becomes competitive with DFT. Similarly, local treatments of correlation[6d,8] that treat electron correlation hierarchically have extended the reach of gold-standard coupled cluster methods. The use of heterogeneous computing architectures along with tailored algorithms[6a–c] also have made it possible to shift by orders of magnitude the tractable system sizes and timescales accessible especially to DFT or molecular mechanics modeling (Figure 2). At the same time, judicious correction of errors in approximate DFT, including through rangeseparated parameter adjustments[9] to recover long-range exchange, Hubbard U terms[10] to eliminate delocalization error,[11] dispersion corrections,[12] and minimal basis corrections,[13] can increase accuracy without increasing computational cost (Figure 2). Many such developments have matured in the past ten to fifteen years and are now routinely used for large-scale and high-throughput modeling. With many people running DFT calculations with the intent of generating data for new ML models, computational chemistry finds itself at an interesting crossroads, and computational chemists find themselves in an awkward marriage with nascent applications of machine learning. Why is this marriage awkward? As computational chemistry methods have gotten much better and faster, they are increasingly amenable to use in automated workflows[14] that generate large data sets[15] for ML models. These ML models no longer necessarily follow the same principles of the cost/ accuracy trade-off nor the idea that imparting more physics to a model leads to obtaining the right answer for the right reason. Using more parameters to get the right answer for the wrong reason has been a point of dispute in traditional computational chemistry for some time.[16] Thus, there is a tension in how we might view computational chemistry, with its historic accomplishments in the past decade, and its use in the development of ML models. We have yet to understand how to predict when an underlying bias in a computational chemistry model used to generate training data will lead to bias in an ML model. Further, training a machine learning model may not be necessary in cases where good functional forms in physics-based models[17] already provide an excellent balance of accuracy and computational cost. An overlooked issue in the cost versus accuracy trade-off paradigm is the cost of expert knowledge. The most accurate computational chemistry calculations typically require years of accrued expert knowledge to guide the selection of active space, initial orbitals, method, and basis set, something which is impractical during large-scale data generation for ML models. In the interest of developing accurate ML models, our focus is also divided: how much should we encourage young researchers to develop expert knowledge in the traditional
Figure 2. Example of a cost (number of atoms) vs accuracy hierarchy in computational chemistry shown on a log-log scale with a typical relationship between system size and highest level of tractable theory shown by the gray line. Correction schemes for DFT that disrupt this hierarchy are shown by a green arrow labeled “ + X corrections”, reduced-scaling approaches to correlated wavefunction theory are labeled by the blue arrow indicated as “Rank reduction”, and acceleration of DFT and classical MM with algorithms tailored for GPUs are indicated by the purple arrows labeled as “Tailored for GPUs”.
Essays
Isr. J. Chem. 2022, 62, e202100016 (3 of 13) © 2021 Wiley-VCH GmbH
18695868, 2022, 1-2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ijch.202100016 by University Of Massachusetts, Wiley Online Library on [21/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License


methods that generate training data at the same time they are developing expertise on machine learning model architecture and training? Ultimately, while some may imagine a world where ML models completely replace physics-based ones, advances in traditional computational chemistry modeling are a big part of any ML-driven future. While there is indeed a tension between the old guard and new in computational chemistry, many opportunities also present themselves at this union, as I will describe in more detail in Sec. 4.
2.2 Machine Learning: What’s Old Is New Again
In laboratories across the world, scientists are identifying ways in which they can incorporate machine learning into their everyday research. This rapid growth has even led some to ask “Is machine learning overhyped?”,[18] a question which has elicited a wide range of answers. I fall into the camp that believes if the question has to be asked, the answer is probably yes. Given the sudden onslaught of machine learning and artificial intelligence both in the popular press and in scientific journals, it may be a surprise to learn that machine learning in chemistry is not new.[19] A similar surge in interest in applying machine learning (e. g., artificial neural networks or ANNs especially) occurred in the late 1980s to mid-1990s.[19c] Symposia on neural networks in chemistry were held at the American Chemical Society national meetings in the early 1990s.[19d] A detour to review this era of machine learning provides context with regard to where machine learning can take research in the chemical sciences beyond the present-day hype cycle.
Early developments in artificial intelligence and machine learning date even further back to the 1940s[20] (Figure 3). The earliest applications of data science techniques to chemical problems included the development of feedback-trained expert or decision-based systems[19a] as well as dimensionality reduction and pattern recognition on chemical data sets.[21] However, machine learning saw a resurgence in the late 1980s and 1990s that most closely resembles the excitement around the topic today due to a couple of key factors that were noted in reviews written at the time[19b–e] (Figure 3). First, developments by the computer science community had demonstrated that nonlinear models, such as neural networks with hidden layers (i. e., those that act on the data between the input and output layers), could model input-output mappings[22] that had not previously been thought possible in the early stages of simple perceptron models that lacked hidden layers[23] (Figure 4). Simultaneous developments in machine learning beyond fully connected ANNs included advances in kernel methods, such as Gaussian process regression (GPR),[24] support vector machines (SVM),[25] regularized linear regression (i. e., LASSO),[26] and early forays into the developments of convolutional neural networks[27] (CNNs, e. g., the neocognitron,[28] Figures 3 and 4). Second, the advent of supercomputing resources meant that complex models such as ANNs could be trained with the back-propagation algorithm[29] even on large data sets (e. g., 32,000 mass spectra[30]) in a reasonable amount of time, ranging from hours to weeks. This model training step is the process during which the model “learns” with feedback the mapping between observed data and the inputs (in our preceding example, the input mass spectral signals and the presence of certain substructures in the
Figure 3. Timeline depicting key developments in the computer science of machine learning (bottom, gray text) and applications of machine learning to chemistry (top, black text) with relevant timeframes indicated, as discussed in the main text.
Essays
Isr. J. Chem. 2022, 62, e202100016 (4 of 13) © 2021 Wiley-VCH GmbH
18695868, 2022, 1-2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ijch.202100016 by University Of Massachusetts, Wiley Online Library on [21/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License


molecule, Figure 4). Third, software to carry out this training was increasingly available in commercial packages around this time (Figure 3). Finally, it was appreciated that, once trained, such models could be evaluated much more quickly than competing methods, such as the developing quantum chemical methods that were still quite costly to compute on most molecules and materials.[19c,e] Were the areas that researchers applied machine learning to thirty years ago vastly different from the areas that chemists are interested in today? In short, the answer is no. Early studies focused on predicting and interpreting complex spectroscopic measurements including nuclear magnetic resonance (NMR) chemical shifts,[31] mass spectrometry signals,[30] infrared spectra,[32] and the energy levels of ions[33] (Figure 3). Reactivity prediction and synthesis planning in small-molecule organic chemistry were early targets, with applications in the electrophilic aromatic substitution reactions of substituted benzenes,[34] identification of most probable bond-breaking events,[35] or prediction of broad reaction outcomes.[36] In some cases, these models were found to outperform both expert chemists[34] as well as so-called expert systems (i. e., computer programs) that used rule-based workflows.[19c,34,37] As it became clear that non-linear ML models could augment or outperform[38] traditional, multiple linear regression quantitative structure-activity relationship (QSAR) methods,[39] neural networks were used to predict solubility[40] or toxicity[41] of drug molecules (Figures 3 and 4). Extension of machine learning to heterogeneous catalysis commenced slightly later,[42] with the first applications aimed at predicting the catalytic performance of mixed oxides.[42a] Researchers also developed ANNs that could predict protein secondary and tertiary structure[43] from sequence, based on crystal structures that were available at the time, and successfully applied these models to proteins sufficiently similar to training data.[19c,d] There was also significant interest in interfacing machine learning with physics-based modeling.[19e,44] Neural networks were used to solve the Schrödinger equation for a twodimensional, uncoupled harmonic oscillator problem,[44b] pav
ing the way for later applications of neural network models in fitting potential energy surfaces (e. g., for reactive molecular dynamics)[45] and in interpreting molecular motions.[44a] The early success of these approaches led the authors to conclude that should higher-level, quantum chemical data become more freely available, training machine learning models on such data could be quite powerful.[19e] As chemists today share many of the broad interests and goals with preceding generations, it is not surprising that many of the primary applications of modern machine learning have precedents in the early 1990s. The exhaustive application of machine learning to nearly every branch of the chemical sciences led numerous reviews to remark on the explosive growth in the early 1990s, with estimates[19b,c,e] pointing to exponential growth in the number of papers on the topic. Since this may sound familiar to current discourse, it is useful for us to ask in comparison to thirty years ago, what has stayed the same, what has changed, and what lessons from that era still apply today?
2.3 What Has Stayed the Same, What Has Changed, and What Lessons Still Apply?
What has stayed the same? Many of the essential ingredients and concerns in modern machine learning for the chemical sciences have been explored previously. Early practitioners were concerned with the best way to select a representation of the molecule for the model. They experimented with heuristics,[42a] quantum mechanical descriptors (e. g., partial charges),[34] connectivity-derived descriptors,[34,36] and threedimensional descriptions of the molecule’s response[46] (e. g., comparative molecular field analysis or CoMFA) to a probe molecule or its 3D structure (Figure 5). As an example of 3D structure, matrices in which off-diagonal elements represented some interaction strength between the relevant atoms and the diagonal elements represented the atomic number found early success[36] in predictions of reactivity (Figure 5). Similar forms
Figure 4. Schematic of some machine learning models with increasing model complexity from left to right: multiple linear regression or LASSO linear models, kernel models such as kernel ridge regression (KRR) or Gaussian process regression (GPR), and artificial neural networks. The fully connected ANN is depicted with an input layer (maroon), hidden layers (gray), and an output layer (green).
Essays
Isr. J. Chem. 2022, 62, e202100016 (5 of 13) © 2021 Wiley-VCH GmbH
18695868, 2022, 1-2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ijch.202100016 by University Of Massachusetts, Wiley Online Library on [21/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License


have been widely employed in more recent ML models.[47] In fact, considerable effort into representation choice made decades earlier need not be wasted. My research group has leveraged and extended[48] Moreau-Broto discrete autocorrelations[49] for machine learning properties of transition-metal complexes, despite the fact that initial applications[49–50] focused exclusively on organic chemistry (Figure 5). Systematic evaluation of feature sets was an important part of early ML model development, just as it is today. People were also concerned with interpretability of more complex, non-linear (i. e., ANN) models versus the transparency of simpler, interpretable models (e. g., linear regression).[19e] Although sensitivity analysis and interpretation of weights in neural networks were attempted, researchers were naturally cautious regarding what could be gleaned from these analyses.[19e,40b] Concerns about overfitting or overtraining[51] of models and the related problem of finding the optimal architecture[52] were front and center then just as they are today.[53] Quantifying uncertainty of predictions was pursued, e. g., through ensembles of models trained on distinct data subsets,[51,54] much as it has been recently.[53c,d] Finally, the question of how to best construct a data set to train a model that generalizes well concerned researchers about the broad application of machine learning[19c,55] much as it does today.[53d,56] In the realm of predicting synthesis outcomes, concerns about the lack of available data on failed or “near
miss” results was recognized thirty years ago[55] just as it has been recently.[57] What has changed? The most significant change for present-day machine learning efforts is a dramatic shift in accessibility to computational resources for data generation and model training. The curation and sharing of large (ca. 100 k–1 M)-sized data sets of small organic molecules is increasingly routine[15b,c] thanks to dramatic advances in computing power. Harnessing advances in natural language processing, researchers have begun to extract[58] vast sets of experimental data from the literature. Thus, it is now possible to train ML models to make predictions or extract patterns at scales that were not possible previously, simply due to access to more data. Powerful open-source toolkits (e. g., Keras and TensorFlow) for ML model training, including tools (e. g., Hyperopt) that simplify selection of ML model architecture[59] have also made ML more accessible. At the same time, physics-based modeling methods have advanced significantly, meaning that very accurate reference data for organic molecules, e. g., from coupled cluster theory, can be generated for model training. Developments in computer science have also enlarged the potential uses of ML. Sophisticated autoencoder models or CNNs have been applied to directly learn suitable molecular representations to bypass the need for their development, and generative models have been used to predict previously unseen chemical structures.[60]
What lessons still apply today? If many of today’s challenges and advances were known or predictable 30 years ago, it stands to reason that lessons learned then still apply. In the 1990s, machine learning was not without its critics,[61] and even its advocates recommended[19c] caution in applying ML without discretion. For instance Duch and Diercksen[61] expressed concern that in many cases, standard methods of data fitting and approximation, with fewer adjustable parameters than present in an ANN, should be both sufficient and more interpretable. Similarly, they objected[61] to the concept that using an ANN to fit solutions to a physical problem (e. g., solutions to the Schrödinger equation[44b] or a potential energy surface[62]) could be equated to a machine learning model “learning” physical laws. Others echoed this concern,[63] arguing that logic-based models were more interpretable than and performed as well as more opaque ANNs. Duch and Diercksen[61] asked of the growing tide of ML applications that appeared to be replacing conventional physics-based studies, “Is this enthusiasm well founded?” I would argue that one of the most valuable lessons to learn from this earlier heyday of machine learning is that one should be cautious in how much meaning one ascribes to what is ultimately a regression of an incomplete data set. In many cases, the enthusiasm may in fact be well founded, but I believe it is always useful to question the role of ML in solving a problem, perhaps even more now than in 1994.
Figure 5. Examples of four types of molecular representations depicted on a methane molecule: (left) QM descriptors such as localized molecular orbitals, (top) 1D heuristics such as the atomic number of constituent atoms, (right) 2D connectivity such as Moreau-Broto autocorrelation functions with the equation shown in inset, and (bottom) 3D structure of the molecule with an example matrix element for the Coulomb matrix shown in inset.
Essays
Isr. J. Chem. 2022, 62, e202100016 (6 of 13) © 2021 Wiley-VCH GmbH
18695868, 2022, 1-2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ijch.202100016 by University Of Massachusetts, Wiley Online Library on [21/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License


3. Computational Chemistry: Fundamental Science Up Front, Useful Tool in the Back
“The experimental data were shown to be incompatible with this picture, but an alternative model capable of satisfactorily explaining the spectrum remained elusive.” – D. G. Leopold et al.[64]
“Some people looked down their noses at it. They might say ‘It’s not science’ or that ‘Gentlemen don’t do random mutagenesis.’ But I’m not a scientist, and I’m not a gentleman, so it didn’t bother me at all.” – Frances Arnold
Over the past few decades, computational chemistry has become an indispensable tool across chemistry and the materials sciences. Thanks to developments by theorists, electronic structure methods can be used to produce highly accurate benchmarks and near-exact results (Figure 6). Theoretical chemistry can be used to “fact check” experiment,[65] even in difficult materials spaces like the transition-metal complexes that are the focus of my group’s research. Carefully developed software packages[6c,66] and workflows[14] make computational chemistry increasingly fast and easy to use. As a result, computational chemistry tools are ubiquitous in the testing of hypotheses either within experimental labs or in close collaboration with computational chemists (Figure 6). In many such cases, experiments are difficult, time-consuming and ambiguous, requiring computation to provide insight into fleeting intermediates. When we don’t yet know a mechanism, computational chemistry tools can predict one[67] (Figure 6).
At the same time, there are cases where computational chemistry is rightfully viewed more as a flawed tool than a principled science. This can both be due to the way in which the methods are applied as well as their limitations in challenging materials spaces. Experimental characterization can reveal the nature of limitations in computational models, especially for difficult materials. Taking the iron dimer as a simple example, the experimental photoelectron and Mössbauer spectra had identified an unambiguous d13 ground state,[64] but most theorists continued to predict a d14 ground state due to delocalization errors in DFT.[10a] A task we might expect to be simple, ground state (i. e., energy-based) assignment for open-shell transition-metal complexes with numerous lowlying electronic states, is instead notoriously difficult, even when the same methods may prove suitable for predicting geometries.[68] We thus might conclude, upon careful inspection of semi-empirical and highly parameterized theories, that we are carrying out a data fitting exercise[69] that is getting the right answer for what might not be the right reason[16b] (Figure 6). With the merger of machine learning and computational chemistry, the question of fundamental science versus application becomes even more complex to answer (Figure 6). Interpreting and treating ML models as physical forms themselves, while discouraged in the past,[61] could overcome some limitations of established analytical theories. On the one hand, widely used representations[45] for neural network potentials are intentionally so near-sighted that they neglect long-range physics (i. e., van der Waals and electrostatics) that we know to be essential to describing matter. Models can thus frequently perform well but fail once outside the domain of applicability, a phenomenon which can be challenging to predict quantitatively.[53c,d] Trained ML models are able to rapidly make predictions that are reasonable estimates of DFT or time-consuming experiments, but it can be even more challenging to determine if these models are getting the right answers for the right reasons (Figure 6). My group has found that with ML-accelerated discovery both larger data sets and the models themselves can force us to confront limitations of design heuristics and “chemical intuition” that had been derived from smaller data sets[48f–h,70] (Figure 6). While inherently harder to interpret than analytical functional forms, ML models also make transparent key considerations such as overfitting versus generalization, sensitivity to parameter choice and number, the transferability of model predictions across materials compositions, and notions of chemical similarity. These ideas are also implicit but often neglected in traditional computational chemistry. For example, we know that electronic structure methods that perform very well in a narrow region of chemical space can fail catastrophically for distinct materials, but computational chemists have seldom been quantitative about method generalization.[71] While computational chemists may expect a method to fail due to the presence of the nebulously defined “strong correlation”, they do so by “intuition” or by using a plethora of heuristics rather than a single rigorous quantitative measure.
Figure 6. Alignment chart of distinct approaches to theoretical chemistry, computational chemistry, and machine learning, distinguished loosely by their focus on fundamental understanding vs practical results (top to bottom) and more traditional theoretical/ computational chemistry vs machine learning (left to right).
Essays
Isr. J. Chem. 2022, 62, e202100016 (7 of 13) © 2021 Wiley-VCH GmbH
18695868, 2022, 1-2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ijch.202100016 by University Of Massachusetts, Wiley Online Library on [21/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License


Despite the often divergent assessment from different heuristics,[48d] ML models can be developed to provide a systematic, consensus-based approach to this problem.[48d,72] Thus, while ML can be used in a black-box fashion or as a mapping of “garbage in” to “garbage out”, it is also promising as a way to overcome existing limitations in computational chemistry. So which is it: fundamental science or useful tool? To me, computational chemistry is a “mullet”: the fundamental science “up front” drives the “party in the back” where it has become a widespread service tool that has touched all other branches of chemistry. As a scientist and an engineer, I believe in the promise of machine learning to continue to drive this trend. Machine learning allows us to explore and test the limits of our hypotheses and theories while extending and exploiting the methodology that has already worked.
4. Some Achievements Unlocked and “Holy Grails” that Remain
““O Deep Thought computer,” he said, “the task we have designed you to perform is this. We want you to tell us....” he paused, “The Answer.” “The Answer?” said Deep Thought. “The Answer to what?” “Life!” urged Fook. “The Universe!” said Lunkwill. “Everything!” they said in chorus. Deep Thought paused for a moment‘s reflection. “Tricky,” he said finally. “But can you do it?” Again, a significant pause. “Yes,” said Deep Thought, “I can do it.”” –Hitchhikers Guide to the Galaxy by Douglas Adams
“If you think about it too much you might stumble, trip up,... World, the time has come to push the button.” Galvanize by The Chemical Brothers As I’ve discussed, computational chemistry and machine learning have played wide-ranging roles in increasing understanding in the chemical sciences. I’ll narrow my focus to some of the recent ways in which data-driven machine learning and computational chemistry have enlarged my own group’s understanding. We first developed molSimplify[14a] to take a divide-and-conquer approach to automating the screening of transition-metal complexes. The molSimplify[14a] code uses as a backend OpenBabel,[73] which is a powerful tool for high-throughput screening in organic chemistry that required adaptations for transition-metal chemistry. We were inspired by the numerous discovery efforts in organic chemistry and the solid-state communities that had benefitted from established software workflows.[14,73] We made judicious use of GPU-accelerated quantum chemistry[6c] to rapidly screen transition-metal complexes through automated calculation management and property optimization extensions to molSimplify.[74] Given the ease with which we could generate data, it was a natural next step to train the first machine learning models to predict properties of open-shell transition-metal complexes, including their ground state spin and geometric properties.[48b,h]
Our objectives were to accelerate the discovery of new complexes, such as catalysts that disrupted[48f,75] so-called “scaling relations” that present simplifying limitations in computational catalyst design. In the interest of transparency, when I first came to machine learning, I came as a skeptic to the idea that an ANN could predict properties of transitionmetal complexes better than existing semi-empirical theories, which tended to fail catastrophically for transition-metal chemistry,[76] or better than my intuition tuned by a decade of experience. However, I was quickly humbled by the limits of my own “chemical intuition”, an experience I suspect many who wade into the areas of big data and machine learning have shared. Our data (i. e., optimized geometries of openshell transition-metal complexes) were harder to come by and thus the data set sizes were significantly smaller[48b,h] (ca. 3001000 points) than those widely used in organic chemistry (ca. 100 k points).[15b,c] Still, using tailored, graph-based representations,[48b,h] we built reasonably accurate (1–3 kcal/ mol mean absolute error) models that outperformed those trained on traditional whole-molecule descriptors widely used for organic molecules.[47] With such models in hand, we predicted DFT-level properties and the sensitivity to DFT functional choice[48g,h] in seconds instead of days (Figure 7). We, like others,[78] then set about to design new materials with machine learning models. One of our biggest lessons was the importance of uncertainty quantification. A chief danger in ML is the extent to which we may not be aware of model overconfidence. When searching for spin-crossover complexes, we found that a naïve genetic algorithm would suggest complexes very different from training data.[48c] When attempting to validate these predictions with DFT, the DFT results were very different from those predicted by the ML model.[48c] By developing ways to either limit model error in discovery[48c] or exploit it in active learning,[77] we discovered new transition-metal complexes optimized for a range of properties and applications including spin crossover,[48c] band gap,[74] methane oxidation,[48f] and redox flow batteries[77] (Figure 7). Design principles could also be directly extracted, for example from feature-selected subsets that yielded the most predictive kernel (i. e., KRR) models.[48h] For instance, analysis of features needed to predict spin splitting reveals the metal-local nature of that property,[48h] recapitulating our understanding of ligand field theory. On the other hand, redox potential is strongly size-dependent.[48h] This observation suggests it should be challenging to simultaneously optimize a redox couple’s solubility through addition of polar functional groups while increasing its redox potential[77] (Figure 7). Nevertheless, through active learning with efficient global optimization, we were able to discover lead complexes with improved properties from 2.8 million candidates in about 5 weeks instead of the 5 decades that a fully random, parallelized DFT-based search would have required[77] (Figure 7). We are certainly not the first research group to have realized the power of ML regression models for accelerating computational materials discovery.[78] ML has demonstrated significant promise to sidestep normally tedious calculations in
Essays
Isr. J. Chem. 2022, 62, e202100016 (8 of 13) © 2021 Wiley-VCH GmbH
18695868, 2022, 1-2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ijch.202100016 by University Of Massachusetts, Wiley Online Library on [21/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License


unearthing new materials. Nevertheless, in open-shell transition-metal chemistry, unique considerations come to the forefront. In particular, many structures fail to remain intact[48e,74] or are otherwise not suitable for DFT[48e] in a manner that would erode ML model performance and subsequently the leads obtained from ML-accelerated discovery. Still, there are ways to leverage ML to also overcome these challenges as a complement to traditional efforts aimed at improving method accuracy in computational chemistry. The likelihood of calculation success can be predicted with ML much like any other property, and failures can be predicted and detected “in situ” through ML models trained on properties of the wavefunction.[48e] With regard to the strong sensitivity of properties to DFT functional choice, we can use geometric structure[68] to identify the experimental ground state spin and use ML models that predict property variation with changes in DFT functional.[48g] We can also detect regions of low and high multi-reference character,[48d,72] where the former are “safe islands” for DFT but the latter would motivate multireference treatment. These advances are useful to ensure that ML-accelerated computational discovery workflows deliver
trustworthy results without requiring extensive manual assessment of calculations. Although machine-learning-accelerated discovery has demonstrated great promise in a short time, there is a significant distance to reach the “holy grail” in this research area. The ultimate goal for materials discovery on a computer, and in transition-metal catalysis in particular, is to reach a point where not only can large materials spaces be explored orders of magnitude more quickly than through traditional computation or experiment but that materials designed in this fashion are as readily realized and adopted in comparison to products of traditional experimentation. This means that energetics used to predict catalytic or functional materials properties from the computational chemistry method or trained ML model must be obtained at unprecedented accuracy. Workflows and models that can detect[48d,e,72,74,79] method inaccuracy in order to adapt and improve in new materials spaces will be essential (Figure 8). Such tools will guide method improvements that might be needed particularly for newly discovered regions of strongly correlated materials or could be used to automate reparameterization of physics-based models on the fly.
Figure 7. Example of our approach for ML-accelerated materials discovery in transition-metal chemistry. (left) ML models predict DFT-level properties of transition-metal complexes such as the high-spin to low-spin (H L) spin-splitting energy, ΔEH-L (in kcal/mol), with varying ligand field strength to recapitulate the DFT-derived spectrochemical series. (middle) Analysis of features selected in ML models reveals lengthscales and constituent atomic properties that contribute most to predicting distinct molecular properties such as spin splitting or redox. (right) ML-accelerated design with active learning can simultaneously improve multiple properties in a fraction of the time full DFT would require. Adapted with permission from refs. [48g, 48h, 74, 77]. Copyright 2017–2020 the American Chemical Society.
Figure 8. Schematic of the “holy grail” in ML-accelerated discovery: an end-to-end workflow that carries the researcher from design objective to new design rules. Outstanding challenges to realize this goal are highlighted, including: determining when the chemical space is balanced between accessible materials and hypothetical materials, adapting the relevant level of theory with artificial intelligence, and addressing multiple objectives in ML-accelerated materials optimization.
Essays
Isr. J. Chem. 2022, 62, e202100016 (9 of 13) © 2021 Wiley-VCH GmbH
18695868, 2022, 1-2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ijch.202100016 by University Of Massachusetts, Wiley Online Library on [21/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License


For a material to be designed on a computer, we must also devise ways to robustly predict and optimize the material’s synthesizability;[80] interrelated objectives such as mechanism, activity, stability, and solubility;[67,77,81] and economic or environmental cost[82] (Figure 8). Such tools should ultimately be used with robotic chemistry workflows either in a one-shot or iterative method to take manual synthesis out of the loop. Although active learning has been demonstrated as a way for models to iteratively explore new materials spaces,[77,83] we are also starting to understand[70,84] that bias in both experimental and computational data sets limits discovery of both design rules and new materials. The holy grail of materials discovery mandates that we develop a better understanding of these biases and devise new ways to enumerate[77,85] or generate[60] materials that overcome current limitations (Figure 8). It is also probably not enough to just have a black-box tool that designs materials, but we will want to develop new ways for scientists and machine learning models to interact so that both expert-driven and ML-driven hypotheses of design rules can be tested iteratively. We will need new visualization and analysis tools that illuminate discovered design principles. We will need to better quantify and understand uncertainty[53c,d] and bias in both model and expert predictions and design rules. If such tools come to fruition, they would change how many chemists carry out research. It would free us up to imagine new problems or design targets and to find solutions on an unprecedented timescale. Of course this sounds ambitious, but I believe that transforming machine-learning-accelerated computational chemistry from a useful tool to a platform in which lead compounds are treated as “real” as a molecule in a beaker is a realistic goal. Not only that but, by confronting some of the technical challenges in this process, I expect that we will achieve many new “firsts” in ML-accelerated discovery along with a better understanding of the limitations of the expert in the domains of chemical discovery and traditional electronic structure.
5. Outlook: What About the Next 20–30 Years?
“Ask again later” – Magic 8-Ball
It demands significant chutzpah to say what I think the next 20–30 years could look like for computational chemistry and machine learning. Although we have already covered it earlier in this Rosarium, it might help to first recall the status quo three decades ago. Thirty years ago, DFT had just started to move from theory to practice with the first generalized gradient approximations that could produce useful thermochemistry. Due to the steep scaling of first-principles methods, molecular mechanics and semi-empirical quantum chemistry remained the main means of studying molecules larger than a few atoms. The materials that chemists were interested in then —earth-abundant transition-metal catalysts, functional materials for sensors and magnets, and useful materials for energy conversion such as dyes—largely remain the same despite emerging prominence of some new materials classes (e. g.,
metal-organic frameworks). What is different now is that a single calculation on one of these materials requires vastly less time. While thirty years ago machine learning was on the rise with the training of the first neural networks to accelerate prediction of properties in chemistry, such efforts took days to weeks. In short, a lot can happen in 20–30 years, but a lot will stay the same. While we have ever increasingly accurate models in DFT and WFT, none have truly overcome the challenges of balancing delocalization error and static correlation error. Although the path to an “exact method” should exist, there has been a proliferation of a “zoo” of both DFT and WFT methods in the past 20–30 years, with no one reigning supreme. It is easy to imagine that this problem may not be solved in the next few decades either, but continued improvement in computing power and algorithms will likely make accurate and efficient methods more and more tractable. Moreover, machine learning is poised to act as a lens through which we can see methods as systematically adaptable as we traverse through new materials spaces. The models themselves will provide a path for the functional to evolve with the material. On the machine learning side, it is reasonable to expect that while computer scientists may take the lead in developing more and more sophisticated models, that chemists may succeed in using transparent and interpretable ones. Computer science may also provide better paths to interpreting increasingly complex models. Finally, one should hope that 20–30 years from now there will remain a healthy skepticism towards computational chemistry and machine learning alike by scientists who have developed their expert knowledge in chemistry through other paths, but hopefully no more skepticism than is just enough to keep us on our toes to include more and more realism into our calculations. In truth, I hardly recognize the computational chemistry that I mentor my students to advance as the same field that I myself came into. The challenges they encounter are ones that I could not have anticipated when I first started in the field, and many of the challenges that used to preoccupy me are ones that we have in large part made trivial as a community. My biggest hope for the next few decades is that this is a trend that continues, with each year bringing a “level up” in the way we think about carrying out chemistry on a computer.
Acknowledgement
This work has been generously supported over the years by the Office of Naval Research under grant numbers N0001417-1-2956, N00014-18-1-2434, and N00014-20-1-2150, DARPA grant D18AP00039, the Department of Energy under grant numbers DE-SC0018096 and DE-SC0012702, the National Science Foundation under grant numbers CBET-1704266 and CBET-1846426, an AAAS Marion Milligan Mason Award, and a Career Award at the Scientific Interface from the Burroughs Wellcome Fund. I’d like to thank Adam H. Steeves
Essays
Isr. J. Chem. 2022, 62, e202100016 (10 of 13) © 2021 Wiley-VCH GmbH
18695868, 2022, 1-2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ijch.202100016 by University Of Massachusetts, Wiley Online Library on [21/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License


for providing a critical reading of the manuscript along with the students and mentors whom I’ve had the pleasure to work with over the past several years.
References
[1] a) C. Dykstra, G. Frenking, K. Kim, G. Scuseria, Theory and applications of computational chemistry: the first forty years, Elsevier, 2011; b) A. D. Becke, J. Chem. Phys. 2014, 140, 18, A301. [2] a) W. Kohn, Rev. Mod. Phys. 1999, 71, 1253; b) J. A. Pople, Rev. Mod. Phys. 1999, 71, 1267.
[3] D. A. Doyle, J. M. Cabral, R. A. Pfuetzner, A. Kuo, J. M. Gulbis, S. L. Cohen, B. T. Chait, R. MacKinnon, Science 1998, 280, 6977. [4] R. MacKinnon, Biosci. Rep. 2004, 24, 75. [5] S. E. J. Bowman, J. Bridwell-Rabb, C. L. Drennan, Acc. Chem. Res. 2016, 49, 695–702. [6] a) Á. Jász, Á. Rák, I. Ladjánszki, G. Cserey, Wiley Interdiscip. Rev.: Comput. Mol. Sci. 2020, 10, e1444; b) I. S. Ufimtsev, T. J. Martinez, J. Chem. Theory Comput. 2008, 4, 222–231; c) S. Seritan, C. Bannwarth, B. S. Fales, E. G. Hohenstein, S. I. Kokkila-Schumacher, N. Luehr, J. W. Snyder Jr, C. Song, A. V. Titov, I. S. Ufimtsev, J. Chem. Phys. 2020, 152, 224110; d) C. Riplinger, F. Neese, J. Chem. Phys. 2013, 138, 034106. [7] a) E. G. Hohenstein, R. M. Parrish, T. J. Martínez, J. Chem. Phys. 2012, 137, 044103; b) F. Weigend, A. Köhn, C. Hättig, J. Chem. Phys. 2002, 116, 3175–3183.
[8] Q. Ma, H. J. Werner, Wiley Interdiscip. Rev.: Comput. Mol. Sci. 2018, 8, e1371. [9] a) L. Kronik, T. Stein, S. Refaely-Abramson, R. Baer, J. Chem. Theory Comput. 2012, 8, 1515–1531; b) J. Toulouse, F. Colonna, A. Savin, Phys. Rev. A 2004, 70, 062505. [10] a) H. J. Kulik, J. Chem. Phys. 2015, 142, 240901; b) A. Liechtenstein, V. I. Anisimov, J. Zaanen, Phys. Rev. B 1995, 52, R5467. [11] a) Q. Zhao, E. I. Ioannidis, H. J. Kulik, J. Chem. Phys. 2016, 145, 054109; b) A. Bajaj, J. P. Janet, H. J. Kulik, J. Chem. Phys. 2017, 147, 191101. [12] S. Grimme, J. Comput. Chem. 2006, 27, 1787–1799. [13] a) H. J. Kulik, N. Seelam, B. Mar, T. J. Martinez, J. Phys. Chem. A 2016, 120, 5939–5949; b) H. Kruse, S. Grimme, J. Chem. Phys. 2012, 136, 04B613; c) A. Otero-De-La-Roza, G. A. DiLabio, J. Chem. Theory Comput. 2017, 13, 3505–3524.
[14] a) E. I. Ioannidis, T. Z. H. Gani, H. J. Kulik, J. Comput. Chem. 2016, 37, 2106–2117; b) S. Curtarolo, W. Setyawan, G. L. Hart, M. Jahnatek, R. V. Chepulskii, R. H. Taylor, S. Wang, J. Xue, K. Yang, O. Levy, Comput. Mater. Sci. 2012, 58, 218–226; c) S. P. Ong, W. D. Richards, A. Jain, G. Hautier, M. Kocher, S. Cholia, D. Gunter, V. L. Chevrier, K. A. Persson, G. Ceder, Comput. Mater. Sci. 2013, 68, 314–319.
[15] a) K. Alberi, M. B. Nardelli, A. Zakutayev, L. Mitas, S. Curtarolo, A. Jain, M. Fornari, N. Marzari, I. Takeuchi, M. L. Green, J. Phys. D 2018, 52, 013001; b) J. S. Smith, O. Isayev, A. E. Roitberg, Sci. Data 2017, 4, 170193; c) R. Ramakrishnan, P. O. Dral, M. Rupp, O. A. Von Lilienfeld, Sci. Data 2014, 1, 140022. [16] a) T. Z. H. Gani, H. J. Kulik, J. Chem. Theory Comput. 2016, 12, 5931–5945; b) M. G. Medvedev, I. S. Bushmarinov, J. Sun, J. P. Perdew, K. A. Lyssenko, Science 2017, 355, 49–52.
[17] a) F. Paesani, Acc. Chem. Res. 2016, 49, 1844–1851; b) J. W. Ponder, C. Wu, P. Ren, V. S. Pande, J. D. Chodera, M. J. Schnieders, I. Haque, D. L. Mobley, D. S. Lambrecht, R. A. DiStasio Jr, J. Phys. Chem. B 2010, 114, 2549–2564. [18] S. Lemonick, Chem. Eng. News 2018, 96, 16–20. [19] a) P. C. Jurs, B. R. Kowalski, T. L. Isenhour, Anal. Chem. 1969, 41, 21–27; b) J. Zupan, J. Gasteiger, Anal. Chim. Acta 1991, 248, 1–30; c) J. Gasteiger, J. Zupan, Angew. Chem. Int. Ed. 1993, 32, 503–527; Angew. Chem. 1993, 105, 510–536; d) J. A. Burns, G. M. Whitesides, Chem. Rev. 1993, 93, 2583–2601; e) B. G. Sumpter, C. Getino, D. W. Noid, Annu. Rev. Phys. Chem. 1994, 45, 439–481; f) V. Venkatasubramanian, AIChE J. 2019, 65, 466478. [20] a) W. S. McCulloch, W. Pitts, Bull. Math. Biol. 1943, 5, 115–133; b) W. Pitts, W. S. McCulloch, Bull. Math. Biol. 1947, 9, 127–147. [21] a) B. R. Kowalski, C. F. Bender, J. Am. Chem. Soc. 1972, 94, 5632–5639; b) A. J. Stuper, P. C. Jurs, J. Chem. Inf. Comput. Sci. 1976, 16, 99–105; c) G. S. Zander, A. J. Stuper, P. C. Jurs, Anal. Chem. 1975, 47, 1085–1093. [22] J. J. Hopfield, Proc. Mont. Acad. Sci. 1982, 79, 2554–2558.
[23] M. Minsky, S. A. Papert, Perceptrons: An introduction to computational geometry, MIT press, 2017. [24] C. E. Rasmussen, Gaussian processes in machine learning, Springer, 2003. [25] C. Cortes, V. Vapnik, Mach. Learn. 1995, 20, 273–297. . [26] R. Tibshirani, J. R. Stat. Soc.: Ser. B 1996, 58, 267–288. .
[27] Y. LeCun, Y. Bengio, The handbook of brain theory and neural networks 1995, 3361, 1995.
[28] K. Fukushima, Biol. Cybern. 1980, 36, 193–202. [29] D. E. Rumelhart, G. E. Hinton, R. J. Williams, Nature 1986, 323, 533–536. [30] B. Curry, D. E. Rumelhart, Tetrahedron Comput. Methodol. 1990, 3, 213–237. [31] a) J. U. Thomsen, B. Meyer, J. Magn. Reson. 1989, 84, 212–217; b) B. Meyer, T. Hansen, D. Nute, P. Albersheim, A. Darvill, W. York, J. Sellers, Science 1991, 251, 542–544; c) V. Kvasnicka, S. Sklenak, J. Pospichal, J. Chem. Inf. Comput. Sci. 1992, 32, 742747. [32] a) E. W. Robb, M. E. Munk, Microchim. Acta 1990, 100, 131155; b) B. J. Wythoff, S. P. Levine, S. A. Tomellini, Anal. Chem. 1990, 62, 2702–2709; c) M. E. Munk, M. S. Madison, E. W. Robb, Microchim. Acta 1991, 104, 505–514. [33] K. L. Peterson, Phys. Rev. A 1990, 41, 2457. [34] D. W. Elrod, G. M. Maggiora, R. G. Trenary, J. Chem. Inf. Comput. Sci. 1990, 30, 477–484.
[35] V. Simon, J. Gasteiger, J. Zupan, J. Am. Chem. Soc. 1993, 115, 9148–9159. [36] D. W. Elrod, G. M. Maggiora, R. G. Trenary, Tetrahedron Comput. Methodol. 1990, 3, 163–174.
[37] T. D. Salatin, W. L. Jorgensen, J. Org. Chem. 1980, 45, 20432051. [38] a) T. Aoyama, Y. Suzuki, H. Ichikawa, J. Med. Chem. 1990, 33, 2583–2590; b) T. Aoyama, Y. Suzuki, H. Ichikawa, J. Med. Chem. 1990, 33, 905–908; c) F. R. Burden, Quant. Struct.-Act. Relat. 1996, 15, 7–11; d) F. R. Burden, D. A. Winkler, J. Med. Chem. 1999, 42, 3183–3187. [39] C. Hansch, Acc. Chem. Res. 1969, 2, 232–239. [40] a) J. Huuskonen, M. Salo, J. Taskinen, J. Pharm. Sci. 1997, 86, 450–454; b) J. Huuskonen, M. Salo, J. Taskinen, J. Chem. Inf. Comput. Sci. 1998, 38, 450–456.
[41] D. Bahler, B. Stone, C. Wellington, D. W. Bristol, J. Chem. Inf. Comput. Sci. 2000, 40, 906–914.
Essays
Isr. J. Chem. 2022, 62, e202100016 (11 of 13) © 2021 Wiley-VCH GmbH
18695868, 2022, 1-2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ijch.202100016 by University Of Massachusetts, Wiley Online Library on [21/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License


[42] a) T. Hattori, S. Kito, Catal. Today 1995, 23, 347–355; b) J. M. Serra, A. Corma, A. Chica, E. Argente, V. Botti, Catal. Today 2003, 81, 393–403. [43] a) S. Brunak, J. Engelbrecht, S. Knudsen, Nucleic Acids Res. 1990, 18, 4797–4801; b) S. Brunak, J. Engelbrecht, S. Knudsen, Nature 1990, 343, 123; c) L. H. Holley, M. Karplus, Proc. Mont. Acad. Sci. 1989, 86, 152–156; d) H. Bohr, J. Bohr, S. Brunak, R. M. J. Cotterill, B. Lautrup, L. Nørskov, O. H. Olsen, S. B. Petersen, FEBS Lett. 1988, 241, 223–228; e) H. Bohr, J. Bohr, S. Brunak, R. M. J. Cotterill, H. Fredholm, B. Lautrup, S. B. Petersen, FEBS Lett. 1990, 261, 43–46; f) J. Bohr, H. Bohr, S. Brunak, R. M. J. Cotterill, H. Fredholm, B. Lautrup, S. B. Petersen, J. Mol. Biol. 1993, 231, 861–869. [44] a) B. G. Sumpter, C. Getino, D. W. Noid, J. Chem. Phys. 1992, 97, 293–306; b) J. A. Darsey, D. W. Noid, B. R. Upadhyaya, Chem. Phys. Lett. 1991, 177, 189–194.
[45] J. Behler, M. Parrinello, Phys. Rev. Lett. 2007, 98, 146401. [46] K. B. Lipkowitz, M. Pradhan, J. Org. Chem. 2003, 68, 46484656. [47] M. Rupp, A. Tkatchenko, K.-R. Müller, O. A. Von Lilienfeld, Phys. Rev. Lett. 2012, 108, 058301.
[48] a) J. P. Janet, T. Z. H. Gani, A. H. Steeves, E. I. Ioannidis, H. J. Kulik, Ind. Eng. Chem. Res. 2017, 56, 4898–4910; b) J. P. Janet, H. J. Kulik, Chem. Sci. 2017, 8, 5137–5152; c) J. P. Janet, L. Chan, H. J. Kulik, J. Phys. Chem. Lett. 2018, 9, 1064–1071; d) C. Duan, F. Liu, A. Nandy, H. J. Kulik, J. Chem. Theory Comput. 2020, 16, 4373–4387; e) C. Duan, J. P. Janet, F. Liu, A. Nandy, H. J. Kulik, J. Chem. Theory Comput. 2019, 15, 2331–2345; f) A. Nandy, J. Zhu, J. P. Janet, C. Duan, R. B. Getman, H. J. Kulik, ACS Catal. 2019, 9, 8243–8255; g) J. P. Janet, F. Liu, A. Nandy, C. Duan, T. Yang, S. Lin, H. J. Kulik, Inorg. Chem. 2019, 58, 10592–10606; h) J. P. Janet, H. J. Kulik, J. Phys. Chem. A 2017, 121, 8939–8954. [49] P. Broto, G. Moreau, C. Vandycke, Eur. J. Med. Chem. 1984, 19, 71–78. [50] L. S. Ramos, K. R. Beebe, W. P. Carey, E. Sanchez, B. C. Erickson, B. E. Wilson, L. E. Wangen, B. R. Kowalski, Anal. Chem. 1986, 58, 294–315. [51] I. V. Tetko, D. J. Livingstone, A. I. Luik, J. Chem. Inf. Comput. Sci. 1995, 35, 826–833. [52] I. V. Tetko, A. E. P. Villa, D. J. Livingstone, J. Chem. Inf. Comput. Sci. 1996, 36, 794–803. [53] a) Y. Gal, Z. Ghahramani, in international conference on machine learning, pp. 1050–1059; b) F. Musil, M. J. Willatt, M. A. Langovoy, M. Ceriotti, J. Chem. Theory Comput. 2019, 15, 906915; c) A. A. Peterson, R. Christensen, A. Khorshidi, Phys. Chem. Chem. Phys. 2017, 19, 10978–10985; d) J. P. Janet, C. Duan, T. Yang, A. Nandy, H. J. Kulik, Chem. Sci. 2019, 10, 7913–7922. [54] D. K. Agrafiotis, W. Cedeño, V. S. Lobanov, J. Chem. Inf. Comput. Sci. 2002, 42, 903–911.
[55] H. Gelernter, J. R. Rose, C. Chen, J. Chem. Inf. Comput. Sci. 1990, 30, 492–504.
[56] a) H. J. Kulik, Wiley Interdiscip. Rev.: Comput. Mol. Sci. 2020, 10, e1439; b) J. E. Herr, K. Yao, R. McIntyre, D. W. Toth, J. Parkhill, J. Chem. Phys. 2018, 148, 241710; c) J. S. Smith, B. Nebgen, N. Lubbers, O. Isayev, A. E. Roitberg, J. Chem. Phys. 2018, 148, 241733. [57] P. Raccuglia, K. C. Elbert, P. D. F. Adler, C. Falk, M. B. Wenny, A. Mollo, M. Zeller, S. A. Friedler, J. Schrier, A. J. Norquist, Nature 2016, 533, 73.
[58] E. Kim, K. Huang, A. Saunders, A. McCallum, G. Ceder, E. Olivetti, Chem. Mater. 2017, 29, 9436–9444.
[59] J. Bergstra, D. D. Cox, D. Yamins, Proc. 12th Python Sci. Conf. 2013, 13–20. [60] B. Sanchez-Lengeling, A. Aspuru-Guzik, Science 2018, 361, 360–365. [61] W. Duch, G. H. F. Diercksen, Comput. Phys. Commun. 1994, 82, 91–103. [62] B. G. Sumpter, D. W. Noid, Chem. Phys. Lett. 1992, 192, 455462. [63] R. D. King, S. H. Muggleton, A. Srinivasan, M. J. Sternberg, Proc. Mont. Acad. Sci. 1996, 93, 438–442.
[64] D. G. Leopold, J. Almlöf, W. C. Lineberger, P. R. Taylor, J. Chem. Phys. 1988, 88, 3780–3783.
[65] T. Husch, L. Freitag, M. Reiher, J. Chem. Theory Comput. 2018, 14, 2456–2468. [66] A. Krylov, T. L. Windus, T. Barnes, E. Marin-Rimoldi, J. A. Nash, B. Pritchard, D. G. Smith, D. Altarawy, P. Saxe, C. Clementi, J. Chem. Phys. 2018, 149, 180901. [67] a) S. Kozuch, S. Shaik, Acc. Chem. Res. 2011, 44, 101–110; b) L.-P. Wang, A. Titov, R. McGibbon, F. Liu, V. S. Pande, T. J. Martínez, Nat. Chem. 2014, 6, 1044–1048. [68] a) M. G. Taylor, T. Yang, S. Lin, A. Nandy, J. P. Janet, C. Duan, H. J. Kulik, J. Phys. Chem. A 2020, 124, 3286–3299; b) K. P. Jensen, J. Cirera, J. Phys. Chem. A 2009, 113, 10033–10039. [69] F. Jensen, J. Chem. Theory Comput. 2018, 14, 4651–4661. [70] S. M. Moosavi, A. Nandy, K. M. Jablonka, D. Ongari, J. P. Janet, P. G. Boyd, Y. Lee, B. Smit, H. J. Kulik, Nat. Commun. 2020, 11, 4068. [71] J. J. Mortensen, K. Kaasbjerg, S. L. Frederiksen, J. K. Nørskov, J. P. Sethna, K. W. Jacobsen, Phys. Rev. Lett. 2005, 95, 216401. [72] a) C. Duan, F. Liu, A. Nandy, H. J. Kulik, J. Phys. Chem. Lett. 2020, 11, 6640–6648; b) F. Liu, C. Duan, H. J. Kulik, J. Phys. Chem. Lett. 2020, 11, 8067–8076.
[73] a) N. M. O’Boyle, M. Banck, C. A. James, C. Morley, T. Vandermeersch, G. R. Hutchison, J. Cheminf. 2011, 3, 33; b) N. M. O’Boyle, C. Morley, G. R. Hutchison, Chem. Cent. J. 2008, 2, 5.
[74] A. Nandy, C. Duan, J. P. Janet, S. Gugler, H. J. Kulik, Ind. Eng. Chem. Res. 2018, 57, 13973–13986. [75] T. Z. H. Gani, H. J. Kulik, ACS Catal. 2018, 8, 975–986. [76] Y. Minenkov, D. I. Sharapa, L. Cavallo, J. Chem. Theory Comput. 2018, 14, 3428–3439. [77] J. P. Janet, S. Ramesh, C. Duan, H. J. Kulik, ACS Cent. Sci. 2020, 6, 513–524. [78] a) P. Friederich, G. dos Passos Gomes, R. De Bin, A. AspuruGuzik, D. Balcells, Chem. Sci. 2020, 11, 4584–4601; b) M. Zhong, K. Tran, Y. Min, C. Wang, Z. Wang, C.-T. Dinh, P. De Luna, Z. Yu, A. S. Rasouli, P. Brodersen, S. Sun, O. Voznyy, C.-S. Tan, M. Askerka, F. Che, M. Liu, A. Seifitokaldani, Y. Pang, S.-C. Lo, A. Ip, Z. Ulissi, E. H. Sargent, Nature 2020, 581, 178–183; c) R. Gómez-Bombarelli, J. Aguilera-Iparraguirre, T. D. Hirzel, D. Duvenaud, D. Maclaurin, M. A. Blood-Forsythe, H. S. Chae, M. Einzinger, D.-G. Ha, T. Wu, Nat. Mater. 2016, 15, 1120–1127; d) B. Meredig, A. Agrawal, S. Kirklin, J. E. Saal, J. Doak, A. Thompson, K. Zhang, A. Choudhary, C. Wolverton, Phys. Rev. B 2014, 89, 094104.
[79] a) S. McAnanama-Brereton, M. P. Waller, J. Chem. Inf. Model. 2017, 58, 61–67; b) C. J. Stein, M. Reiher, J. Chem. Theory Comput. 2016, 12, 1760–1771; c) W. Jeong, S. J. Stoneburner, D. King, R. Li, A. Walker, R. Lindh, L. Gagliardi, J. Chem. Theory Comput. 2020.
[80] F. T. Szczypiński, S. Bennett, K. E. Jelfs, Chem. Sci. 2021, 12, 830-840. [81] a) Y. Himeda, N. Onozawa-Komatsuzaki, H. Sugihara, K. Kasuga, Organometallics 2007, 26, 702–712; b) S. Feng, M.
Essays
Isr. J. Chem. 2022, 62, e202100016 (12 of 13) © 2021 Wiley-VCH GmbH
18695868, 2022, 1-2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ijch.202100016 by University Of Massachusetts, Wiley Online Library on [21/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License


Chen, L. Giordano, M. Huang, W. Zhang, C. V. Amanchukwu, R. Anandakathir, Y. Shao-Horn, J. A. Johnson, J. Mater. Chem. A 2017, 5, 23987–23998. [82] a) R. M. Bullock, J. G. Chen, L. Gagliardi, P. J. Chirik, O. K. Farha, C. H. Hendon, C. W. Jones, J. A. Keith, J. Klosin, S. D. Minteer, Science 2020, 369; b) M. J. Orella, S. M. Brown, M. E. Leonard, Y. Román-Leshkov, F. R. Brushett, Energy Technol. 2019, 1900994. [83] a) H. C. Herbol, W. Hu, P. Frazier, P. Clancy, M. Poloczek, npj Comput. Mater. 2018, 4, 51. ; b) J. M. Hernández-Lobato, J. Requeima, E. O. Pyzer-Knapp, A. Aspuru-Guzik, in: Proceedings of the 34th International Conference on Machine Learning, Vol.
70 (Eds.: P. Doina, T. Yee Whye), PMLR, Proceedings of Machine Learning Research, 2017, pp. 1470–1479. [84] X. Jia, A. Lynch, Y. Huang, M. Danielson, I. Lang’at, A. Milder, A. E. Ruby, H. Wang, S. A. Friedler, A. J. Norquist, Nature 2019, 573, 251–255. [85] a) S. Gugler, J. P. Janet, H. J. Kulik, Mol. Syst. Des. Eng. 2020, 5, 139–152; b) L. Ruddigkeit, R. Van Deursen, L. C. Blum, J.-L. Reymond, J. Chem. Inf. Model. 2012, 52, 2864–2875.
Manuscript received: March 4, 2021 Version of record online: April 15, 2021
Essays
Isr. J. Chem. 2022, 62, e202100016 (13 of 13) © 2021 Wiley-VCH GmbH
18695868, 2022, 1-2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ijch.202100016 by University Of Massachusetts, Wiley Online Library on [21/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License