A physics-aware, probabilistic machine learning framework for
coarse-graining high-dimensional systems in the Small Data
regime
Constantin Grigo1 and Phaedon-Stelios Koutsourelakis∗1
1Department of Mechanical Engineering, Technical University of Munich, Boltzmannstr. 15, 85748 Munich, Germany
September 10, 2019
Abstract
The automated construction of coarse-grained models represents a pivotal component in computer simulation of physical systems and is a key enabler in various analysis and design tasks related to uncertainty quantification. Pertinent methods are severely inhibited by the high-dimension of the parametric input and the limited number of training input/output pairs that can be generated when computationally demanding forward models are considered. Such cases are frequently encountered in the modeling of random heterogeneous media where the scale of the microstructure necessitates the use of high-dimensional random vectors and very fine discretizations of the governing equations. The present paper proposes a probabilistic Machine Learning framework that is capable of operating in the presence of Small Data by exploiting aspects of the physical structure of the problem as well as contextual knowledge. As a result, it can perform comparably well under extrapolative conditions. It unifies the tasks of dimensionality and model-order reduction through an encoder-decoder scheme that simultaneously identifies a sparse set of salient lower-dimensional microstructural features and calibrates an inexpensive, coarse-grained model which is predictive of the output. Information loss is accounted for and quantified in the form of probabilistic predictive estimates. The learning engine is based on Stochastic Variational Inference. We demonstrate how the variational objectives can be used not only to train the coarse-grained model, but also to suggest refinements that lead to improved predictions.
Keywords: Uncertainty Quantification, Bayesian Inference, Coarse-Graining, Variational Inference, Random Media, Multi-Physics Models
∗Corresponding author p.s.koutsourelakis@tum.de, constantin.grigo@tum.de
Source code as well as examples can be found at: github.com/congriUQ/physics aware surrogate
1
arXiv:1902.03968v2 [stat.ML] 9 Sep 2019


1 Introduction
The present paper is concerned with the data-driven construction of coarse-grained descriptions and predictive surrogate models in the context of random heterogeneous materials. This is of paramount importance for carrying out various uncertainty quantification (UQ) tasks, both in terms of forward and backward uncertainty propagation. More generally, the development of efficient surrogates is instrumental in enabling many-query applications in the presence of complex models of physical and engineering systems. The significant progress that has been materialized in the fields of statistical or machine learning Ghahramani (2015) in the last few years, particularly in the context of Deep Learning LeCun et al. (2015); Goodfellow et al. (2016) has found its way in physical systems where training data for the surrogates are generated by forward simulations of the reference, high-fidelity model. We note however that such problems exhibit (at least) two critical differences as compared to typical machine learning applications. Firstly, the dimension of the (random) inputs and outputs encountered in complex physical models is generally much larger than speech- or image-related data and associated tasks. Furthermore, the dependencies between these two sets of variables cannot be discovered with brute-force dimensionality reduction techniques. Secondly and most importantly, these are not Big Data problems. The abundance of data that can be rightfully assumed in many data-driven contexts, is contrary to the nature of the problem we are trying to address, that is learning surrogate models, or more generally enabling UQ tasks, with the least possible high-fidelity output data. Such regimes could be more appropriately be referred to as Small Data or better yet Tall Data in consideration of the dimensionality above Koutsourelakis et al. (2016). The development of surrogates for UQ purposes in the context of continuum thermodynamics where pertinent models are based on PDEs and ODEs has a long history and some of the most well-studied methods have been based on (generalized) Polynomial Chaos expansions (gPC) Wiener (1938), Ghanem and Spanos (1991), Xiu and Karniadakis (2002) which have become popular in the last decade also due to the emergence of non-intrusive, sparse-grid stochastic collocation approaches Xiu and Hesthaven (2005); Ma and Zabaras (2009); Lin and Tartakovsky (2009). Despite recent progress in connecting gPC with domain decomposition methods Lin et al. (2010); Tipireddy et al. (2017, 2018), these approaches typically struggle in problems with high-dimensional stochastic input, as is the case with random heterogeneous materials Torquato and Lu (1993) encountered in problems such as flow through porous media Bilionis et al. (2013); Atkinson and Zabaras (2019); Mo et al. (2018) or electromagnetic wave propagation through random permittivity fields Chauvi`ere et al. (2006). Another well-established strategy makes use of statistical couplings between the reference model and less expensive, lower-fidelity models in order to enable multi-fidelity methods O’Hagan and Kennedy (2000). In the context of PDEs, such lower-fidelity or reduced-order models may readily be constructed by utilizing potentially much coarser spatio-temporal discretizations which, in conjunction with appropriate statistical models, are capable of providing fully probabilistic predictions Koutsourelakis (2009); Perdikaris et al. (2017). The most striking benefit of such methods is that they automatically retain some of the underlying physics of the high-fidelity model and are therefore able to produce accurate predictions even when the data is scarce and the stochastic input dimensionality is high. Another category of algorithms falling under the class of reduced-order models is given by reduced-basis (RB) methods Hesthaven et al. (2016); Quarteroni et al. (2016) where, based on a small set of ‘snapshots’ i.e. forward model evaluations, the solution space dimensionality is reduced by projection onto the principal ‘snapshot’ directions. The reduced-order solution coefficients are then found either by standard Galerkin projection Rowley et al. (2004); Cui et al.
2


(2015); Afkham and Hesthaven (2017) or regression-based strategies Yu and Hesthaven (2018); Guo and Hesthaven (2018); Hesthaven and Ubbiali (2018). Apart from issues of efficiency and stability, both gPC and RB approaches in their standard form are generally treated in a non-Bayesian way and therefore only yield point estimates instead of full predictive posterior distributions. A more recent trend is to view surrogate modeling as a supervised learning problem that can be handled by pertinent statistical learning tools, e.g. Gaussian Process (GP) regression Rasmussen and Williams (2006); Bilionis and Zabaras (2017) which provide closed-form predictive distributions by default. Although recent advances have been made towards multi-fidelity data fusion O’Hagan and Kennedy (2000); Raissi et al. (2017); Perdikaris et al. (2015) and incorporation of background physics Raissi and Karniadakis (2018); Yang et al. (2018); Lee et al. (2018); Tipireddy and Tartakovsky (2018) via Gaussian Processes, the poor scaling behavior with stochastic input dimension remains one of the main challenges for GP models. More recently, deep neural networks (DNNs) LeCun et al. (2015); Goodfellow et al. (2016) have found their way into surrogate modeling of complex computer codes Tripathy and Bilionis (2018); Zhu and Zabaras (2018); Mo et al. (2018); Yang and Perdikaris (2019). One of the most promising developments in the integration of such tools in physical modeling are physics-informed neural networks Raissi et al. (2017, 2019); Tartakovsky et al. (2018) which are trained by minimization of a loss function augmented by the residuals of the governing equations Raissi et al. (2017) and Raissi et al. (2017). In Zhu et al. (2019), this methodology is generalized and extended to cases without any labeled data, i.e. without computing the output of the reference, high-fidelity model. However, such models transfer much of the numerical burden from the generation of training data to the repeated evaluation of the residual during model training. While promising results have been obtained, we note that several challenges remain with regards to the incorporation of prior physical knowledge in the form of symmetries/invariances etc. into DNNs as well as their capabilities under extrapolative conditions. In this paper, we present a novel, fully Bayesian strategy for surrogate modeling designed for problems characterized by
• high input dimension, i.e. the “curse of dimensionality” – in the problems examined, we consider input data (random material microstructures) of effective dimensionality & 104; and
• small number of training data N , i.e. forward or fine grained model (FGM) input/output pairs that can practicably be generated in order to train any sort of reduced-order model or surrogate – in the examples presented here, we use N . 100 training samples.
In the context of random media and in order to achieve the aforementioned objectives, it is of primary importance (a) to extract only the microstructural features pertinent for reconstruction of the fine-grained model (FGM) output and (b) to construct a surrogate that retains as much as possible the underlying physics of the problem. Whilst point (a) should effectively reduce the input dimension, (b) may exclude a myriad of potential model outputs violating essential physical principles of the FGM. Consequently, the core unit of the surrogate model established here is based on a coarse-grained model (CGM) operating on simplified physics valid on a larger scale than the characteristic length of the microstructural features in the FGM. The CGM serves as a stencil that automatically retains the primary physical characteristics of the FGM. It is parametrized by a latent, low-dimensional set of variables that locally encode the effective material properties of the underlying random heterogeneous microstructure and are found by appropriate sparsityinducing statistical learning Sch ̈oberl et al. (2017). Finally, a decoder distribution maps the CGM
3


outputs to the FGM solution space, yielding fully probabilistic model predictions. Instead of viewing the encoding step (dimension reduction) separately from the surrogate modeling task Ma and Zabaras (2011); Xing et al. (2016), both are integrated and trained simultaneously, ensuring that the encoded, latent representation of the microstructure is maximally predictive of the sought FGM response instead of the original fine-scale microstructure itself. Model training is carried out using modern Stochastic Variational Inference (SVI) techniques Paisley et al. (2012); Hoffman et al. (2013) in combination with efficient stochastic optimization algorithms Kingma and Ba (2014). A hierarchical Bayesian treatment of the model parameters obviates any need for fine-tuning or additional input by the analyst Bishop and Tipping (2000). The rest of the paper is organized as follows. Section 2 lays the methodological foundations for the proposed model, including a presentation of the pertinent physical structure, associated state variables and governing equations. Moreover, a detailed definition of both encoding and decoding steps is given, as well as a clear specification of the prior model and the training/prediction algorithms. In section 3, we provide numerical evidence that the proposed model can lead to accurate predictions even when trained on a few tens of FGM data. Furthermore, we present a methodological framework for adaptively adjusting the CGM complexity in order to enhance its predictive ability. Section 4 summarizes the main findings and presents further extensions and possible applications of the proposed model.
2 Methodology
We present a general, probabilistic framework to coarse-graining which we specialize in the subsequent sections. The starting point is an expensive, deterministic, fine-grained model (FGM) with high-dimensional, random inputs λf and high-dimensional outputs uf = uf (λf ). Due to the high dimensionality dim(λf ) of the input uncertainties λf and the small number of forward evaluations that can practically be performed, any naive regression approach, i.e. learning a direct map from λf to uf is doomed to fail. We seek instead a lower-dimensional, effective representation λc of the high-dimensional stochastic inputs λf which encodes as much as possible information on the respective FGM output uf , see Figure 1. We note that this differs from classical dimension reduction approaches in the sense that the encoded representation λc should be maximally predictive of the FGM output uf (λf ) – and not for the reconstruction of λf itself Tishby et al. (1999). Rather than assigning an abstract, statistical meaning to λc, we associate them with a less-expensive, physical, coarse-grained model (CGM). In particular, the latent variables λc represent the inputs of the CGM. We denote by uc = uc(λc) the corresponding CGM outputs which we subsequently attempt to link with the FGM response uf (λf ), see Figure 1. The CGM effectively represents the bottleneck through which information that the FGM input λf provides about the FGM output uf is squeezed. While several possibilities exist, independently of the final choice for the CGM, information loss will generally take place during the coarse-graining process, which in turn results in predictive uncertainty that we aim at quantifying. Moreover, tools for comparing different CGMs as well as strategies of refining the CGM adaptively in order to improve predictive accuracy are developed in a later section. The general framework described above can be effected by a model that integrates the following three key components (see Figure 1 and Grigo and Koutsourelakis (2017, 2019)):
• Encoding - Dimension reduction: A mapping from the high - dimensional microstructural description λf to a much lower-dimensional quantity λc retaining as much as possible infor
4


Encoding - Dimension reduction reduced physics
model
Decoding 
Reconstruction
Expensive FOM
Figure 1: Graphical illustration of the three-step process described in section 2. Beginning from the top left: For the given microstructure λf , a low-dimensional latent space representation λc is found via pc(λc|λf , θc). In the next step, λc serves as the input to a CGM based on simplified physics and/or coarser spatial resolution. The CGM output uc(λc) is then used to reconstruct the FGM solution using pcf (uf |uc(λc), θcf ).
mation on the corresponding response uf (λf ) – this mapping is carried out probabilistically by the conditional distribution pc(λc|λf , θc) parametrized by θc.
• Model reduction: A skeleton of a coarse-grained model (CGM) that employs the dimensionreduced quantities λc as input and yields an output uc. Such a model can be based on simplified physics and/or coarser spatial discretization. In our case, this is constructed by employing a coarse discretization of the Darcy-flow governing equations as it will be explained in detail later. We denote by pCGM(uc|λc) the generally stochastic input-output map implied.
• Decoding - Reconstruction: A probabilistic mapping from the CGM output uc back onto the original FGM output uf – this mapping is mediated by the distribution pcf (uf |uc, θcf ) parametrized by θcf .
The combination of the aforementioned three densities yields
p(uf |λf , θcf , θc) =
∫
pcf (uf |uc, θcf )
} {{ }
decoder
pCGM(uc|λc, θc)
} {{ }
CGM
pc(λc|λf , θc)
} {{ }
encoder
dλcduc. (1)
5


Figure 2: Sample of a porous medium with randomly distributed, non-overlapping polydisperse spherical solid inclusions (black). Fluid can only flow in the pore space (white). The left side shows the whole unit square domain, whilst the right shows a zoomed segment for clarity.
Assuming a deterministic CGM, i.e. pCGM(uc|λc) = δ(uc − uc(λc)), this simplifies to
p(uf |λf , θcf , θc) =
∫
pcf (uf |uc(λc), θcf )pc(λc|λf , θc)dλc. (2)
The resulting parametrized density can be viewed as the likelihood of a FGM input/output pair {λf , uf } and is thus the key quantity in the proposed probabilistic surrogate. We note that the three constitutive densities are highly modular and can be adapted to the particulars of the FGM and the CGM adopted. In the following, the FGM consists of a Stokes-flow simulator through a random heterogeneous medium, see section 2.1. The CGM is described by Darcy’s equations of flow through a permeable medium and is explained in section 2.2. We present the essential components of the proposed model as well as the specialization of the variables and densities above in section 2.3. In section 2.4, we present the learning engine that is capable of training the model with small data and in section 2.5, we discuss how the trained model can be used to produce probabilistic predictions. We conclude this methodological section with a discussion on the numerical complexity (section 2.6) as well as a novel algorithm for the adaptive refinement of the coarse-grained model (section 2.7).
2.1 The fine-grained model: Stokes flow through random porous media
Flow of incompressible Newtonian fluids through random porous media is characterized by low fluid velocities and is therefore dominated by viscous forces. In such regimes, the steady-state
6


Navier-Stokes equations reduce to Stokes equations of momentum and mass conservation,
∇xP − μ∆xV = f for x ∈ Ωf , (3a)
∇x · V = 0 for x ∈ Ωf , (3b)
where P and V are pressure and velocity fields, f is an external force field, Ωf = Ω\Ωs the pore part of the domain Ω = Ωf ∪ Ωs, where Ωs denotes the part of impermeable solid material, and μ the fluid viscosity which we may set to μ = 1 for convenience. On the solid-fluid interface Γint, the no-slip boundary condition
V = 0 for x ∈ Γint (3c)
is imposed, whereas on the boundary Γext = ∂Ω = ∂Ωf \Γint = ΓP ∪ ΓV of the macroscopic system, we apply
V = V bc for x ∈ ΓV , (3d)
t = (∇xV bc − PbcI)n for x ∈ ΓP , (3e)
where t is the Cauchy traction and n the unit outward normal. For the remainder of the paper, we consider a unit square domain Ω = [0, 1]2 ⊂ R2 with randomly distributed non-overlapping polydisperse spherical exclusions, see Figure 2 for a schematic representation. Such microstructures exhibit topologically fully connected pore spaces Ωf and lead to unique solutions of (3) for the case where Γext = ∂Ω. For the numerical solution of Equation (3), we employ triangular meshes with standard TaylorHood elements (i.e. quadratic and linear shape functions for velocity and pressure, respectively), implemented using the FEniCS finite element software package Logg et al. (2012). For generic two-phase media, λf consists of a list of binary variables representing the phase of each pixel/voxel. For the particular case considered of randomly distributed non-overlapping polydisperse spherical exclusions, one could also represent the microstructure with a list of exclusion center coordinates and the corresponding radii. We denote by uf the discretized solution vector which can consist of pressures or velocities and write uf (λf ) : λf 7→ uf (4)
to indicate the map from the microstructure λf to the PDE response of interest.
Computational considerations Typically, the structure of the porous material is very complex with a characteristic length-scale ` much smaller than the macroscopic length scale L = 1 of the problem domain Ω. This large difference in scales means that (a) from a modeling point of view, a probabilistic description would be more realistic for the porous microstructure, and (b) the numerical solution of (3) requires discretizations fine enough to resolve all microstructural details. Moreover, the uncertainty of microstructures λf ∼ p(λf ) requires repeated evaluation of the numerical solver (4) to obtain accurate estimates (e.g. with Monte Carlo) of any statistic of interest relating to the output which quickly becomes impracticable due to the significant cost associated with each FGM run.
7


2.2 The coarse-grained model: Darcy equations for diffusion through a permeable medium
The macroscopic Darcy-like behavior of Stokes flow through random porous media (3) is a longstanding result from homogenization theory first shown for periodic microstructures SanchezPalencia (1980); Tartar (1980), later generalized for connected solid phase matrix Allaire (1989), the non-stationary process Allaire (1992) and non-periodic media Whitaker (1986). In classical homogenization, the crucial step is to find a local boundary value problem on a Representative Volume Element (RVE) Sandstr ̈om et al. (2013); Marchenko and Khruslov (2006); Tartar (2010). The RVE size r0, i.e. the radius of the averaging volume, must be large compared to the microstructural length scales `f , `s. On the other hand, the volume averaged quantities, i.e. the intrinsic phase
averages P ̄ and V ̄ , should show sufficient variation over the problem domain Ω, i.e. L should be large compared to r0. To summarize, Stokes to Darcy convergence is observed in the limit of
`f  r0  L (5)
which, according to Whitaker (1986), may slightly be relaxed to the conditions
( r0
L
)2
 1, r0 & 5`f (6)
to safely assume Darcy-type flow. Using the definition for the intrinsic phase averages of pressure and velocity fields P and V
P ̄ = 1
|Ωf, RVE|
∫
Ωf,RVE
P (x)dV, V ̄ = 1
|Ωf, RVE|
∫
Ωf,RVE
V (x)dV, (7)
where |Ωf,RVE| is the volume of the fluid part of the RVE, the effective Darcy constitutive behavior can be written as
V ̄ = − K
μ (∇xP ̄) for x ∈ Ω, (8a)
∇x · V ̄ = 0 for x ∈ Ω, (8b)
V ̄ · n = V bc · n for x ∈ ΓV , (8c)
P ̄ = Pbc for x ∈ ΓP , (8d)
with the unit outward normal vector n, the viscosity μ = 1 and some unknown permeability tensor K. Determining/estimating the effective permeability field K remains a substantial computational problem which in homogenization theory is usually approached by solving an RVE subscale problem Sandstro ̈m et al. (2013). We emphasize that full Stokes/Darcy equivalence can only be assumed in the limit of infinite scale separation as defined by Equation (5). However, this does not rule out the possibility to use the Darcy equations as a stencil of a machine learning model applied to fine-grained Stokes flow data even far away from the scale separation limit, as we will see in the experiments section 3.
2.2.1 The Darcy permeability tensor field K = K(x, λc)
The material parameters of the CGM which provide the necessary closure pertain to the permeability tensor field K = K(x, λc). We subdivide the problem domain Ω into Ncells,c = dim(λc) of
8


non-overlapping subregions/ cells {Ωm}Ncells, c
m=1 such that ∪Ncells, c
m=1 Ωm = Ω and Ωm1 ∩ Ωm2 = ∅ for m1 6= m2. In most cases examined in the experimental section 3, the subregions Ωm are squares of equal size, as can be seen in the bottom row of Figure 11. Within each subregion Ωm, a constant permeability tensor K is assumed, i.e.
K(x, λc) =
Ncells, c ∑
m=1
1Ωm (x)Km(λc), (9)
where 1Ωm (x) is the indicator function of subregion Ωm. The local permeability tensors Km(λc) are positive definite matrices which, in the simplest case based on the isotropy assumption, take the form
Km(λc) = Km(λc,m) = eλc,m I, (10)
where I is the identity matrix. We note that the number of cells Ncells,c does not need to coincide with the finite elements employed for the solution of the CGM. The former control the dimension of the reduced representation whereas the latter determine the computational cost in the solution of the CGM. Naturally, different discretizations or representations of the coarse model’s spatially-varying properties can be adopted, e.g. λc could relate to basis functions’ coefficients in an appropriate expansion of K(x, λc). Analogously to the FGM in Equation (4), the Darcy-based, coarse-grained model (CGM) is defined as the deterministic mapping
uc(λc) : λc 7→ uc. (11)
Remarks
• The low-dimensional latent variables λc encode the high dimensional descriptors λf of the fine scale porous domain of the Stokes flow FGM as an effective, Darcy-type permeability random field K = K(x, λc). We re-emphasize that it is extraneous if the latent representation λc is a faithful decoding of its fine scale analogue λf for the purpose of accurately reconstructing λf directly. Instead, λc must be maximally predictive (when solving the inexpensive CGM uc(λc)) of the reconstruction of the FGM output uf . Hence, λc may be completely different from the reduced coordinates identified by typical dimensionality reduction techniques applied directly on λf . The encoder density pc(λc|λf , θc) should therefore be trained in such a way that it extracts all microstructural features from λf that are relevant in predicting the FGM output uf .
• The decoder density pcf (uf |uc(λc), θcf ) attempts to reconstruct the FGM Stokes flow response uf given the solution uc(λc) of the Darcy-based CGM Equation (8). While a general parametrization can be employed (e.g. making use of (deep) neural networks), this would necessitate either huge amounts of data or very strong regularization assumptions. We exploit instead the spatial character of the problem, i.e. if the FGM and CGM outputs uf,i, uc,j are directly related to spatial locations xi, xj (e.g. if they represent pressure or velocity responses at these points), it is natural to assume that components uc,j corresponding to points xj in the vicinity of xi are most relevant for reconstruction of uf,i corresponding to point xi.
9


• Independently of the values assigned to the parameters θc, θcf , predictions produced by the surrogate will be probabilistic as reflected by the density in Equation (2). This can be attributed to the epistemic uncertainty due to the unavoidable information loss during the coarse-graining process as there is an upper bound on the mutual information I(λc, uf ) < I(λf , uf ) when dim(λc)  dim(λf ) and no redundancies in λf Tishby et al. (1999).
The following subsections are devoted to giving a clear description of the main building blocks pc and pcf as well as of the prior model we apply on the model parameters θc and θcf .
2.3 Components of the proposed model
2.3.1 The encoder distribution pc
The encoder distribution pc that effects the mapping from the high-dimensional λf to the lowerdimensional λc can assume various forms. Any successful strategy should be capable of extracting appropriate features from λf or, more generally, learning the right representation for predicting λc. An obvious choice would be to exploit the expressive power of (deep) Neural Nets (NNs) and, given the spatial character of λf , Convolutional Neural Networks (CNNs) in particular Fukushima (1980); Lecun et al. (1998); Krizhevsky et al. (2012). A common misconception is that Deep Learning techniques are devoid of any feature engineering. While deep NNs are highly expressive, contain a lot of learnable parameters and reduce the need for feature engineering, it is not true that data pre-processing or feature extraction are totally irrelevant2. Several components, such as the number, size and type of layers or e.g. the type and number of pooling layers in CNNs are in fact hard-coded, feature-extraction mechanisms. More importantly, the basic premise in successful applications of these models is the availability of large amounts of data, from which the necessary information can be extracted. As mentioned earlier however, we operate in the Small Data regime. Based on this, we make use of a large set of physically-motivated, feature functions φjm(λf ) (discussed in detail in the sequel) in the context of the following model:
λc,m =
Nfeatures,m ∑
j=1
θ ̃c,jmφjm(λf ) + σc,mZm, Zm ∼ N (0, 1), (12)
In principle, the type and number of features can differ with every latent space component m.
These feature functions are linearly combined with coefficients θ ̃c,m =
{θ ̃c,jm
}Nfeatures,m
j=1
(potentially
different for every component m as well) and augmented with residual Gaussian white noise of
variance σc2,m, so that the set of free parameters for the distribution pc is θc =
{θ ̃c,m, σc2,m
}dim(λc )
m=1
3.
The distribution for pc is thus
pc(λc|λf , θc) =
dim(λc )
∏
m=1
N (λc,m|θ ̃T
c,mφm(λf ), σ2
c,m), (13)
2https://github.com/tensorflow/transform 3We use the notation Σc = diag[σc2], where σc2 = {σc2,m
}dim λc
m=1 is the vector of variances, wherever it is more
convenient.
10


Figure 3: Graphical illustration of certain classes of feature functions. Left: Depiction of the chord length density (blue line with red dots), pore size density (red/gray circles), mutual distances (orange lines) and lineal path function (green lines). Right: Euclidean distance transform. Explanations see text.
with φm(λf ) = {φjm(λf )}Nfeatures,m
j=1 being the vector of feature functions used to predict the latent space component λc,m.
Feature functions: The most critical aspect in the expressivity of the encoder pertains to the feature functions φm(λf ) employed. In this work, we make use of morphological measures Torquato (2001); Lu and Torquato (1992); Torquato and Lu (1993); Lowell et al. (2006), quantities from classical fluid dynamics Sutera and Skalak (1993); Kozeny (1927); Carman (1937) or other physics Archie (1947), image analysis Soille (1999) and low-dimensional autoencoder representations Doersch (2016). Some important classes of possible feature functions are depicted in Figure 3 and briefly explained in the following list:
• Pore fraction: The fraction of pore space |Ωf |/|Ω| over a certain region and several nonlinear functions thereof (e.g. exp, log, certain powers);
• Interface area: The interface area between matrix- and exclusion phase over a certain region and nonlinear functions thereof;
• Effective medium approximations: There exist several approximation formula for effective material properties in diffusion problems (Bruggeman formula Bruggeman (1935), MaxwellGarnett approximation Landauer (1978)) which can be generalized to the infinite contrast limit of matrix/exclusion phase permeabilities;
• Chord length density: The density of lengths of segments of an infinitely long line being in either matrix or exclusion phase, see blue line with red dots in Figure 3. A ‘segment’ is a part of the line between two neighboring dots – the feature function is given by evaluating the chord length density for several lengths d, see 3 for the length scales used in the experiments;
11


• Statistics of exclusion radii : Mean, variance, max, min, etc. of exclusion radii;
• Statistics of mutual exclusion distances: Mean, variance, max, min, etc. of mutual exclusion distances (center-to-center or edge-to-edge). See the orange lines in Figure 3 for illustration;
• Pore-size density: Density of the distance from a random point in Ω to the nearest matrix/exclusion interface. For illustration see the red/gray circles in Figure 3. Nearest-neighbor functions Torquato (2001) are closely related. Again, the pore-size density is to be evaluated for several lengths d, see 3;
• Lineal-path function: The probability of a random line of length d being purely in matrix (or exclusion) phase. See the green lines in Figure 3 for illustration and 3 for lengths d used;
• 2-point correlations: The probability, that two random points of distance d lie both in matrix (exclusion) phase – see 3 for lengths d;
• Distance transforms: For any point in Ω, compute the distance to the closest exclusion (matrix phase), see right part of Figure 3. Features can be constructed from statistics of the distance transform.
Ultimately, several hundreds of such functions can be deployed – in Section 3, we use the 150 listed in Table 3. In order to avoid overfitting in the Small Data regime we are operating (i.e. N . 100), we make use of a fully Bayesian, sparsity-inducing prior model Bishop and Tipping (2000) capable of automatically controlling model complexity by pruning all irrelevant features and identifying the most salient ones, see section 2.3.3.
2.3.2 The decoder distribution pcf
The probabilistic coarse-to-fine mapping pcf (uf |uc, θcf ) should take into account the spatial characteristics of the problem, e.g. that FGM and CGM outputs uf,i, uc,j may be given by physical quantities evaluated at spatial locations xi, xj. For simplicity, let us assume that uf,i = P (xi) is the pressure field solution of the FGM Equation (3) interpolated at a regular fine scale grid G(f), xi ∈ G(f), and uc,j = P ̄(xj) its Darcy counterpart from Equation (8) evaluated at a much coarser grid G(c), xj ∈ G(c). Then the model for pcf could look like
uf,i =
∑
j∈G(c)
Wij uc,j (λc) + τ −1/2
cf,i Zi, Zi ∼ N (0, 1), (14)
where W should have the structure of an interpolation matrix from G(c) to G(f) and diag[τ cf ] is a diagonal precision matrix to be learned from the data. The decoder distribution pcf can thus be written as pcf (uf |uc(λc), θcf ) = N (uf |W uc(λc), diag[τ −1
cf ]), (15)
with model parameters θcf = {W , τ cf }. We emphasize that many different possibilities exist in terms of the parametrization and architecture of pcf . One should note here that, in contrast to pc which involves a map from the high-dimensional λf to a lower-dimensional λc, pcf encompasses a map from the lower-dimensional uc to the high-dimensional uf which is obviously much more manageable. As with pc, the option of NNs comes to mind and given the spatial character of both uc and uf CNNs could be introduced. We have again decided against this option, due to the Small Data setting of the problem. Given the lower dimension of the inputs uc, nonlinear models employing polynomials, kernels etc could be employed. An interesting alternative involves Bayesian
12


non-parametric extensions. For instance, a Gaussian Process regression could be deployed for the probabilistic mapping uc,j = P ̄(xj) 7→ uf,i = P (xi), exploiting the spatial structure of the problem
by making use of a stationary covariance kernel of the form cov(uf,i, uc,j) = cov(P (xi), P ̄(xj)) = cov(xi, xj) = cov(|xi − xj|). Alternatively, each uf,i can be expressed using an independent Gaussian Process in a fashion analogous to GP-LVMs Lawrence (2005); Titsias and Lawrence (2010). Such an extension is not pursued here due to the complexities associated with the inference and prediction steps Damianou et al. (2015).
2.3.3 The prior model
We advocate a fully Bayesian formulation where the (approximate) posterior densities on the model parameters θc, θcf may be computed. In view of the Small Data setting, we employ priors that can safeguard against overfitting, that do not require any fine-tuning or have ad hoc hyperparameters, but also promote interpretability of the results. To that end, in order to induce sparsity in the feature functions of pc given in Equation (13), we make use of the Automatic Relevance Determination (ARD) model Faul (2003) corresponding to
p(θ ̃c|γ) =
dim(λc )
∏
m=1
Nfeatures,m ∏
j=1
N (θ ̃c,jm|0, γ−1
jm), (16)
where the precision hyperparameters γjm are equipped with a conjugate Gamma hyperprior
p(γjm) = Gamma(γjm|a, b) = baγa−1
jm e−bγjm /Γ (a). (17)
This hierarchical model has been widely studied and the capability of pruning irrelevant parameters θc,jm has been shown when small values for the hyperparameters are employed4 Bishop and Tipping (2000); Tipping (2001). Similarly, we apply conjugate, uninformative Gamma hyperpriors5 on the precision parameters τ c, τ cf in the encoder/decoder distributions pc and pcf , i.e.
p(τc,m) = Gamma(τc,m|c, d), (18)
p(τcf,i) = Gamma(τcf,i|e, f ). (19)
A vague, uninformative prior could also be used for W but as this was found to have no measurable effect on the results and in order to lighten the notation, we completely omit it.
2.4 Model training
The probabilistic graphical model described thus far is depicted in Figure 4. Given a dataset
consisting of FGM input/output pairs (i.e. Stokes-flow model simulations) D =
{
λ(n)
f , u(n)
f
}N
n=1
,
4In our investigations, we set a = b = c = d = e = f = 10−10 to avoid division by zero without affecting the converged values. 5See footnote 4
13


Figure 4: Graphical representation of the Bayesian network defined by Equation (22). All internal vertices (with teal background) are latent variables.
the model likelihood can be written as (see Equation (2))
L(θcf , θc) =
N
∏
n=1
p(u(n)
f |λ(n)
f , θcf , θc)
=
N
∏
n=1
∫
N (u(n)
f |W uc(λ(n)
c ), diag[τ cf ]−1)·
dim(λc )
∏
m=1
[
N (λ(n)
c,m|θ ̃T
c,mφm(λ(n)
f ), τ −1
c,m)
]
dλ(n)
c,
(20)
where diag[τ c] = diag[σc−2] is a diagonal precision matrix and λ(n)
c are the latent variables that encode the coarse-grained material properties for each sample n. Given the aforementioned priors, the posterior of the model parameters is
p(θ ̃c, τ c, τ cf |D) ∝
N
∏
n=1
[∫
N (u(n)
f |W uc(λ(n)
c ), diag[τ cf ]−1)
dim(λc )
∏
m=1
[
N (λ(n)
c,m|θ ̃T
c,mφm(λ(n)
f ), τ −1
c,m)
]
dλ(n)
c
]
·
∫
p(θ ̃c|γ)p(γ)dγ · p(τ c)p(τ cf )
(21)
where θ ̃c =
{θ ̃c,m
}dim(λc )
m=1
. The posterior over all variables θ =
{ {
λ(n)
c
}N
n=1
, θ ̃c, τ c, τ cf , γ
}
is
given by
p(θ|D) = p(θ, D)
p(D) = 1
p(D)
N
∏
n=1
[
N (u(n)
f |W uc(λ(n)
c ), diag[τ cf ]−1)·
·
dim(λc )
∏
m=1
[
N (λ(n)
c,m|θ ̃T
c,mφm(λ(n)
f ), τ −1
c,m)
]
]
p(θ ̃c|γ)p(γ)p(τ c)p(τ cf )
(22)
14


where p(D) is the model evidence of the postulated coarse-graining model Gelman et al. (2003). This is a pivotal quantity in model validation that as we show later will serve as the main driver for the refinement/enhancement of the CGM. Evaluating the integrals in Equation (21) is analytically intractable, making necessary the use of suitable (approximate) inference methods that are capable of giving fast and accurate estimates of the above posterior. In this work, we employ Stochastic Variational Inference (SVI, Paisley et al. (2012); Hoffman et al. (2013)) methods which produce closed-form approximations of the true posterior p(θ|D) and simultaneously of the model evidence p(D). In contrast to sampling-based procedures (e.g. MCMC, SMC), SVI yields biased estimates at the benefit of computational efficiency and computable convergence objectives in the form of the Kullback-Leibler (KL) divergence between the approximation Q(θ) and the true posterior p(θ|D) Blei et al. (2017):
KL(Q(θ)||p(θ|D)) = −
∫
Q(θ) log p(θ|D)
Q(θ) dθ. (23)
We note that the log evidence log p(D) can be decomposed as
log p(D) = log
∫
p(θ, D)dθ = F(Q) + KL(Q(θ)||p(θ|D)), (24)
where
F(Q) =
∫
Q(θ) log p(θ, D)
Q(θ) dθ. (25)
Since KL(Q(θ)||p(θ|D)) ≥ 0, F(Q) provides a rigorous lower bound to log p(D), called the evidence lower bound (ELBO). Hence minimizing KL(Q(θ)||p(θ|D)) ≥ 0 is equivalent to maximizing the ELBO F(Q). We employ a mean-field approximation to the full posterior based on the following decomposition:
Q(
{
λ(n)
c
}N
n=1
, θ ̃c, τ c, τ cf , γ) =
=
dim(λc )
∏
m=1

Qτc,m (τc,m)
Nfeatures,m ∏
j=1
[
Qθ ̃c,jm (θ ̃c,jm)Qγjm (γjm)
]


dim(uf )
∏
i=1
[Qτcf,i (τcf,i)]
N
∏
n=1
Qλ(n)
c (λ(n)
c)
≈ p(
{
λ(n)
c
}N
n=1
, θ ̃c, τ c, τ cf , γ|D).
(26)
If θk denotes an arbitrary subset of the parameters θ above (i.e. if θk is effectively one of the component variables θ ̃c,jm, τc,m, γjm, τcf,i, λ(n)
c ), then the optimal Qθk (θk) can be found by setting the first order variation of F(Q) to zero yielding Beal and Ghahramani (2003)
Qθk (θk) = exp 〈log p(θ, D)〉`6=k
∫ exp 〈log p(θ, D)〉`6=k dθk
, (27)
where 〈 · 〉`6=k denotes expectation w.r.t. all Qθ` ’s except for Qθk . We emphasize that every Qθk implicitly depends on all other Qθ` ’s, which means we need to self-consistently cycle over all k and
15


update Qθk given expected values w.r.t. all Qθ` , ` 6= k, until convergence is attained. We provide complete details of the derivations in A and summarize here the closed-form update equations
Qγjm (γjm) = Gamma(γjm| ̃a,  ̃bjm),
a ̃ = a + 1
2 ,  ̃bjm = b + 1
2
〈θ ̃2
c,jm
〉
,
(28)
Qτc,m (τc,m) = Gamma(τc,m|c ̃, d ̃m),
c ̃ = c + N
2 , d ̃m = d + 1
2
N
∑
n=1
〈 (
λ(n)
c,m − θ ̃T
c,mφm(λ(n)
f)
)2〉
, (29)
Qτcf,i (τcf,i) = Gamma(τcf,i|e ̃, f ̃i),
e ̃ = e + N
2 , f ̃i = f + 1
2
N
∑
n=1
〈 [
u(n)
f − W uc(λ(n)
c)
]2
i
〉
, (30)
Qθ ̃c,jm (θ ̃c,jm) = N (θ ̃c,jm|μθ ̃c,jm , σ2
θ ̃c,jm ),
σ2
θ ̃c,jm =
(
〈τc,m〉
N
∑
n=1
(φ(n)
jm )2 + 〈γjm〉
)−1
,
μθ ̃c,jm = σ2
θ ̃c,jm 〈τc,m〉
∑
n
φ(n)
jm


〈
λ(n)
c,m
〉
−
∑
k6=j
φ(n)
km
〈θ ̃c,km
〉

,
(31)
where 〈 · 〉 denotes expected values w.r.t. Q(
{
λ(n)
c
}N
n=1
, θ ̃c, τ c, τ cf , γ) as specified in (26). Due
to the choice of conjugate priors, most expected values are given in closed form,
〈γjm〉 =  ̃a
 ̃bjm
, 〈τc,m〉 = c ̃
d ̃m
, 〈τcf,i〉 = e ̃
f ̃i
,
〈θ ̃c,jm
〉
= μθ ̃c,jm ,
〈θ ̃2
c,jm
〉
= μ2
θ ̃c,jm + σ2
θ ̃c,jm .
(32)
The optimal form of the remaining approximate posteriors, i.e. Qλ(n)
c , is given by (based on Equation (27))
Qλ(n)
c
(λ(n)
c )∝
〈
N (u(n)
f |W uc(λ(n)
c ), diag[τ cf ]−1)
dim(λc )
∏
m=1
[
N (λ(n)
c,m|θ ̃T
c,m φm (λ(n)
f ), τ −1
c,m )
]
〉 (33)
which is analytically intractable due to the expectations with respect to the output of the CGM uc(λ(n)
c ) (i.e. of the Darcy-flow simulator). Instead of approximating the expectations above by Monte Carlo, we retain the variational character of the algorithm by employing approximations to the optimal Qλ(n)
c which take the form of multivariate Gaussians with a diagonal covariance, i.e.
Q ̃ λ(n)
c (λ(n)
c |μ(n)
λc , σ(n)
λc ) ≈ N (λ(n)
c |μ(n)
λc , diag[(σ(n)
λc )2]), (34)
16


Algorithm 1: Model training.
Data: D =
{
λ(n)
f , u(n)
f
}N
n=1
; // Training data
Input: b ̃jm ←  ̃b(0)
jm, d ̃m ← d ̃(0)
m , f ̃i ← f ̃(0)
i , μθ ̃c,jm ← μ(0)
θ ̃c,jm
, σ2
θ ̃c,jm
← (σ(0)
θ ̃c,jm
)2 ;
// Initialization
Output: Variational approximation Q(θ) to p(θ|D) minimizing KL-divergence eq. (23)
1 Evaluate all features {φm(λ(n)
f )}nN=1;
2  ̃a ← a + 1
2 , c ̃ = c + N
2 , e ̃ = e + N
2; 3 while (not converged) do 4 for n ← 0 to N do
// Fully parallelizable in n
5 Update Q ̃λ(n)
c (λ(n)
c |μ(n)
λc , σ(n)
λc ) according to eq. (33)–(68)
6 Estimate
〈
λ(n)
c,m
〉
Q ̃ λ(cn)
,
〈
(λ(n)
c,m)2
〉
Q ̃ λ(cn)
,
〈
uc,k (λ(n)
c)
〉
Q ̃ λ(cn)
,
〈
u2
c,k (λ(n)
c)
〉
Q ̃ λ(cn)
7 end
8 Update Qθ ̃c,m (θ ̃c,m|μθ ̃c,m , Σθ ̃c,m ) according to (31) and (32);
9 Update Qγjm (γjm|a ̃,  ̃bjm) according to (28) and (32);
10 Update Qτc,m (τc,m|c ̃, d ̃m) according to (29) and (32);
11 Update Qτcf,i (τcf,i|e ̃, f ̃i) according to (30); 12 end
13 return Variational approximation Q(θ) to p(θ|D)
and find the optimal values of the parameters μ(n)
λc , σ(n)
λc via black-box variational inference Ranganath et al. (2014); Hoffman et al. (2013); Paisley et al. (2012), i.e. by minimizing the KLdivergence between Q ̃λ(n)
c and the optimal Qλ(n)
c in Equation (33),
min
μ(n)
λc ,σ(n)
λc
KL
( Q ̃λ(n)
c (λ(n)
c |μ(n)
λc , σ(n)
λc )
∣ ∣ ∣
∣
∣
∣Qλ(n)
c (λ(n)
c)
)
. (35)
The computation of the objective above as well as gradients with respect to μ(n)
λc , (σ(n)
λc )2 involves
expectations of the CGM outputs as well as their respective gradients ∂uc(λ(n)
c )/∂λ(n)
c . The latter are efficiently computed using adjoint formulations (see e.g. Heinkenschloss (2008); Constantine et al. (2014)). In order to minimize the noise in the expectations involved, we apply the the reparametrization trick Kingma and Welling (2013) and supply the stochastic gradients to the adaptive moment estimation optimizer (ADAM Kingma and Ba (2014)). We note that this stochastic optimization problem needs to be run for every data point n in every training iteration, but is
entirely parallelizable in n. After optimization, the expected values
〈
λ(n)
c,m
〉
,
〈
(λ(n)
c,m)2
〉
are readily
given by the first and second moments of the resulting Gaussian specified in Equation (34), whereas
the expected values
〈
uc,k (λ(n)
c)
〉
,
〈
u2
c,k (λ(n)
c)
〉
can efficiently be obtained by direct Monte Carlo.
Full details about the performed black-box VI are given in A.1. All steps of the training process
17


are summarized in Algorithm 1.
2.5 Model predictions
A main feature of the coarse-grained model presented in this work is the capability to produce probabilistic predictions that quantify uncertainty both due to limited training data and due to limited model complexity, i.e. the information loss happening during the coarse-graining process going λf → λc → uc → uf . In particular, given training data D and a new FGM input λf (which is not included in the training data D), the predictive posterior density ppred(uf |λf , D) for the corresponding FGM output uf is
ppred(uf |λf , D) =
∫
p(uf |λf , θ)
} {{ }
Equation (2)
p(θ|D)dθ
=
∫
pcf (uf |uc(λc), θcf )pc(λc|λf , θc)dλcp(θcf , θc|D)dθc dθcf ,
(36)
with the decoding/encoding distributions pc, pcf as described in sections 2.3.1, 2.3.2. We make use of the variational approximation (section 2.4) to the posterior
p(θ|D) = p(θ ̃c, τ c, τ cf |D) ≈ Qθ ̃c (θ ̃c)Qτ c (τ c)Qτ cf (τ cf ), (37)
in order to rewrite the predictive posterior as
ppred(uf |λf , D) =
∫
pcf (uf |uc(λc), τ cf )pc(λc|λf , θ ̃c, τ c)dλc
· Qθ ̃c (θ ̃c)dθ ̃c · Qτ c (τ c)dτ c · Qτ cf (τ cf )dτ cf
=
∫
pcf (uf |uc(λc), τ cf )
[∫
pc(λc|λf , θ ̃c, τ c)Qθ ̃c (θ ̃c)dθ ̃c
]
dλc
· Qτ c (τ c)dτ c · Qτ cf (τ cf )dτ cf .
(38)
By drawing samples from the approximate posterior, the integration above can be performed efficiently with Monte Carlo since the cost of each sample is very small as it entails solely CGM output evaluations. We note though that some of the integrations involved in the computation of lower-order statistics can also be done (semi)-analytically. In particular, we note that the predictive posterior mean μpred(λf ) is given by:
μpred(λf ) =
∫
uf ppred(uf |λf , D) duf = W 〈uc(λc)〉
≈1
Nsamples
W
Nsamples ∑
s=1
uc(λ(s)
c)
(39)
where 〈uc(λc)〉 denotes the predictive posterior average of the CGM output computed with Monte Carlo by ancestral sampling. Similarly, one can compute the posterior predictive covariance of the
18


Training/offline stage
Generate training data D λ(n)
f ∼ p(λf )
u(n)
f = uf (λ(n)
f)
D=
{
λ(n)
f , u(n)
f
}N
n=1
Evaluate features
φj m (λ(n)
f ), sec. 2.3.1
Train surrogate, sec. 2.4
Output: approximate posterior Q(θ) ≈ p(θ|D)
Prediction/online stage
Average over parameters, ∫ p(uf |λf , θ)p(θ|D)dθ, sec. 2.5
Evaluate features φjm(λf )
Unseen λf
Sample λ(s)
c ∼ pc(λc|λf , θc)
Solve CGM u(s)
c = uc(λ(s)
c)
Project back u(s)
f = W u(s)
c
Repeat and update QoI f (e.g., μpred),
〈f 〉 ≈ 1
Nsamples
∑Nsamples
s=1 f (u(s)
f)
Figure 5: Model workflow training (left) and prediction phases (right).
FGM output vector uf , which we denote by Covpred(λf ), as
Covpred(λf ) =
= diag
[〈
τ −1
cf
〉]
+W
〈
(uc(λf ) − 〈uc(λf )〉) (uc(λf ) − 〈uc(λf )〉)T 〉
WT
≈1
Nsamples
W


Nsamples ∑
s=1
(
uc(λ(s)
c ) − 〈uc〉
)(
uc(λ(s)
c ) − 〈uc〉
)T

WT
+ diag
[ f ̃
e ̃ − 1
]
,
(40)
where
〈
(uc(λf ) − 〈uc(λf )〉) (uc(λf ) − 〈uc(λf )〉)T 〉
denotes the predictive posterior covariance of
the CGM output vector uc which can be estimated by Monte Carlo and
〈
τ −1
cf,i
〉
correspond to the
posterior averages of reciprocal precisions which can be computed in closed form based on Equation (30). The computational steps to generate predictive samples from the generally non-Gaussian ppred(uf |λf , D) for an unseen microstructural input λf is summarized in Algorithm 2. The training (offline) and prediction (online) stages are represented graphically in the workflow diagram in Figure 5.
2.5.1 Model performance metrics
Apart from computational efficiency, the key objective of any surrogate model is its predictive accuracy, i.e. its closeness to the true FGM solution uf (λf ). In order to assess this, we consider
separate test datasets Dtest =
{
λ(n)
f , u(n)
f
}Ntest
n=1
, where the λ(n)
f ∼ p(λf ) are drawn from the same
distribution p(λf ) that was used for the training set D. We report metrics that reflect both the
19


Algorithm 2: Generation of predictive samples.
Data: λf,new ; // Test microstructure λf,new
Input: p(θ ̃c, τ c, τ cf |D) ≈ Qθ ̃c (θ ̃c)Qτ c (τ c)Qτ cf (τ cf ) ; // (Approximate) posterior
Output: Predictive sample u(s)
f ∼ ppred(uf |λf,new, D), eq. (38);
1 Evaluate all feature functions φm(λf,new);
2 Sample τ (s)
cf ∼ Qτ cf (τ cf ), τ (s)
c ∼ Qτ c , θ ̃(s)
c ∼ Qθ ̃c (θ ̃c);
3 Sample λ(s)
c ∼ pc(λc|φm(λf,new), θ ̃(s)
c , τ (s)
c );
4 Solve CGM u(s)
c = uc(λ(s)
c );
5 Draw predictive sample u(s)
f ∼ pcf (uf |W , τ (s)
cf );
6 return Predictive sample u(s)
f
accuracy of point estimates of the predictive posterior (e.g. μpred(λf ) in Equation (39)) as well as the whole density ppred(uf |λf , D) (Equation (38)). In particular, we compute the coefficient of determination R2 Zhang (2017)
R2 = 1 −
∑Ntest
n=1 ‖u(n)
f − μpred(λ(n)
f )‖2
∑Ntest
n=1 ‖u(n)
f −  ̄uf ‖2 , (41)
where u ̄f = 1
Ntest
∑Ntest
n=1 u(n)
f is the sample mean of the FGM output uf over the test dataset. The denominator normalizes the point deviation in the numerator by the actual variability of the true outputs. In the case of perfect prediction, μpred(λ(n)
f ) = u(n)
f ∀n, the second term in (41) vanishes and R2 = 1 which is the largest value it can attain. Smaller values of R2 indicate either larger deviation of the mean prediction from the truth or smaller variability in the uf components. The second metric we employ is the mean log likelihood M LL Zhu and Zabaras (2018) given by
M LL = 1
Ntestdim(uf )
Ntest ∑
n=1
log ppred(uf (λ(n)
f )|λ(n)
f , D). (42)
We note that when log ppred(uf |λf , D) degenerates to a discrete density centered at the true uf (λf ), ∀λf i.e. not only it predicts perfectly the FGM output but there is no predictive uncertainty, then the M LL attains the highest possible value of 0. In the other extreme if for one or more of the test samples λ(n)
f , ppred → 0 at the true uf (λf ), then M LL attains its lowest possible value of −∞. Hence, M LL attempts to measure the quality of the whole predictive posterior ppred. Given that the integration in Equation (38) is analytically intractable and no closed-form expression for ppred can be established, we evaluate M LL under the assumption that ppred can be adequately approximated by a multivariate Gaussian with mean μpred (Equation (39)) and a diagonal covariance which consists of the diagonal entries of Covpred(λf ) (Equation (40)) which we
20


Quantity
Phase N dim(uf ) dim(λf ) dim(uc) dim(λc) dim(θ ̃c,m)
training N dim(uf ) 1 . . . (dim(λf ))2
dim(uc) dim(λc) dim(θ ̃c,m)
prediction 1 dim(uf ) 1 . . . (dim(λf ))2
dim(uc) dim(λc) dim(θ ̃c,m)
Table 1: Computational complexity of training and prediction stages. The scaling with dim(λf ) is dependent on the applied set of feature functions φm(λf ) and reaches from O(1) (e.g. for
φm(λf ) = 1) to O ((dim(λf ))2) (e.g. φm(λf ) = mean mutual distance of exclusion centers).
represent with the vector σ2
pred(λf ). In this case:
M LL = 1
Ntestdim(uf )
Ntest ∑
n=1
log ppred(uf (λ(n)
f )|λ(n)
f , D)
≈ −1
2 log 2π − 1
Ntestdim(uf )
Ntest ∑
n=1
(
1 2
dim(uf )
∑
i=1
log σ2
pred, i(λ(n)
f)
−1
2
dim(uf )
∑
i=1
(
uf,i(λ(n)
f ) − μpred,i(λ(n)
f)
)2
σ2
pred, i(λ(n)
f)
)
(43)
In the numerical experiments discussed in section 3, Ntest = 1024 is used, leading to negligible Monte Carlo error due to variation of test samples.
2.6 Numerical complexity analysis
For analysis of the numerical complexity of the proposed model, it is vital to distinguish between training (offline) and prediction (online) phases, see also Figure 5. There are basically six quantities that are relevant for the computational cost of both stages. These are the number of training data N , the dimension of the FGM input dim(λf ), the dimension of the FGM output dim(uf ), the dimension of CGM input dim(λc), the dimension of the CGM output dim(uc) and the number of
feature functions per latent space variable dim(θ ̃c). As it can be seen in Algorithm 1, the complexity of the training stage depends linearly on the number of training data N (for-loop), but is fully parallelizable over every data point. The prediction stage only sees the final (approximate) posterior p(θ ̃c, τ c, τ cf |D) ≈ Qθ ̃c (θ ̃c)Qτ c (τ c)Qτ cf (τ cf ), so that the scaling is independent of N . Both training and prediction phases scale linearly with the fine scale output dimension dim(uf ), since they only involve vector operations. For the same reason, the scaling with respect to dim(λc) is linear. Scaling with dim(uc) depends on the CGM solver i.e. in the worst case, for the linear Darcy model with a direct solver, the scaling is O (dim((uc))3). We apply a sparse banded solver, which scales as O (dim((uc))). Scaling with the dimension of microstructural inputs dim(λf ) is dependent on the set of applied feature functions φm and ranges
from O(1) (e.g. constant feature, i.e. φ(λf ) = 1) to O ((dim(λf ))2) (e.g. mean mutual distance
of circular exclusions). Finally, scaling with respect to dim(θ ̃c,m) is O
(
(dim(θ ̃c,m))
)
see Equation
21


(31). Furthermore, in the prediction stage, it is possible to omit components of θ ̃c,m that have been pruned out by the applied sparsity prior, thereby avoiding the cost connected to the evaluation of corresponding feature functions. The scaling characteristics of training and prediction phases is summarized in table 1.
2.7 Automatic adaptive refinement
As already mentioned in section 2.4, the evidence lower bound (ELBO) is an approximation to the model’s log −evidence. The latter balances the model’s fit to the data with the model’s complexity and serves as a parsimonious metric of how well the model can explain the training data Murray and Ghahramani (2005); Rasmussen and Ghahramani (2001). This quantity can therefore be used not only for scoring different competing models which have been trained on the same data, but also as the objective to any refinement of an existing model. According to Equation (25), the ELBO is given by the expected value
F (Q) = 〈log p(θ, D) − log Q(θ)〉Q(θ) , (44)
where p(θ, D) is given in Equation (22) and Q(θ) is the variational approximation to the posterior p(θ|D) as defined in (26). Since all of the expected values needed in the right side of the above equation are estimated during model training, the ELBO F(Q) can be evaluated explicitly and be used to also monitor convergence behavior during training. In order to keep this section uncluttered, the final expression for the ELBO is given in D. While one can envision many ways of refining the coarse-grained model, we focus here on the latent variables λc. These constitute the bottleneck through which information that the FGM input λf provides about the FGM output uf is squeezed. Physically, they relate to the effective permeability of the Darcy-based CGM and assuming a model according to equations (9)–(10), they are inherently tied with the piecewise-constant discretization adopted, i.e. each component λc,m relates to the Darcy permeability in a square subregion Ωm of the problem domain. The question we would like to address is the following: If one was to refine the piecewise-constant representation of the Darcy permeability field in equations (9)–(10) by subdividing one square cell Ωm of the existing partition into smaller subregions, then which Ωm would yield the best possible model? We note that such a refinement would lead to an increase in the dimension dim(λc) of the bottleneck variables which, in principle, should lead to more information form λf being retained. Given a fixed refinement mechanism of an Ωm, say by subdividing into 4 equal squares (in twodimensions), a brute force solution strategy would be to refine one of the existing Ωm at a time, train the new model, compute its ELBO and compare with the others. Obviously such a strategy is computationally cumbersome particularly when the number of existing subregions Ncells,c is high. The cost would be even greater if one was to consider all possible combinations of two or more subregions that could be refined at the same time. We thus propose a procedure based on the decomposition of the ELBO F(Q) into contributions from different subregions Ωm (and therefore λc,m) according to F (Q) = ∑Ncells,c
m=1 Fm(Q) + H(Q), where with H(Q) we denote terms that cannot be assigned to any of the subregions Ωm. As the goal is ultimately to increase the ELBO of the model, we propose selecting for refinement the cell m with the lowest contribution Fm(Q) and therefore contributes the least in explaining the data. This strategy is analogous to the one advocated in Ghahramani and Beal (2000) for refining components in a mixture of factor analyzers. Hence we use Fm(Q) as the scoring function of each subregion/cell.
22


The derivation of Fm(Q) is contained in D. and we include here only the final expression which, for a model with tied γjm = γj, is
Fm(Q) =
N
∑
n=1
log σ(n)
λc,m − c ̃log d ̃m +
dim(γ )
∑
j=1
log σθ ̃c,jm . (45)
where (σ(n)
λc,m )2 is the posterior variance of λ(n)
c,m (see equations (34) and (35)), c ̃, d ̃m are given in
Equation (29) and σ2
θ ̃c,jm
is the posterior variance of the parameters θc,jm given in Equation (31).
Keeping in mind that the subregion/cell Ωm with the smallest Fm is proposed for refinement,
the scoring function defined by Equation (45) favours cells where the posterior variances (σ(n)
λc,m )2
of the λ(n)
c,m tend to be small, i.e. tight (approximate) posterior distribution Qλ(n)
c,m (λ(n)
c,m), whereas
at the same time, the expected decoder misfit ∑N
n=1
〈 (
λ(n)
c,m − θ ̃T
c,mφm(λ(n)
f)
)2〉
(see Equation
(29)) should be large but there should be high certainty about the feature coefficients θ ̃c,m, i.e. low σ2
θ ̃c,jm
. We finally note that once the cell m with the lowest Fm(Q) has been identified and split,
training is restarted with all model parameters initialized to their previously converged values. We illustrate these steps by example in section 3.6.
3 Numerical examples
The numerical experiments carried out in this paper are aimed at replacing the expensive Stokes flow FE simulation defined by Equations (3)–(4) by an inexpensive, Bayesian, coarse-grained model built around the Darcy-flow skeleton as presented in sections 2.2–2.6. We begin this section with a clear specification of the FGM data generation and the model distributions pc, pcf . Subsequently, we show that close to the homogenization limit (i.e. infinite scale separation) defined by Equation (5), the proposed coarse-grained model is capable to accurately reconstruct the FGM solution by learning (from a small number of FGM training samples) the requisite effective properties without ever solving any of the equations homogenization theory describes. Thereafter, we investigate the coarse-grained model’s predictive quality as a function of the number of training data N and the latent space dimensionality dim(λc) for microstructural data far from the homogenization limit (without scale separation). We show that the model indeed exhibits feature sparsity in learning the effective Darcy permeability field K(x, λc). We then apply the surrogate to an uncertainty propagation (UP) problem and finally demonstrate the performance of the proposed adaptive refinement objective and associated algorithm. Source code as well as examples can be found at: https://github.com/congriUQ/physics aware surrogate
3.1 Experimental setup
3.1.1 Fine scale microstructural data
Throughout the following experiments, we use randomly distributed, non - overlapping polydisperse spherical exclusions from a unit square domain Ω = [0, 1]2 as depicted in Figure 2. The total number
of exclusions N (n)
ex of a material sample n as well as the radii r(n)
ex,i, i = 1, . . . , N (n)
ex , are sampled
23


Figure 6: Microstructures generated as described in section 3.1.1. The corresponding parameters are μex = 7.8, σex = 0.2, σr = 0.3, ls = 1.2, lx = 0.08 and lr = 0.05.
from a log-normal distribution,
N (n)
ex ∼ round [Lognormal (μex, σ2
ex
)] , (46)
r(n)
ex,i ∼ Lognormal
(
μ(n)
r (x), σ2
r
)
. (47)
The coordinates of the center of each exclusion x(n)
ex,i are drawn from a Gaussian process with squared exponential kernel of length scale lx warped with the logistic sigmoid function S(z) = 1/(1 + e−lsz),
x(n)
ex,i ∼ 1
Nρ(n)
ρ(n)
xex (x), ρ(n)
xex (x) ∼ S (GP (0, kx(x − x′))) , (48)
where Nρ(n) = ∫ ρ(n)
xex (x)x and a squared exponential covariance kx =
exp
{
− |x − x′|2 /l2x
}
6. Also, the (location-dependent) log-normal mean of exclusion radii μr(x) is
drawn from a Gaussian process for every sample according to
μ(n)
r (x) ∼ GP (0, kr(x − x′)), (49)
with kr = exp
{
− |x − x′|2 /lr2
}
the corresponding covariance kernel. Four microstructural samples
generated according to the described distributions are depicted in Figure 6. After generation of a microstructure, the domain is pixelized into a grid of size 256 × 256. The corresponding binary vector representing the phase of each pixel makes up the FGM input λf , i.e.:
dim(λf ) = 65, 536. (50)
The sampled microstructres are passed to the FEniCS mshr Logg et al. (2012) module to generate a triangular finite element mesh containing roughly 256 × 256 = 65, 536 vertices. The output of interest, i.e. uf , consists of the the interpolated pressure values on a regular grid of 129 × 129 i.e.:
dim(uf ) = 16, 641. (51)
Depending on the total number of exclusions N (n)
ex , generation of a single mesh takes in between ∼ 2 hours (for N (n)
ex ≈ 1, 000) up to 10 days (for N (n)
ex ≈ 20, 000) of computation time on a single Intel
6Note that the resulting distribution is distorted by the fact that a sampled exclusion is rejected if it overlaps with an exclusion which has been inserted before.
24


Xeon E5-2620 (2.00 GHz) CPU. The average solution for each FGM (i.e. Stokes-flow) PDE-solve takes 1512s ± 5.7s on the same hardware. We use boundary conditions on the FGM pressure Pbc(x) and velocity fields V bc(x) of the form
Pbc(x) = 0,
V bc(x) =
(ax + axyy ay + axyx
)
,
for x ∈ ΓP = 0,
for x ∈ ΓV = ∂Ω\0, (52)
where ΓP = 0 denotes the origin of the domain, i.e. the point x = 0. The boundary condition coefficients ax, ay and axy may assume different values in the following experiments and are therefore specified later.
3.1.2 The CGM uc(λc)
For the Darcy-type reduced order model uc(λc), we use a finite-element solver to Equations (8) as described in section 2.2, operating on a regular 16 × 16 square grid endowed with bilinear shape functions. The coarse-grained random permeability field K(x, λc) of the CGM Darcy solver is assumed to have the form defined by equations (9)–(10) throughout this section.
3.1.3 The encoder distribution pc
For the encoder distribution pc(λc|λf , θc), we assume a model as defined by equations (12)–(13), with the restriction that
λc,m =
Nglob ∑
j=1
θ ̃c,jmφglob,j (λf ) +
Nglob +Nloc ∑
j =Nglob +1
θ ̃c,jmφloc,j (λ(m)
f ) + σc,mZm,
Zm ∼ N (0, 1),
(53)
where λ(m)
f denotes the subset of the microstructure λf which corresponds to the permeability field cell Ωm. Equation (53) essentially states that the same set of features φm(λf ) = φ(λf ) =
{
φglob(λf ), φloc(λ(m)
f)
}
are used for every permeability field cell m, where φglob, φloc denote fea
tures evaluated on the global domain and on a local cell permeability field cell m, respectively. It is noted though that the coefficients θ ̃c,jm are generally different for different cells m. A Table of all feature functions used in the experiments is given in B.
Prior on θ ̃c,jm The prior defined in Equation (16) is restricted such that the precision hyperparameters γjm = γj, i.e. all permeability field cells m share the same set of hyperparameters γ. This leads to the same set of activated features, i.e. features with nonzero, but generally different θ ̃c,jm for different cells m, such that permeability cells are enabled to share information across the whole domain Ω.
3.1.4 The decoder distribution pcf
The decoder distribution we use in the experiments is defined by equations (14)–(15). As the FGM model output uf , we choose to use the Stokes flow pressure response uf,i = P (xi) linearly interpolated onto a square rectangular grid xi ∈ G(f) = 129 × 129 in the unit square domain
25


Figure 7: Prediction examples for data with μex = 8.35, σex = 0.6, σr = 0.5, ls = 1.5, lx = 0.1 and lr = 0.05, N = 128 training samples and a dim(λc) = 4 × 4 Darcy CGM. The colored surface is the true response P (x) of the test sample and the blue is the predictive mean μpred of the surrogate. The transparent gray surfaces are the predictive standard deviations μrped ± σpred. Note the different
scales on the z-axis. Note also that the higher the exclusion numbers N (n)
ex i.e. the smaller the pore phase length scales `f (see the two samples on the left), the better predictions are. This is because such data is closer to the homogenization limit as defined by Equation (5).
Ω = [0, 1]2. Although parameter uncertainty on the coarse-to-fine projection matrix W could readily be included to the model by specifying a suitable prior distribution p(W ), it has proven to be beneficial (in terms of predictive quality) in the low-data regime N . 100 to fix W to the CGM Darcy solver shape function interpolant, i.e.
Wij = ψc,j (xi), (54)
where ψc,j(x) is the Darcy solver finite element shape function associated with the CGM degree of freedom uc,j. The precisions τcf,i remain a free parameter and give the inverse reconstruction variance for uf,i = P (xi).
3.2 Case 1: Scale-separation (homogenization limit) & high data variability
To reveal the full potential of the proposed approach, we use input data that contains microstructures λf that fulfill the homogenization limit Equation (5) as much as it is computationally affordable, whilst having large variability, i.e. high σe2x, σr2, see equations (46)–(47). We therefore sample FGM inputs λf from the model presented earlier with the following parameter values:
μex = 8.35, σex = 0.6,
μr = −5.53, σr = 0.5, (55)
`s = 1.5, `x = 0.1,
The boundary conditions employed are of the form given in Equation (52) with
ax = ay = 1, axy = 0. (56)
We generate three training data sets of size N = 128 and report the averages of the performance metrics defined in 2.5.1 over the three models trained. We find
R2 = 0.995 ± 0.004, and M LL = −11.003 ± 0.013 (57)
26


Figure 8: Error measures coefficient of determination R2 and mean log likelihood M LL as defined in equations (41)–(43) as a function of the number of training samples N and for different discretization of the effective permeability field K(x, λc). The error metrics are computed on Ntest = 1024 test samples. The error bars are due to randomization of training data sets (errors due to variation of the test set can be neglected).
evaluated on a test set of size Ntest = 1024. Predictions for four random test samples are illustrated in Figure 7. We use dim(λc) = 16 corresponding to a uniform 4 × 4 square discretization of the problem domain Ω. In C, we report the results on the PCA decomposition of the FGM outputs uf . One observes a rapid decay of the corresponding eigenvalues in Figure 18 which would suggest that the problem could be addressed by methods that project the governing equations on the low-dimensional subspace spanned by the first few eigenvectors (e.g. reduced-basis (RB) methods Hesthaven et al. (2016); Quarteroni et al. (2016)). We would like to emphasize though that it would be unclear how the Stokes/Darcy homogenization process should be accomplished for the high-dimensional random porous domain described by λf , how to find the RB solution coefficients and if a standard Galerkin projection procedure would indeed be computationally more efficient. Moreover, we note that a RB-based Darcy CGM could readily be included as a substitute for the uc(λc) CGM as was defined in section 2.2, which might be a possible extension to investigate in the future. Apart from that, we emphasize that fast decay of PCA eigenmodes of the output data uf does not affect the high effective dimension of inputs λf .
3.3 Case 2: No scale separation (far from homogenization limit) and high output variability
In this setting, we consider microstructures that lead to PCA decompositions of the FGM outputs uf that are not concentrated on a very small subset of eigenvectors. In particular we generate microstructures based on the following parameters:
μex = 7.8, σex = 0.2,
μr = −5.23, σr = 0.3, (58)
`s = 1.2, `x = 0.08,
27


Figure 9: Predictive examples for N = 4, 16, 32 training data samples with μex = 7.8, σex = 0.2, σr = 0.3, ls = 1.2, lx = 0.08 and lr = 0.05, and a dim(λc) = 4 × 4 Darcy CGM. The colored surface is the true response P (x) of the test sample and the blue is the predictive mean μpred of the surrogate. The transparent gray surfaces give the predictive standard deviations μpred ± σpred. It can nicely be observed how predictions improve with increasing number of training data N .
see Figure 18 for a PCA analysis of the output data
{
u(n)
f
}2048
n=1
, revealing the effective output
dimension. Boundary conditions are again fixed to ax = ay = 1, axy = 0. We consider three cases in terms of dim(λc) which correspond to regular, square discretization of size 2×2 (i.e. dim(λc) = 4), 4×4 (i.e. dim(λc) = 16) and 8×8 (i.e. dim(λc) = 64). While the PDE discretization is independent of the representation of λc, it needs to be able to resolve the corresponding Darcy permeability field. To that end, we employ a regular square mesh of 16 × 16 bilinear elements for the CGM model, leading to dim(uc) = 288. One CGM evaluation requires (1.04 ± 2) × 10−3 s of computational time on a single Intel Xeon E5-2620 (2.00 GHz) CPU, as opposed to the 1512 ± 5.7s needed for each FGM solve. For predictive purposes (section 2.5), we use Nsamples = 100 for the estimation e.g. of
μpred, σ2
pred as defined in Equations (39) and (40). Figure 8 depicts both error metrics R2 and M LL as a function of the number of training samples N for the different dim(λc) as described above. One observes that with only N ≈ 32 training samples (i.e. FGM simulations), the asymptotic values for R2 and M LL are attained. Furthermore, it is interesting to note that the highest dim(λc) = 64 achieves the second best R2 score after dim(λc) = 16. This could be attributed to the higher information loss taking place using the local feature functions φloc on a finer grid in Equation (53). Figure 9 shows three predictive means μpred (blue) ± one predictive standard deviation σpred (transparent gray) on the same microstructure λf , but using different number of training data N = 4, 16, 32, computed with dim(λc) = 16. One can observe how the true solution (colored surface) is better captured the more data N is used for training.
3.3.1 Predictive variance σ2
pred & L2-error
In Figure 10, the predictive uncertainty σpred (top row) and the corresponding ‘true’ L2-error |uf (λf ) − μpred(λf )| are plotted for four different microstructures λf drawn according to the same distribution as in section 3.3, using a 4 × 4 permeability field discretization and N = 128 training samples. To provoke different predictive errors in different regions in the domain, we use
28


Figure 10: Predictive uncertainty σpred (top row) and corresponding absolute L2-error |uf (λf ) − μpred(λf )| for microstructural data drawn as in section 3.3, but with boundary conditions according to ax = ay = 0, axy = −1. It can be observed that the predictive uncertainty σpred is high/low in regions where the true absolute L2-error is high/low.
non-homogeneous flux boundary conditions of type (52) with ax = ay = 0, axy = −1. It can be observed that σpred is indeed higher in samples/regions where the true L2 error |uf (λf )−μpred(λf )| is higher, whereby it is incapable to resolve errors on a smaller scale than the one defined by the effective permeability field discretization.
3.3.2 Effective Darcy permeabilities
Further insight on the numerical homogenization process taking place in the λf 7→ λc encoding process is provided by Figure 11, which shows the mean effective permeability field 〈K(x, λc)〉Qλc (bottom row, see section 3.1.2) and the corresponding microstructures (top) for four randomly chosen training samples of a 4 × 4 effective model trained on N = 128 samples using boundary conditions and microstructure distributions as in 3.3. It is clear that effective permeabilities are lower in cells with higher solid fraction/interface area. Figure 12 provides information on the activated feature functions. These are determined by the posterior mean of the precision parameters γjm = γj that do not converge to large values (according
to Equation (31) when 〈γjm〉 → ∞ then σ2
θ ̃c,jm
→ 0). We note that, apart from the bias term, the
features activated pertain to the self-consistent approximation Torquato (2001) (infinite contrast limit), the squared area of total solid/fluid interfaces, and void-phase lineal-path function which pertains to the connectivity of the void phase. In Figure 12, dim(λc) = 16 is the dimension of the CGM’s constitutive parameter vector and a set of Nfeatures = dim(γ) = 150 feature functions φ(λf ) specified in B is used. The left side of the Figure 12 shows the average number of activated features (i.e. with nonzero 〈γj〉−1) as a function of the number of training data N , where error bars are due to randomization of training sets. As expected, the number of activated feature functions increases with the number of training data. The right part is an example of the final, converged
29


Figure 11: Mean effective permeability field 〈K(x, λc)〉Qλc (bottom) and corresponding microstructures (top). The data is chosen randomly from an N = 128 training set as was used in section 3.3.
values of 〈γj〉−1 for N = 128 training samples, where 5 features are activated (explained in the Figure caption).
3.4 Predictions under different boundary conditions
A particularly useful quality of the proposed coarse-grained model is that boundary conditions can be imprinted deterministically onto the Darcy-based CGM as specified in equations (8) (c-d). As a result, a model trained on boundary conditions, say a (see section 3.1.1), can be used for predictions on different boundary conditions, say  ̃a 6= a. Good predictive performance in such cases would suggest the ability of the proposed coarse-grained model to extrapolate which presupposes that it has correctly encoded salient physical information. This is a desirable trait in all surrogates which cannot always be achieved with black-box statistical models that attempt to interpolate the input-output map Tripathy and Bilionis (2018); Zhu and Zabaras (2018). As an example, we use the boundary conditions a = {ax = ay = 1, axy = 0} (as in section 3.3) and boundary conditions  ̃a = { ̃ax = a ̃y = 0,  ̃axy = −1}. We generate N = 128 FGM input-output training pairs for each of the aforementioned boundary conditions and train a coarse-grained model with dim(λc) = 16 as previously described. In Table 2, we report the performance metrics R2 and M LL as evaluated on test data generated with each of the above boundary conditions. We observe that the off-diagonal entries which correspond to different BCs for training and testing are comparable with the diagonal entries which correspond to identical BCs for training and testing. Indicative test samples and the corresponding predictions are shown in Figure 13. We observe again comparable predictive accuracy between the different combinations of training and test data. We also perform full randomization of boundary conditions in a further scenario, where different boundary conditions for every single test and training datum are drawn. In particular, we draw ax ∼ N (0, 1), ay ∼ N (0, 1), axy ∼ N (0, 1) and generate N = 128 training data (i.e. FGM input
30


Figure 12: Expected number of active features as a function of the number of training samples N (left). We see that the prior model described in section 2.3.3 effectively prunes most of the provided feature functions φ. The right side of the picture shows a bar plot of the converged values of the inverse precisions < γi >−1 corresponding to feature φi. These were found using a 4 × 4 effective permeability field discretization and 128 training samples with microstructures and boundary conditions as used in section 3.3. Activated are: the constant feature/bias (i.e. φi(λf ) = 1), the log self-consistent approximation Torquato (2001) (infinite contrast limit), the squared area of total solid/fluid interfaces, and fluid-phase lineal-path function evaluated at distances d = 0.025 and d = 0.005.
output pairs). Figure 14 depicts predictions over four test cases with randomly selected BCs as above, where it is clearly seen that the surrogate is capable of producing accurate estimates both in terms of the mean as well as the breadth of the predictive uncertainty. The error metrics R2 and M LL are evaluated on a Ntest = 1024 test set and yield the following values:
R2 = 0.9776 ± 0.0027 and M LL = −11.07 ± 0.089 (59)
which are comparable to the values provided earlier.
3.5 Uncertainty propagation problem
The numerical experiment carried out in this section pertains to a simple uncertainty propagation (UP) problem. In a UP setting, one is interested in computing statistics of some quantity of interest (QoI) f (uf ) of the FGM output uf given some density p(λf ) for the FGM input λf . For the sake of illustration we assume that the QoI is scalar and therefore its density can be written as
p(f ) =
∫
δ(f − f (uf ))p(uf |λf )p(λf )dλf duf , (60)
where p(uf |λf ) = δ(uf − uf (λf )) if uf is computed by running the expensive FGM uf (λf ). One can approximate p(f ) with a histogram where the values in each bin are estimated with Monte Carlo (MC) sampling. Due to slow MC convergence rates and the high cost of the FGM forward solver, an obvious strategy is to replace the FGM by the probabilistic coarse-grained model proposed and more specifically the predictive posterior ppred(uf |λf , D) (section 2.5) as follows:
p(f |D) =
∫
δ(f − f (uf ))ppred(uf |λf , D)p(λf )dλf duf . (61)
31


Figure 13: Cross-prediction example on boundary conditions a = {ax = 1, ay = 1, axy = 0} and  ̃a = {a ̃x = 0,  ̃ay = 0,  ̃axy = −1} using a 4 × 4 effective permeability field and N = 128 training samples. The colored surface is the true solution of a test sample, the blue is the predictive mean μpred of the surrogate and the transparent gray surfaces indicate the predictive error μpred ± σpred. The figures on the diagonal have identical boundary conditions a,  ̃a on both training and test data. The top right example is trained on data with boundary conditions  ̃a, but tested on data with boundary conditions a, whereas the lower left is trained on a, but tested on  ̃a.
To illustrate the performance of such a scheme, we use as the QoI the FGM pressure P (x) at
x = (1, 1)T (i.e. top right corner of the problem domain Ω). The blue line in Figure 15 shows a kernel density estimate of the MC-based histogram of 10,000 FGM runs. The black line is based on a trained coarse-grained model (dim(λc) = 16) with N = 32 training samples. The gray shaded area of Figure 15 aims to capture predictive uncertainties about p(f ) due to limited data. To that end, instead of ppred(uf |λf , D) as defined in Equation (36), we generate samples of the model
parameters θ ̃(s)
c , τ (s)
c and τ (s)
cf from their approximate posteriors Qθ ̃c , Qτ c , Qτ cf and for each of those values we generate samples from
p(f |θ ̃(s)
c , τ (s)
c , τ (s)
cf , D) =
∫
δ(f − f (uf ))ppred(uf |λf , θ ̃(s)
c , τ (s)
c , τ (s)
cf , D)p(λf )dλf duf .
(62)
The mean and 90% credible intervals for the histogram value in each bin are subsequently computed and plotted in Figure 15. One can observe that the credible intervals envelop correctly even the tails of the true, MC-based histogram.
32


Coefficient of determination R2
trained on
prediction on a = ( ax=1 ay=1 axy=0 )T  ̃a = ( a ̃x=0 a ̃y=0 a ̃xy=−1 )T
a = ( ax=1 ay=1 axy=0 )T 0.930 ± 1.3 · 10−3 0.863 ± 1.1 · 10−2 a ̃ = (  ̃ax=0 a ̃y=0 a ̃xy=−1 )T 0.887 ± 7.5 · 10−3 0.903 ± 7.0 · 10−3
Mean log likelihood M LL
trained on
prediction on a = ( ax=1 ay=1 axy=0 )T  ̃a = ( a ̃x=0 a ̃y=0 a ̃xy=−1 )T
a = ( ax=1 ay=1 axy=0 )T −11.1 ± 7.6 · 10−3 −11.8 ± 9.3 · 10−3 a ̃ = (  ̃ax=0 a ̃y=0 a ̃xy=−1 )T −10.2 ± 1.3 · 10−2 −9.93 ± 2.3 · 10−2
Table 2: Error metrics R2 and M LL corresponding to the cross-prediction example in section 3.4 using distinct boundary conditions a and  ̃a. Metrics are evaluated and averaged over 3 separate training sets of N = 128 size (indicated errors are due to variation of training data) and tested on Ntest = 1024 microstructural samples. Only slight deterioration is observed if boundary conditions of the training and test sets differ.
3.6 Illustration of automatic adaptive refinement
In this section, we illustrate the automatic adaptive refinement scheme of the coarse-grained model as explained in section 2.7. We consider as the base model dim(λc) = 4 which corresponds to a regular 2 × 2 square grid discretization of the Darcy permeability field (see top left of Figure 17). In order to better illustrate the capabilities of the proposed method, we draw the FGM microstructural samples, i.e. λf , from a particular distribution and refer to the corresponding samples as ‘tiled’ microstructures. These exhibit a homogeneous distribution of circular exclusions (solid phase) over the whole domain except for the lower left quadrant, i.e. Ωll = [0, 0.5] × [0, 0.5]. In this region, exclusions are distributed homogeneously on sub-cells of size 0.125×0.125, but with varying random volume fraction as can be seen in Figure 16 where four representative microstructures are depicted. Due to this localized inhomogeneity, it is natural to expect that refinements in [0, 0.5] × [0, 0.5] would be most promising. The baseline dim(λc) = 2 × 2 model is trained on N = 32 data generated by the Stokes-based FGM. After training, the cell scoring function defined in equation (45) indicates which of the 2 × 2 = 4 cells of constant permeability should be split into four new square sub-cells. After splitting that cell, training continues with all model parameters initialized to their previously found values. To facilitate the problem as much as possible, boundary conditions of type ax = 1, ay = axy = 0 are used, i.e. uniform expected velocity 〈V (x)〉 = (1, 0)T all over the domain. Given these boundary conditions and the ‘tiled’ microstructures as described above, it should be beneficial to have a finer CGM permeability field discretization in Ωll than in the rest of the domain, where exclusions are distributed homogeneously. Figure 17 depicts the process of 4 successive splits each of which corresponds to dividing the selected cell into four equal-sized squares. Hence the number of cells/subregions and therefore the dimension of the associated dim(λc) will become 7, 10, 13 and 16 after each of the aforementioned
33


Figure 14: Predictions on four randomly chosen test samples where boundary conditions in both training and test data are randomized according to ax ∼ N (0, 1), ay ∼ N (0, 1), axy ∼ N (0, 1). The colored surface is the true Stokes pressure response of the test sample, the blue is the predictive mean μpred of the surrogate and the transparent gray surfaces show the predictive standard deviation
μpred ± σpred.
splits. The left part of Figure 17 shows the cell scores Fm(Q) computed as in Equation (45) right before each split. The cell that is subsequently subdivided is the one with the lowest Fm(Q). As expected, the splits are concentrated in the lower-left quadrant Ωll and the right part of the Figure shows the corresponding discretization of the permeability field after the 4 splits. The tendency to refine close to the origin x = 0 (lower left corner) and along the lower boundary y = 0 may be explained by the fact that the microstructures are constructed such that solid phase exclusions have a minimum distance of d = 0.003 to the domain boundary, i.e. there always exists a narrow path along the boundaries where fluid can flow. As boundary conditions imply flux from left to right, it is beneficial to refine close to the lower domain boundary. The lower left corner is further distinguished by the fact that the pressure boundary condition P (x = 0) = 0 is implied here. More importantly, the middle part of 17 shows the evolution of the ELBO over training iterations (blue line). The jumps that are observed after each split are a result of the improvement in the model which is to be expected due to the increased dimension dim(λc). We also compare the ELBO score after 4 splits (where dim(λc) = 16) with a regular discretization of the Darcy permeability into 4 × 4 squares in which dim(λc) = 16 as well, i.e. the dimension of the information bottleneck variables is the same. We note that the adaptive refinement proposed leads to an improved final model.
4 Conclusions
We have presented a physics-informed, fully Bayesian surrogate model to predict response fields of stochastic partial differential equations with high-dimensional stochastic input as those arising in random heterogeneous media. Employing an encoder-decoder architecture with a coarse-grained model (CGM) based on simplified physics as a stencil at its core, it is possible to provide tight and accurate probabilistic predictions even when the training data i.e. fine-grained model simulations, is scarce (i.e. of the order of N ≈ 10∼100) and the stochastic input dimension is large (i.e. ∼ 104 or larger). The encoding or coarse-graining step is capable of extracting the salient features of the input random field which are most predictive of the fine-scale PDE-response, thereby drastically reducing the stochastic input dimension in an optimal way. This low-dimensional, latent representation then serves as the input to the CGM, the output of which is basal for the probabilistic reconstruction of the fine scale output. Predictive uncertainty due to limited training data as well as due to the
34


Figure 15: Probability density function for the fine-scale Stokes pressure response at x = (1, 1)T estimated using 10,000 Monte Carlo samples (blue line) and a surrogate model (black) trained on N = 32 training samples using an effective Darcy solver based on a 4 × 4 permeability field discretization. Microstructures are sampled as in section 3.3. The gray shaded area shows the 90% credible intervals obtained by propagating the uncertainty of model parameters θ ̃c, τ c and τ cf as described in the text.
information loss unavoidably taking place during the coarse-graining step is accurately quantified by the fully probabilistic nature of the model. All parts of the model were trained simultaneously in a fully Bayesian way using Stochastic Variational Inference, without the need of any parameters to be specified by the analyst. On the basis of the Stokes/Darcy similarity for flow through random porous media, we provided numerical evidence of the predictive capabilities under the aforementioned setting. Another important advantage of the proposed formulation is its ability to perform comparably well under extrapolative conditions. In the context of the models explored, we demonstrated this by carrying out predictions on data under a considerably different (or even randomized) boundary conditions than those used for training. Furthermore, we demonstrated one of the many possible application of the proposed model for Uncertainty Propagation problems where not only accurate estimates of output statistics but the confidence in these predictions is quantified. Finally, we outlined a method for automatically and adaptively refining the model and demonstrated its benefits in the context of the problems explored. Applications of the proposed model can be found in any multi-query setting with expensive forward model evaluations, as is often the case in design-, optimization- or inverse problems. Several model extensions can be contemplated. For coarse-graining of the high-dimensional stochastic inputs, a deep learning framework could be adopted, potentially benefiting from recent advances in computer vision with e.g. convolutional neural networks (CNNs) or deep Gaussian process models. We emphasize however that significant regularization would be needed to make those work in the Small Data setting considered. Furthermore, one might think of a model that not only passes information through the CGM, but also entails ‘bypassing’ components, thereby effectively expanding the information bottleneck and allowing for more detailed reconstruction of fine scale output variability.
35


Figure 16: ‘Tiled’ microstructures, where spherical exclusions (solid phase) are distributed homogeneously except for the lower left corner Ωll = [0, 0.5] × [0, 0.5] of the domain, where exclusions are homogeneously distributed (with differing random volume fraction) on 0.125 × 0.125 sub-cells. Such microstructures are used to enforce adaptive refinement in the lower left corner Ωll of the domain.
Figure 17: Left: Cell scoring function −F (i)
m given in equation (45) directly before every split
(lighter color indicates smaller F (i)
m ). Middle: evidence lower bound (ELBO) as given by equations (44), (74). The blue line shows the ELBO vs. training iteration of the adaptively refined model, the black line shows the final ELBO of a model with effective permeability discretized on a 4 × 4 square grid. The right part of the Figure shows the final effective permeability field discretization (blue grid) with a representative microstructure underneath.
A Optimizing the variational distributions Q
To maximize the ELBO functional F(Q) in equation (25) w.r.t. the variational distributions Q under the normalization constraint ∫ Qθk (θk)dθk = 1, we maximize the functional J [Q; ξ]
J [Q; ξ] =
∫ ∏
k
(Qθk (θk)) log p(θ, D)
∏
l (Qiθl(θl)) dθ
+
∑
k
ξk
(∫
Qθk (θk)dθk − 1
)
=
∫ ∏
k
(Qθk (θk)) log p(θ, D)dθ −
∑
k
∫
Qθk (θk) log Q(θk)dθk
+
∑
k
ξk
(∫
Qθk (θk)dθk − 1
)
(63)
36


where θk ⊂ θ =
{ {
λ(n)
c
}N
n=1
, θ ̃c, τ c, τ cf , γ
}
is a subset of the model parameters θ and ξ are
Lagrangean multipliers. The first order variation δJ w.r.t. Qθk is
δJ (θk; ξj) =
∫
δQθk (θk)
( ∫ ∏
i6=k
(Qθi (θi)) log p(θ, D)dθ−k
− log Qθk (θk) − 1 + ξk
)
dθk ,
(64)
where dθ−k = ∏
i6=k dθi. As δJ != 0 for arbitrary δQθk ,
∫ ∏
i6=k
(Qθi (θi)) log p(θ, D)dθ−k − log Qθk (θk) − 1 + ξk = 0. (65)
or
log Qθk (θk) = 〈log p(θ, D)〉i6=k + ξk − 1, (66)
where 〈 . 〉i6=k denotes the expected value w.r.t. ∏
i6=k (Qθi (θi)). The normalization constraints are
recaptured by ∇ξJ =! 0 and determine the constant ξk − 1 in Equation (66). Thus, it is found that
Qθk (θk) = exp 〈log p(θ, D)〉i6=k
∫ exp 〈log p(θ, D)〉i6=k dθi
. (67)
A.1 Black-box variational inference for Qλ(n)
c
We note that minimization of the KL divergence as given in equation (35) is equivalent to
arg min
μ(n)
λc ,σ(n)
λc
KL
( Q ̃λ(n)
c (λ(n)
c |μ(n)
λc , σ(n)
λc )
∣ ∣ ∣
∣
∣
∣Qλ(n)
c (λ(n)
c)
)
= arg min
μ(n)
λc ,σ(n)
λc
∫
Q ̃ λ(n)
c (λ(n)
c |μ(n)
λc , σ(n)
λc ) log
Q ̃ λ(n)
c (λ(n)
c |μ(n)
λc , σ(n)
λc )
Qλ(n)
c (λ(n)
c ) dλ(n)
c
= arg max
μ(n)
λc ,σ(n)
λc


〈
log N (u(n)
f |W uc(λ(n)
c ), diag[τ cf ]−1)
〉
Q ̃ λ(cn)
+
dim(λc )
∑
m=1
〈
log N (λ(n)
c,m|θ ̃T
c,mφm(λ(n)
f ), τ −1
c,m)
〉
Q ̃ λ(cn)
+H
(Q ̃ λ(n)
c (λ(n)
c)
)

,
(68)
where H
(Q ̃ λ(n)
c (λ(n)
c)
)
is the Shannon entropy of Q ̃λ(n)
c . Expected values of terms in (68) depending
on the CGM uc(λ(n)
c ) are analytically intractable and gradients w.r.t. μ(n)
λc , σ(n)
λc are therefore only
37


accesible by noisy Monte Carlo estimates. To reduce the variance in those estimates, we apply the reparametrization trick Kingma and Welling (2013)
λ(n)
c,m = μ(n)
λc,m + σ(n)
λc ,m (n)
m , (n)
m ∼ N (0, 1) (69)
and express Equation (68) as
μ(n)
λc , σ(n)
λc =
arg max
μ(n)
λc ,σ(n)
λc


〈
log N (u(n)
f |W uc(μ(n)
λc + σ(n)
λc ◦ (n)), diag[τ cf ]−1)
〉
N ((n)|0,I)
+
dim(λc )
∑
m=1
〈
log N (μ(n)
λc,m + σ(n)
λc ,m (n)
m |θ ̃T
c,mφm(λ(n)
f ), τ −1
c,m)
〉
N ((n)|0,I)
+
dim(λc )
∑
m=1
log σ(n)
λc ,m

,
(70)
where ‘◦’ denotes element-wise multiplication and we used the fact that
H
(Q ̃ λ(n)
c (λ(n)
c)
)
∝
∑dim(λc )
m=1 log σ(n)
λc,m. Derivatives w.r.t. μ(n)
λc , σ(n)
λc can be computed using the
chain rule as
∂
∂ μ(n)
λc
:
〈
∂ λ(n)
c ∂ μ(n)
λc
∂uc(λ(n)
c)
∂ λ(n)
c
∂ ∂uc
log N (u(n)
f |W uc(λ(n)
c ), diag[τ cf ]−1)
〉
p((n) )
(71)
+
dim(λc )
∑
m=1
〈
∂ λ(n)
c,m ∂ μ(n)
λc
∂
∂ λ(n)
c,m
log N (λ(n)
c,m|θ ̃T
c,m φm (λ(n)
f ), τ −1
c,m )
〉
p((n) )
,
∂
∂ σ (n)
λc
:
〈
∂ λ(n)
c ∂ σ (n)
λc
∂uc(λ(n)
c)
∂ λ(n)
c
∂ ∂uc
log N (u(n)
f |W uc(λ(n)
c ), diag[τ cf ]−1)
〉
p((n) )
(72)
+
dim(λc )
∑
m=1
〈
∂ λ(n)
c,m ∂ σ (n)
λc
∂
∂ λ(n)
c,m
log N (λ(n)
c,m|θ ̃T
c,m φm (λ(n)
f ), τ −1
c,m )
〉
p((n) )
+ (σ(n)
λc )−1, (73)
where ∂λ(n)
c /∂μ(n)
λc = I and ∂λ(n)
c /∂σ(n)
λc = diag[(n)]. To solve the optimization problem in (70), the above gradients are passed to the adaptive moment estimation optimizer (ADAM) using the default optimization parameters suggested in Kingma and Ba (2014).
B Feature functions
38


Feature functions φ
Index j Function φjm Comment 1 constant φj = 1 2 pore fraction in Ω pore fraction evaluated on full domain Ω 3 log pore fraction in Ω 4–7 (pore fraction)0.5...0.5...2.5 in Ω 8 exp(pore fraction) 9 log SCA in Ω log self-cons. approx. Torquato (2001), sec. 18.1.2, inf. contrast limit 10 Maxwell-Approximation in Ω inf. contrast limit, see Torquato (2001), sec. 18.2.1 11 log Maxwell-Approximation 12–17 log chord length density in Ω, d = 0.05 . . . 0 see Torquato (2001) sec. 6.2.4 18 interface area in Ω
19 log interface area in Ω 20–27 | log interface area|3/2,1/2,1/3,1/4,1/5,2,3,4 in Ω
28–31 (interface area)1/3,1/4,1/5,2 in Ω 32 mean distance edge in Ω measured from excl. edge to edge 33 log mean distance edge in Ω measured from excl. edge to edge 34 log2 mean distance edge in Ω measured from excl. edge to edge 35 log3 mean distance edge in Ω measured from excl. edge to edge 36 mean distance center in Ω measured from excl. center to center
37 min. distance center in Ω measured from excl. center to center
38 log min. distance center in Ω measured from excl. center to center 39 log2 min. distance center in Ω measured from excl. center to center 40–43 lineal path in Ω for d = 0.025, 0.01, 0.005, 0.002 see Torquato (2001), sec. 2.4 44–47 log lineal path in Ω for d = 0.025, 0.01, 0.005, 0.002 48 void nearest-neighbor pdf, d = 0, in Ω see Torquato (2001), sec. 2.8 49 log void nearest-neighbor pdf, d = 0, in Ω see Torquato (2001). sec. 2.8 50 pore size density, d = 0, in Ω see Torquato (2001), sec. 2.6 51 log pore size density, d = 0, in Ω see Torquato (2001), sec. 2.6 52 mean chord length in Ω see Torquato (2001), sec. 2.5 53 log mean chord length in Ω see Torquato (2001), sec. 2.5 54 exp mean chord length in Ω see Torquato (2001), sec. 2.5 55 (mean chord length)0.5 in Ω see Torquato (2001), sec. 2.5 56–58
〈
r0.2,0.5,1
ex
〉
in Ωm expected exclusion radii moments
59–61 log
〈
r0.2,0.5,1
ex
〉
in Ωm log expected exclusion radii moments
62 pore fraction in Ωm
63 log pore fraction in Ωm
64 exp pore fraction in Ωm
65–68 (pore fraction)0.5,1.5,2,2.5 in Ωm 69 log self-consistent approximation (inf. contrast) in Ωm
70 Maxwell Approximation in Ωm
71 log Maxwell Approximation in Ωm
72–78 log chord length dens. in Ωm,
d = (50, 25, 12.5, 6.25, 3, 1.5, 0) · 10−4 79 interface area in Ωm
80–88 | log interface area|1,1.5,2,3,4,5,1/2,1/3,1/4 in Ωm
89–93 | interface area|1/2,1/3,1/4,1/5,2 in Ωm
94 mean distance edge in Ωm measured from excl. edge to edge
95 log mean distance edge in Ωm measured from excl. edge to edge
96 max distance edge in Ωm measured from excl. edge to edge
97 log max distance edge in Ωm measured from excl. edge to edge
98 std distance edge in Ωm measured from excl. edge to edge
99 log std distance edge in Ωm measured from excl. edge to edge
100–104 square well potential, width = (1, 2, 3, 4, 5) · 10−2 in Ωm
105 log2 mean distance in Ωm measured from excl. edge to edge
106 log3 mean distance in Ωm measured from excl. edge to edge
107 mean distance center in Ωm measured from excl. center to center
108 min. distance center in Ωm measured from excl. center to center
109 log min. distance center in Ωm measured from excl. center to center
110 log2 min. distance center in Ωm measured from excl. center to center
111–114 lineal path in Ωm for d = 0.025, 0.01, 0.005, 0.002
115–118 log lineal path in Ωm for d = 0.025, 0.01, 0.005, 0.002
119 void nearest-neighbor pdf, d = 0, in Ωm see Torquato (2001), sec. 2.8 120 log void nearest-neighbor pdf, d = 0, in Ωm see Torquato (2001). sec. 2.8 121 pore size density, d = 0, in Ωm see Torquato (2001), sec. 2.6 122 log pore size density, d = 0, in Ωm see Torquato (2001), sec. 2.6 123 mean chord length in Ωm see Torquato (2001), sec. 2.5 124 log mean chord length in Ωm see Torquato (2001), sec. 2.5 125 exp mean chord length in Ωm see Torquato (2001), sec. 2.5
126 (mean chord length)0.5 in Ωm see Torquato (2001), sec. 2.5 127–129
〈
r0.2,0.5,1
ex
〉
in Ωm expected exclusion radii moments
130–132 log
〈
r0.2,0.5,1
ex
〉
in Ωm log expected exclusion radii moments
133 length scale of exp. approx. to lin. path in Ωm
134 mean of euclidean dist. transform in Ωm see Soille (1999) 135 variance of euclidean dist. transform in Ωm see Soille (1999) 136 max. of euclidean dist. transform in Ωm see Soille (1999) 137 mean of chessboard dist. transform in Ωm see Soille (1999) 138 variance of chessboard dist. transform in Ωm see Soille (1999) 139 max. of chessboard dist. transform in Ωm see Soille (1999) 140 mean of cityblock dist. transform in Ωm see Soille (1999) 141 variance of cityblock dist. transform in Ωm see Soille (1999) 142 max. of cityblock dist. transform in Ωm see Soille (1999) 143–145 Gauss lin. filt. d = 2, 5, 10 pixels in Ωm see Grigo and Koutsourelakis (2019)
146 Ising energy in Ωm
147 shortest connected path, x-dir., Euclidean, in Ωm see Soille (1999) 148 shortest connected path, y-dir., Euclidean, in Ωm see Soille (1999) 149 shortest connected path, x-dir., cityblock, in Ωm see Soille (1999) 150 shortest connected path, y-dir., cityblock, in Ωm see Soille (1999)
Table 3: Set of 150 feature functions φ applied in the numerical examples of section 3.
39


Table 3 shows a list of the 150 feature functions used in the numerical examples of section 3. Features 1–55 take the whole microstructure λf as input, features 56–150 use the subset λ(m)
f
pertaining to subdomain/cell Ωm for which Km(x, λc) = eλc,m I.
C Proper orthogonal decomposition of output data uf
Output data, section 3.2 Output data, section 3.3
Input data, section 3.2 Input data, section 3.3
Figure 18: Explained variance versus number of PCA components of the output uf (top) and input λf (bottom) for the data used in sections 3.2 (left) and 3.3 (right). All plots are based on a PCA analysis using 2048 random discretized input/output samples λf , uf drawn according to the distributions described in the text.
In section 3.2, it is mentioned that we observe very fast decay of POD eigenmodes for data with large variance in the pore space fraction vol(Ωf )/vol(Ω), which translates to high σe2x as explained in section 3.1.1. We therefore plot the explained PCA variance, i.e. the cumulative sum of the n
largest variances of a PCA analysis performed on 2048 sample vectors DP CA =
{
uf (λ(n)
f)
}2048
n=1
for
the data used in sections 3.2 and 3.3 in figure 18. We note that this fast decay in POD eigenmodes observed in section 3.2 does not affect the effective input dimensionality of λf . To show that, we discretize the 2048 microstructures from the distributions defined in sections 3.2, 3.3 to bitmap image vectors of 256 × 256 = 65, 536 dimensions and again plot the explained variance from the first n components in the second row of figure 18. We observe that our claim of high-dimensional inputs is verified, as the explained variance is only slowly increasing with the number of POD components.
40


D Evidence Lower BOund (ELBO) F(Q)
Using the expected values found in equations (28)–(32), omitting constants and canceling terms, we find
F(Q) = 〈log p(θ, D) − log Q(θ)〉
= −e ̃
dim(uf )
∑
i=1
log f ̃i +
dim(λc )
∑
m=1
N
∑
n=1
log σ(n)
λc,m − c ̃
dim(λc )
∑
m=1
log d ̃m
−  ̃a
dim(λc )
∑
m=1
dim(γ )
∑
j=1
log  ̃bjm +
dim(λc )
∑
m=1
dim(γ )
∑
j=1
log σθ ̃c,jm
(74)
as the most compact form of the evidence lower bound (ELBO) F(Q). We note that with the exception of the first term, the remaining ones are automatically separated into additive contributions from each cell m or equivalently each λc,m. Hence, in order to arrive at the cell-scoring function Fm(Q) given in Equation (45), we pick out the terms in equation (74) which explicitly carry a cell index m. These are
Fm(Q) =
N
∑
n=1
log σ(n)
λc,m − c ̃log d ̃m − a ̃
dim(γ )
∑
j=1
log  ̃bjm +
dim(γ )
∑
j=1
log σθ ̃c,jm .
In the numerical experiments, we tie the precision parameters γjm across different cells m together i.e. γjm = γj which allows sharing of information in terms of the microstructural features. As a result bjm = bj and the corresponding contribution from each cell m becomes:
Fm(Q) =
N
∑
n=1
log σ(n)
λc,m − c ̃log d ̃m +
dim(γ )
∑
j=1
log σθ ̃c,jm . (75)
References
Z. Ghahramani, Probabilistic machine learning and artificial intelligence, Nature 521 (2015) 452459.
Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (2015) 436–444.
I. Goodfellow, Y. Bengio, A. Courville, Deep Learning, MIT Press, 2016. http://www. deeplearningbook.org.
P. S. Koutsourelakis, N. Zabaras, M. Girolami, Special Issue: Big data and predictive computational modeling, Journal of Computational Physics 321 (2016) 1252–1254.
N. Wiener, The Homogeneous Chaos, American Journal of Mathematics 60 (1938) 897–936.
R. G. Ghanem, P. D. Spanos, Stochastic Finite Elements: A Spectral Approach, Springer, New York, 1991. URL: http://cds.cern.ch/record/1622736.
D. Xiu, G. Karniadakis, The Wiener–Askey Polynomial Chaos for Stochastic Differential Equations, SIAM Journal on Scientific Computing 24 (2002) 619–644.
41


D. Xiu, J. Hesthaven, High-Order Collocation Methods for Differential Equations with Random Inputs, SIAM Journal on Scientific Computing 27 (2005) 1118–1139.
X. Ma, N. Zabaras, An adaptive hierarchical sparse grid collocation algorithm for the solution of stochastic differential equations, Journal of Computational Physics 228 (2009) 3084 – 3113.
G. Lin, A. Tartakovsky, An efficient, high-order probabilistic collocation method on sparse grids for three-dimensional flow and solute transport in randomly heterogeneous porous media, Advances in Water Resources 32 (2009) 712 – 722. Dispersion in Porous Media.
G. Lin, A. Tartakovsky, D. Tartakovsky, Uncertainty quantification via random domain decomposition and probabilistic collocation on sparse grids, Journal of Computational Physics 229 (2010) 6995 – 7012.
R. Tipireddy, P. Stinis, A. Tartakovsky, Basis adaptation and domain decomposition for steadystate partial differential equations with random coefficients, Journal of Computational Physics 351 (2017) 203 – 215.
R. Tipireddy, P. Stinis, A. Tartakovsky, Stochastic Basis Adaptation and Spatial Domain Decomposition for Partial Differential Equations with Random Coefficients, SIAM/ASA Journal on Uncertainty Quantification 6 (2018) 273–301.
S. Torquato, B. Lu, Chord-length distribution function for two-phase random media, Phys. Rev. E 47 (1993) 2950–2953.
I. Bilionis, N. Zabaras, B. A. Konomi, G. Lin, Multi-output separable Gaussian process: Towards an efficient, fully Bayesian paradigm for uncertainty quantification, Journal of Computational Physics 241 (2013) 212 – 239.
S. Atkinson, N. Zabaras, Structured Bayesian Gaussian process latent variable model: Applications to data-driven dimensionality reduction and high-dimensional inversion, Journal of Computational Physics (2019).
S. Mo, Y. Zhu, N. Zabaras, X. Shi, J. Wu, Deep Convolutional Encoder-Decoder Networks for Uncertainty Quantification of Dynamic Multiphase Flow in Heterogeneous Media, Water Resources Research 0 (2018).
C. Chauvi`ere, J. Hesthaven, L. Lurati, Computational Modeling of Uncertainty in Time-Domain Electromagnetics, SIAM Journal on Scientific Computing 28 (2006) 751–775.
A. O’Hagan, M. Kennedy, Predicting the output from a complex computer code when fast approximations are available, Biometrika 87 (2000) 1–13.
P.-S. Koutsourelakis, Accurate Uncertainty Quantification Using Inaccurate Computational Models, SIAM Journal on Scientific Computing 31 (2009) 3274–3300.
P. Perdikaris, M. Raissi, A. Damianou, N. D. Lawrence, G. E. Karniadakis, Nonlinear information fusion algorithms for data-efficient multi-fidelity modelling, Proceedings of the Royal Society A: Mathematical, Physical and Engineering Science 473 (2017) 20160751.
42


J. Hesthaven, G. Rozza, B. Stamm, Certified Reduced Basis Methods for Parametrized Partial Differential Equations, Springer Briefs in Mathematics, Springer International Publishing, 2016. URL: //www.springer.com/de/book/9783319224695.
A. Quarteroni, A. Manzoni, F. Negri, Reduced Basis Methods for Partial Differential Equations. An Introduction, La Matematica per il 3+2. 92, Springer International Publishing, 2016. URL: http://infoscience.epfl.ch/record/218966. doi:10.1007/978-3-319-15431-2.
C. W. Rowley, T. Colonius, R. M. Murray, Model reduction for compressible flows using POD and Galerkin projection, Physica D: Nonlinear Phenomena 189 (2004) 115 – 129.
T. Cui, Y. Marzouk, K. Willcox, Data-driven model reduction for the bayesian solution of inverse problems, International Journal for Numerical Methods in Engineering 102 (2015) 966–990.
B. Afkham, J. Hesthaven, Structure Preserving Model Reduction of Parametric Hamiltonian Systems, SIAM Journal on Scientific Computing 39 (2017) A2616–A2644.
J. Yu, J. Hesthaven, Flowfield Reconstruction Method Using Artificial Neural Network, AIAA Journal (2018) 1–17.
M. Guo, J. Hesthaven, Reduced order modeling for nonlinear structural analysis using gaussian process regression, Computer Methods in Applied Mechanics and Engineering 341 (2018) 807 826.
J. Hesthaven, S. Ubbiali, Non-intrusive reduced order modeling of nonlinear problems using neural networks, Journal of Computational Physics 363 (2018) 55 – 78.
C. Rasmussen, C. Williams, Gaussian Processes for Machine Learning, Adaptive Computation and Machine Learning, MIT Press, Cambridge, MA, USA, 2006.
I. Bilionis, N. Zabaras, Bayesian Uncertainty Propagation Using Gaussian Processes, Springer International Publishing, Cham, 2017, pp. 555–599. URL: https://doi.org/10.1007/ 978-3-319-12385-1_16. doi:10.1007/978-3-319-12385-1_16.
M. Raissi, P. Perdikaris, G. E. Karniadakis, Inferring solutions of differential equations using noisy multi-fidelity data, Journal of Computational Physics 335 (2017) 736 – 746.
P. Perdikaris, D. Venturi, J. O. Royset, G. E. Karniadakis, Multi-fidelity modelling via recursive cokriging and Gaussian–Markov random fields, Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 471 (2015) 20150018.
M. Raissi, G. E. Karniadakis, Hidden physics models: Machine learning of nonlinear partial differential equations, Journal of Computational Physics 357 (2018) 125 – 141.
X. Yang, G. Tartakovsky, A. Tartakovsky, Physics-Informed Kriging: A Physics-Informed Gaussian Process Regression Method for Data-Model Convergence, arxiv e-print (2018).
S. Lee, F. Dietrich, G. Karniadakis, I. Kevrekidis, Linking Gaussian Process regression with datadriven manifold embeddings for nonlinear data fusion, arxiv e-print (2018).
43


R. Tipireddy, A. Tartakovsky, Physics-informed Machine Learning Method for Forecasting and Uncertainty Quantification of Partially Observed and Unobserved States in Power Grids, arxiv e-print (2018).
R. Tripathy, I. Bilionis, Deep UQ: Learning deep neural network surrogate models for high dimensional uncertainty quantification, arXiv:1802.00850 [physics, stat] (2018).
Y. Zhu, N. Zabaras, Bayesian deep convolutional encoderdecoder networks for surrogate modeling and uncertainty quantification, Journal of Computational Physics 366 (2018) 415 – 447.
Y. Yang, P. Perdikaris, Conditional deep surrogate models for stochastic, high-dimensional, and multi-fidelity systems, arxiv e-print (2019).
M. Raissi, P. Perdikaris, G. Karniadakis, Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations, arxiv e-print (2017).
M. Raissi, P. Perdikaris, G. Karniadakis, Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations, Journal of Computational Physics 378 (2019) 686 – 707.
A. Tartakovsky, C. O. Marrero, P. Perdikaris, G. Tartakovsky, D. Barajas-Solano, Learning Parameters and Constitutive Relationships with Physics Informed Deep Neural Networks, arxiv e-print (2018).
M. Raissi, P. Perdikaris, G. E. Karniadakis, Physics informed deep learning (part ii): data-driven discovery of nonlinear partial differential equations, arXiv preprint arXiv:1711.10566 (2017).
Y. Zhu, N. Zabaras, P.-S. Koutsourelakis, P. Perdikaris, Physics-Constrained Deep Learning for High-dimensional Surrogate Modeling and Uncertainty Quantification without Labeled Data, Journal of Computational Physics (2019). Submitted for publication.
M. Sch ̈oberl, N. Zabaras, P.-S. Koutsourelakis, Predictive coarse-graining, Journal of Computational Physics 333 (2017) 49–77.
X. Ma, N. Zabaras, Kernel principal component analysis for stochastic input model generation, Journal of Computational Physics 230 (2011) 7311 – 7331.
W. W. Xing, V. Triantafyllidis, A. A. Shah, P. B. Nair, N. Zabaras, Manifold learning for the emulation of spatial fields from computational models, Journal of Computational Physics 326 (2016) 666–690.
J. Paisley, D. Blei, M. I. Jordan, Variational Bayesian inference with stochastic search, 29th International Conference on Machine Learning (ICML) (2012).
M. D. Hoffman, D. M. Blei, C. Wang, J. Paisley, Stochastic Variational Inference, J. Mach. Learn. Res. 14 (2013) 1303–1347.
D. P. Kingma, J. Ba, Adam: A Method for Stochastic Optimization, CoRR abs/1412.6980 (2014).
C. M. Bishop, M. E. Tipping, Variational Relevance Vector Machines, in: Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence, UAI’00, Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2000, pp. 46–53. URL: http://dl.acm.org/citation. cfm?id=2073946.2073953.
44


N. Tishby, F. C. Pereira, W. Bialek, The Information Bottleneck Method, in: Proc. of the 37-th Annual Allerton Conference on Communication, Control and Computing, 1999, pp. 368–377. URL: /brokenurl#citeseer.nj.nec.com/tishby99information.html.
C. Grigo, P.-S. Koutsourelakis, Probabilistic reduced-order modeling for stochastic partial differential equations, in: 2nd International Conference on Uncertainty Quantification in Computational Sciences and Engineering, Eccomas Thematic Conferences, 2017, pp. 111–129. URL: https: //www.eccomasproceedia.org/conferences/thematic-conferences/uncecomp-2017/5356.
C. Grigo, P. Koutsourelakis, Bayesian model and dimension reduction for uncertainty propagation: Applications in random media, SIAM/ASA Journal on Uncertainty Quantification 7 (2019) 292–323.
A. Logg, K.-A. Mardal, G. N. Wells, et al., Automated Solution of Differential Equations by the Finite Element Method, Springer, 2012. doi:10.1007/978-3-642-23099-8.
E. Sanchez-Palencia, Non-Homogeneous Media and Vibration Theory, Lecture Notes in Physics 127 (1980).
L. Tartar, Incompressible fluid flow in a porous medium-convergence of the homogenization process, Appendix of Sanchez-Palencia (1980) (1980).
G. Allaire, Homogenization of the stokes flow in a connected porous medium, Asymptotic Analysis 2 (1989) 203–222.
G. Allaire, Homogenization of the unsteady Stokes equations in porous media, volume 267 of Pitman Research Notes in Mathematics Series, Longman Higher Education, New York, 1992, pp. 109123. URL: http://www.cmap.polytechnique.fr/~allaire/unsteady-stokes.pdf.
S. Whitaker, ”Flow in porous media I: A theoretical derivation of Darcy’s law”, Transport in Porous Media 1 (1986) 3–25.
C. Sandstr ̈om, F. Larsson, K. Runesson, H. Johansson, A two-scale finite element formulation of Stokes flow in porous media, Computer Methods in Applied Mechanics and Engineering 261-262 (2013) 96 – 104.
V. Marchenko, E. Khruslov, Homogenization of Partial Differential Equations, Birkh ̈auser Boston, Boston, MA, 2006. URL: https://doi.org/10.1007/978-0-8176-4468-0_1. doi:10.1007/ 978-0-8176-4468-0_1.
L. Tartar, The General Theory of Homogenization: A Personalized Introduction, Springer Berlin Heidelberg, Berlin, Heidelberg, 2010. URL: https://doi.org/10.1007/978-3-642-05195-1_1. doi:10.1007/978-3-642-05195-1_1.
K. Fukushima, Neocognitron - a Self-Organizing Neural Network Model for a Mechanism of PatternRecognition Unaffected by Shift in Position, Biological Cybernetics 36 (1980) 193–202.
Y. Lecun, L. Bottou, Y. Bengio, P. Haffner, Gradient-based learning applied to document recognition, Proceedings of the Ieee 86 (1998) 2278–2324.
45


A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classification with deep convolutional neural networks, in: F. Pereira, C. J. C. Burges, L. Bottou, K. Q. Weinberger (Eds.), Advances in Neural Information Processing Systems 25, Curran Associates, Inc., 2012, pp. 1097–1105. URL: http://papers.nips.cc/paper/ 4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf.
S. Torquato, Random Heterogeneous Materials, Springer, 2001.
B. Lu, S. Torquato, Lineal-path function for random heterogeneous materials, Phys. Rev. A 45 (1992) 922–929.
S. Lowell, J. E. Shields, M. A. Thomas, M. Thommes, Characterization of Porous Solids and Powders: Surface Area, Pore Size and Density, volume 1, Springer, 2006.
S. P. Sutera, R. Skalak, The History of Poiseuille’s Law, Annual Review of Fluid Mechanics 25 (1993) 1–20.
J. Kozeny, Ueber Kapillare Leitung der Wasser in Boden, Royal Academy of Science, Vienna, Proc. Class I 136 (1927) 271–306.
P. C. Carman, Fluid Flow through Granular Beds, Trans. Inst. Chem. Eng. 15 (1937) 150–166.
G. E. Archie, Electrical resistivity an aid in core-analysis interpretation, AAPG Bulletin 31 (1947) 350.
P. Soille, Morphological Image Analysis: Principles and Applications, Springer-Verlag Berlin Heidelberg, Berlin, DE, 1999, pp. 89–125.
C. Doersch, Tutorial on Variational Autoencoders, 2016.
D. A. G. Bruggeman, Berechnung verschiedener physikalischer konstanten von heterogenen substanzen. i. dielektrizittskonstanten und leitfhigkeiten der mischkrper aus isotropen substanzen, Annalen der Physik 416 (1935) 636–664.
R. Landauer, Electrical conductivity in inhomogeneous media, AIP Conference Proceedings 40 (1978) 2–45.
N. D. Lawrence, Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models, Journal of Machine Learning Research 6 (2005) 1783–1816.
M. K. Titsias, N. D. Lawrence, Bayesian Gaussian Process Latent Variable Model, in: Y. W. Teh, D. M. Titterington (Eds.), Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, volume 9 of JMLR Proceedings, JMLR.org, 2010, pp. 844–851. URL: http://www.jmlr. org/proceedings/papers/v9/titsias10a.html.
A. C. Damianou, M. K. Titsias, N. D. Lawrence, Variational Inference for Latent Variables and Uncertain Inputs in Gaussian Processes, Journal of Machine Learning Research (JMLR) 2 (2015).
A. C. Faul, Analysis of sparse Bayesian learning, Advances in Neural Information Processing Systems (NIPS) 14 (2003) 383–389.
46


M. E. Tipping, Sparse Bayesian learning and the relevance vector machine, J. Machine Learning Research (2001).
A. Gelman, J. Carlin, H. Stern, D. Rubin, Bayesian Data Analysis, 2nd ed., Chapman & Hall/CRC, 2003.
D. M. Blei, A. Kucukelbir, J. D. McAuliffe, Variational inference: A review for statisticians, Journal of the American Statistical Association 112 (2017) 859–877.
M. J. Beal, Z. Ghahramani, The Variational Bayesian EM Algorithm for Incomplete Data: with Application to Scoring Graphical Model Structures, Bayesian Statistics 7 (2003).
R. Ranganath, S. Gerrish, D. Blei, Black Box Variational Inference, in: S. Kaski, J. Corander (Eds.), Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics, volume 33 of Proceedings of Machine Learning Research, PMLR, Reykjavik, Iceland, 2014, pp. 814–822. URL: http://proceedings.mlr.press/v33/ranganath14.html.
M. Heinkenschloss, Numerical solution of implicitly constrained optimization problems, Rice University Department of Computational and applied Mathematics 05 (2008) 1–25.
P. Constantine, E. Dow, Q. Wang, Active Subspace Methods in Theory and Practice: Applications to Kriging Surfaces, SIAM Journal on Scientific Computing 36 (2014) A1500–A1524.
D. P. Kingma, M. Welling, Auto-Encoding Variational Bayes, CoRR abs/1312.6114 (2013).
D. Zhang, A Coefficient of Determination for Generalized Linear Models, The American Statistician 71 (2017) 310–316.
I. Murray, Z. Ghahramani, A note on the evidence and Bayesian Occams razor, Technical Report, Gatsby Unit Technical Report GCNU-TR 2005-003, 2005.
C. Rasmussen, Z. Ghahramani, Occam’s Razor, in: Neural Information Processing Systems 13, 2001, pp. 294–300.
Z. Ghahramani, M. J. Beal, Variational inference for Bayesian mixtures of factor analysers, NIPS 12 (2000).
47