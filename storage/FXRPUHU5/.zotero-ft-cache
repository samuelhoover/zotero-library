Annual Review of Chemical and Biomolecular Engineering
Data Science in Chemical Engineering: Applications to Molecular Science
Chowdhury Ashraf,1 Nisarg Joshi,1 David A.C. Beck,1,2 and Jim Pfaendtner1
1Department of Chemical Engineering, University of Washington, Seattle, Washington 98195, USA; email: dacb@uw.edu, jpfaendt@uw.edu 2eScience Institute, University of Washington, Seattle, Washington 98195, USA
Annu. Rev. Chem. Biomol. Eng. 2021. 12:15–37
First published as a Review in Advance on March 12, 2021
The Annual Review of Chemical and Biomolecular
Engineering is online at chembioeng.annualreviews.org
https://doi.org/10.1146/annurev-chembioeng101220-102232
Copyright © 2021 by Annual Reviews. All rights reserved
Keywords
molecular data science, chemoinformatics, machine learning, artificial intelligence, continuous representation of molecules, molecular design
Abstract
Chemical engineering is being rapidly transformed by the tools of data science. On the horizon, artificial intelligence (AI) applications will impact a huge swath of our work, ranging from the discovery and design of new molecules to operations and manufacturing and many areas in between. Early adoption of data science, machine learning, and early examples of AI in chemical engineering has been rich with examples of molecular data science—the application tools for molecular discovery and property optimization at the atomic scale. We summarize key advances in this nascent subfield while introducing molecular data science for a broad chemical engineering readership. We introduce the field through the concept of a molecular data science life cycle and discuss relevant aspects of five distinct phases of this process: creation of curated data sets, molecular representations, datadriven property prediction, generation of new molecules, and feasibility and synthesizability considerations.
15


Data science:
the conglomeration of tasks associated with applied statistics, statistical and machine learning, data management, and visualization
Machine learning (ML): algorithms, occasionally deriving from statistical methods, that can improve their performance via exposure to data
Neural network (NN): a serially hyperconnected network that amplifies and attenuates signals according to weights and activation functions resulting in an output signal
1. INTRODUCTION
The field of chemical engineering is changing rapidly with the emergence of extremely highvolume and high-velocity methods for discovering and exploring the property space of new molecules. At the center of this revolution is a suite of methods commonly referred to as molecular data science. Broadly speaking, we define data science as the umbrella of methods and approaches that include statistical and machine learning (ML), data management and curation, visualization, and increasingly, approaches that could be credibly labeled as artificial intelligence (AI). As we have discussed previously (1), these methods will broadly impact the field of chemical engineering, including disparate areas such as process and systems engineering, manufacturing and control, systems and synthetic biology, transport and complex reaction engineering, and beyond. This article focuses specifically on describing how data science is making and will make an impact in chemical engineering work pertaining to discovering and/or optimizing bespoke molecules and materials. This is a rich and rapidly evolving field of study, with new articles appearing in the peer-reviewed literature and on preprint servers at a tremendous rate. Our goal in this article is to take a snapshot of the field, provide an introduction for newcomers to this area, and offer some perspective on future directions as molecular data science matures. In many respects, the application of these approaches follows in the footsteps of successful application of data science methods to solid-state or periodic materials. Examples include the Materials Project (2), the Open Quantum Materials Database (3), AFLOWLIB.ORG (4), and the Novel Materials Database (NOMAD) laboratory (5). Despite their disparate applications, all of these approaches use high-throughput (HT) ab initio or density functional theory (DFT) calculations to compute the energies and properties of crystalline solid-state materials. By construction, these calculations are conveniently automated in software workflows owing to DFT’s ability to provide good estimates using only a relatively small number of intensive calculations. In contrast, our focus here is molecular in nature, with an emphasis on small-molecule properties or properties of molecules in liquid phase.
2. MOLECULAR DATA SCIENCE LIFE CYCLE
The remainder of this review is organized according to the molecular discovery and design life cycle, illustrated in Figure 1. The five steps in the life cycle are described in each of the following sections, followed by some concluding remarks. Briefly, the application of data science to the molecular sciences begins with data—specifically, creating uniform curated data sets of known quality. With data in hand linking molecular structure or material compositions to a property space of interest, attention must then be given to the different ways that molecules can be represented. Our discussion in this section ranges from simple molecular graphs to the use of variational autoencoders (VAEs) and latent property space representations. After translating the molecules into a computationally compatible representation, the next step is often to develop a model that can predict certain properties of arbitrary molecules. This property predictor, in turn, is used to guide the generation and validation of new molecules. Finally, these methods may also be employed to identify synthesis schemes and feasibility of scalable manufacturing for molecular systems. This design life cycle either terminates with new useful products or leads to revision of one or more of the steps, such as increasing the amount of the data used for model building or refining the models themselves.
2.1. Guided Creation of Curated Data Sets
Prior to using data-intensive methods such as neural networks (NNs) for property prediction, considerable thought should be devoted to the way data will be provisioned, used, and
16 Ashraf et al.


01001
n(C)c(=OO
1
2
3
4
5
Molecular
representation and feature engineering
Creating
curated data sets
Data-driven property prediction
Generative methods Real
Fake
Feasibility and
synthesizability Useful products
Method refinement
Molecular data science life cycle
Figure 1
Conceptual framework for application of data science methods for molecular discovery and optimization. The molecular data science life cycle begins with ❶ the creation or ingestion of curated data sets for downstream tasks. ❷ Molecules in the data set are transformed into a variety of representations that are accessible to computational tasks, such as training models for ❸ predicting the properties and activities of molecules or building models for ❹ generating new molecules with targeted properties or activities. In ❺ molecules that are generated by ❹ often need to be verified for valid chemistry and assessed for practical synthesizability. In many instances, the life cycle is a virtuous cycle that repeats by including information gleaned from new molecules back into the ❶ curated data set.
shared—commonly referred to as the data workflow. In general, three different common sources of data (Figure 2) can be used to create data sets for these applications: existing research literature or databases, HT simulation and modeling, or HT experimentation. In practice, real-world data workflows will make some use of two or all three of these data sources. This section briefly touches on the data sources and highlights some recent approaches and considerations.
2.1.1. Data mining. The peer-reviewed published literature in science and engineering contains a wealth of knowledge. It is increasingly difficult for a single individual to digest the
Data mining
HTsim
ulations
Database
HTexpe
riments
Figure 2
Curated data sets for molecular data science studies. Data sets for molecular data science may be obtained from a variety of sources. Common data sets include the text of primary literature, databases constructed by expert knowledge or computational means, analysis of high-throughput (HT) simulations such as molecular dynamics, and HT experimental methods.
www.annualreviews.org • Data Science in Chemical Engineering 17


Corpus (plural: corpora)
a collection of written text on a subject that is authoritative and can be used for machine learning
substantial volume of published research, and it is increasingly common that domain-adjacent research efforts can provide insight into one’s own area of expertise. Moreover, research publications are written for people to read, not for computers to digest. To take advantage of this untapped potential in the published literature, we can turn to text mining (TM) and natural language processing (NLP) algorithms and tools. TM and NLP approaches have evolved rapidly in concert with the development of new types of NN architectures in recent years, in particular bidirectional long short term memory (6). Recent successes with TM and NLP applied to the research literature in chemical and materials sciences have enabled discovery of new thermoelectrics (7), oxides (8), synthesis parameters for zeolites (9), and a host of other species (10–12). Building on these methods are new types of approaches that can propose new functional hypotheses about existing molecules (13, 14). The essential steps in a TM/NLP exercise include building a corpus of abstracts or full-text publications, often using keyword searches. This process can be facilitated or frustrated depending on one’s level of access to large amounts of full-text articles. The corpus can next be distilled by feature extraction from texts using a combination of classifier methods (e.g., support vector machines) and so-called term frequency–inverse document frequency (15) (TF-IDF) features from the full texts. Individual entities, be they characters (e.g., Na+), words (e.g., sodium), or phrases (e.g., sodium ion), are analyzed using a variety of modeling approaches in a process known as named entity recognition (NER). During NER, entities are tagged, extracted from the text, and replaced by a normalized representation that unifies the variety of forms in which an entity may appear in the corpus. As additional examples, character-level entities could be a molecular formula, such as C11H15NO2, and phrase-level entities can be IUPAC (International Union of Pure and Applied Chemistry) molecule names, molecular properties such as boiling point, or complex features of a system such as ionic liquid or corrosion inhibitor. The resulting NER-tagged corpus can be used for several purposes, including building a language model that relates chemicals and properties using approaches such as word2vec (16, 17). The chemdataextractor program (18) implements a version of NER, and many others are working on similar approaches.
2.1.2. High-throughput physics-based simulation. The calculation of molecular properties from molecular dynamics, Monte Carlo, and electronic structure or DFT-based methods continues to improve with time. Whereas quantum chemistry approaches greatly reduce the number of adjustable parameters (and therefore facilitate automation in calculations), the scaling of the computational cost of these methods with the number of atoms in the system belies their use in HT calculations. This is especially true in liquid or glassy systems where large ensembles of molecules must be considered to accurately calculate ensemble properties of interest. As noted in the Introduction, the materials science community has led the development of these approaches for DFT calculations of periodic materials. Additional examples include large-scale campaigns to quantify huge data sets of small molecules using quantum chemistry [e.g., analysis of ∼130K small molecules pruned from the GDB-17 (19, 20) database]. The use of automated workflows for HT molecular simulations is evolving rapidly, and many teams are working on the individual components of such workflows or simultaneous development of multiple components together, as in the MoSDeF ecosystem (21) (Figure 3). On-demand assignment of parameters in classical molecular mechanics force fields remains a challenge, although it has been greatly assisted through several projects (22–24). Subsequent to force field parameterization, an initial configuration of the atomic system with desired features, such as system density or molecular crowding, is an essential component of the HT simulation workflow (21, 25). Once the simulations are ready to run in any number of open-source molecular dynamics or Monte Carlo codes, the jobs must be staged and submitted to a high-performance computing (HPC) resource
18 Ashraf et al.


Assign force field
Structure and input files
Manage jobs
Analyze results
Archive data
Figure 3
Idealized workflow for high-throughput (HT) physics-based molecular simulations. The workflow begins by assigning force field parameters to the individual components in the systems, such as bonded and nonbonded interaction parameters. Spatial configurations for each molecule in the system must be generated, including solvents, solutes, and fixed structures. Additionally, the input deck of specialized commands for the intended simulation software needs to be generated to equilibrate and simulate the system. The simulation tasks or jobs must be distributed to computing platforms and monitored and the resulting trajectories collected for subsequent analysis. Analysis codes that extract the properties, activities, or features of interest must be run on the trajectories or, frequently, ensembles of trajectories. Finally, the entire workflow, trajectories, and resulting analyses must be preserved and stored in a repository that facilitates access by computational tools.
to be executed, and the resulting trajectories must be collected and assembled. An example simulation workflow manager is signac-flow (26, 27). When the simulations are completed, properties of interest must be computed from the trajectories using analysis workflows, often leveraging accessible languages such as Python, and requiring HPC resources, e.g., PyLAT (28) or MDAnalysis (29). Finally, the data must be archived so that they can be readily accessed, shared, and used in future studies. Examples of simulation archival systems include projects from the Molecular Simulation Software Institute, e.g., SEAMM (30) and the ChemBDDB project (31). We expect that these individual components and their encapsulation in overall workflows will continue to expand as users discover niche needs for specific applications and, hopefully, in the coming years will coalesce around a smaller number of more universal applications that can be readily used by many in the community. At the same time, we expect that emerging methods to build potentials and force fields using NNs (32, 33) will continue to grow in importance.
2.1.3. High-throughput experimentation. Many researchers are now developing HT experimental methods for generating large data sets. HT screening has been well known in the contexts of so-called combinatorial chemistry approaches (34–36) or peptide biopanning methods like phage display (37, 38). More recently, through the commoditization and modularization of robotics as a side effect of the 3D printing and additive manufacturing boom and the increasing availability of plug-and-play connectivity for devices deriving from the Internet of Things revolution, the field is seeing general availability of reconfigurable robotic instrumentation platforms. As such, we are seeing approaches to HT experimentation that combine synthesis, sample formulation, and analytics all in one step. As such robotic instrumentation becomes widely available, autonomous laboratories stand to revolutionize chemical and materials sciences by freeing researchers from performing monotonous repetitive tasks at the wet lab bench to focus more intellectual energy on hypothesis development and design of new frameworks for testing (39–43). Furthermore, the development of cyber-physical infrastructure in chemical wet labs will address access and equity by bringing new remote capabilities to researchers who are unable to access state-of-the-art technology and enabling persons with physical disabilities to engage in experimentation (44–46). An essential component within the future vision of these integrated experimental workflows is software to facilitate human–machine interactions. New software tools are continually coming to the fore that enable users with domain-level expertise to take advantage of cutting-edge robotic
www.annualreviews.org • Data Science in Chemical Engineering 19


No free lunch: an
expression in machine learning implying that a solution to a problem likely has a drawback that renders it equivalent to others
Curse of dimensionality:
as the dimensionality of a feature space increases, so too does the number of samples needed to explore it effectively
x Encoder Decoder x'
Cn1cnc2n(C) c(= O)n(C)c(= O)c12
10100110
Graphs and string-based methods Fingerprints Variational autoencoder
COMPLEXITY
N CH3
H3C
H3C
N
O
O
N
N
Figure 4
Examples of representations used in molecular data science studies. A 2D visualization of a molecule (left) carries a lot of information for human researchers but is largely opaque to computer algorithms. Therefore, it is necessary to transform the molecule into a form that is accessible to computation. A variety of molecular representations are available with increasing complexity, but not all tasks require or benefit from highly complex representations, such as the continuous representations of a variational autoencoder comprising at least two neural networks that require extensive training.
tools without the need to learn complex tooling languages for computer numerical controlled robotics (47, 48). With data-intensive methods like ML in mind, HT experimentalists must consider the data ingestion and storage paradigms to facilitate data transfer from the instrumentation to the subsequent steps in the molecular data science life cycle. One example is the ESCALATE program (49), which has seen use in halide perovskite synthesis and screening (50).
2.2. Molecular Representations and Feature Engineering
In many predictive and generative tasks in the molecular data science life cycle, the success of a given statistical and ML-based approach depends on two choices: which model to use based on the application or domain area and how to represent the molecules in a computer friendly way (Figure 4). In this section, we focus on common strategies of molecular representation for molecular data science tasks.
2.2.1. Graph- and string-based methods. Molecular representation refers to the way molecules are encoded such that they can be used for downstream input to modeling tasks such as deep learning. A model’s success depends substantially on the quality of the encoded representations and the relevance and variety of chemical features the representations are able to capture. Features in this sense refer to aspects such as the covalent bonds in a molecule, atomic partial charges, and solvent-accessible surface area of moieties. Some featurization techniques make chemically relevant information more available than others, and for a given prediction or generation task, certain features carry more information content than others. As such, feature engineering, as this process is called, is an integral part of any statistical and ML approach and frequently requires some expert knowledge in the chemical sciences to identify promising strategies. Finally, as is typical in statistical and machine learning, there is no free lunch. A representation that captures a rich set of features is likely very large in its size or dimensionality and therefore exposes the intended task to the curse of dimensionality, whereby the dimensionality of the feature representation space grossly exceeds the dimensionality of the sample space, i.e., the number of molecules used for model training. Recently, graph- and string-based methods for molecular representation have made great progress (51–53). In graph-based methods, a molecule can be considered as a graph in which the atoms and covalent bonds act like nodes and edges, respectively. These graphs can be represented as a
20 Ashraf et al.


Connectivity matrix: a 2D matrix that represents the connectivity of nodes in a graph that is symmetric with ones and zeros
connectivity matrix. In its simplest form, the connectivity matrix contains a 1 or a 0, depending on whether a pair of atoms in the column and row of the matrix are covalently bonded. Additional complexity can be introduced into the connectivity matrix by representing noncovalent bonds or hybridization by different values. Such a representation allows for the mathematical processing of molecular structures using graph theory. This graph theoretical approach has been used to predict quantitative structure property/activity relationships for decades (54–56) and is now being used for generative tasks as well (57–59). The primary advantage of graph representations is that they are both invertible, i.e., every graph is associated with a single molecule, and unique, i.e., every molecule is associated with a single graph (60). An example limitation of this approach is the loss of any three-dimensional data that may be important for predicting certain properties of a molecule or how it interacts with its environment. In addition to graph-based methods, there are string-based methods for describing molecules that go beyond stoichiometry. Many of these predate the advent of molecular data science and have been useful for sharing molecular structure information for decades, e.g., the IUPAC International Chemical Identifier (InChI) (61). Prior to the development of the InChI, another string-based approach achieved considerable adoption and has been slow to be displaced, despite having some limitations: SMILES (Simplified Molecular Input Line Entry System). Originally developed by Weininger (62) and Daylight Chemical Information Systems, SMILES is an ASCII character string representation of a depth-first search of a molecule’s graph. However, unlike the InChI, SMILES representation of particular molecules can be invertible but nonunique when represented in two dimensions, if the molecule is physically asymmetrical by translation/rotation or varies with permutation of atomic indexes. Through applying canonicalization algorithms (63), SMILES strings can be made unique, though no standard method is in consensus. In spite of this limitation, SMILES is the native encoding for widely used databases of small molecules such as ZINC (64), ChEMBL (65), and GDB-17 (19). Owing to its popularity, this representation has become the primary choice for various deep learning studies (51, 66). Other less widely used string-based representations include SMARTS (www.daylight.com) and SELFIES (67).
2.2.2. Fingerprints. Molecular fingerprints are an arbitrarily complex approach to represent molecular features. A typical fingerprinting technique includes some components of a molecule’s connectivity matrix similar to the graph approach, but additionally also indicates the presence or absence of certain structural or chemical features in a molecule. Fingerprints are often highdimensional vectors originally developed for chemometric analysis, such as quantitative structureproperty relationship (QSPR), quantitative structure–activity relationship (QSAR), and structurebased pharmaceutical design methods. A single fingerprint approach may account for various molecular features, e.g., how many hydrogen-bond donors a molecule has, structural fragments such as the presence or absence of methyl groups, and the covalent bond network (68, 69). Molecular ACCess System (MACCS) (70) is one of the most popular fingerprint methods; it uses a 1 or 0, i.e., a bit, with a total of 166 bits to indicate the presence or absence of certain features or moieties in a given molecule. Another fingerprinting scheme, extended connectivity fingerprints (ECFP) (71), extracts chemical characteristics up to a certain distance from each atom in a chemical graph and encodes those patterns using a fixed-length bit vector. Along with MACCS and ECFP, several other fingerprinting methods, such as Atom-Pair (72), topological torsion (73), E-state fingerprints (74), Avalon fingerprints (75), and ErG fingerprints (76), can all be readily calculated in RDKit (77) and ChemoPy (78), two open-source chemoinformatic packages that support a rich set of molecular fingerprints. As with the graph-based methods described previously, the above fingerprinting methods primarily contain two-dimensional structure information. More recently, there have been efforts in
www.annualreviews.org • Data Science in Chemical Engineering 21


Discrete molecular representation: a molecular representation with unordered dimensions and to which no concept of distance applies
Continuous molecular representation: a molecular representation with ordered dimensions and relative distances that can be computed
Loss function: a cost or objective function that is typically minimized to improve the performance of a machine learning model
generating three-dimensional molecular fingerprints, such as extended three-dimensional fingerprints (79) and molecular interaction fields (80), which originated in the pharmaceutical application space.
2.2.3. Variational autoencoders and continuous representations. The molecular representations mentioned above are all discrete: They describe a molecular landscape whose objects are unordered, and therefore the location of molecules in that space cannot inform the direction to a desired destination, whether structurally or functionally. In contrast, a continuous molecular representation is organized according to a high-dimensional latent space. In this landscape, distances between objects are meaningful, and this enables gradient-based optimization to approach both structural and functional or property targets. Kuhn & Beratan (81) performed one of the first attempts to use a continuous representation of molecules for the primary purpose of molecular design, using a linear combination of atomic orbitals. More recently, through the use of the VAEs (82), Gómez-Bombarelli et al. (51) demonstrated that it is possible to map discrete molecular space to a continuous and differentiable one. This creates the most complex representation of a molecule thus far. VAEs differ considerably from the graph, string-based, and fingerprint methods above, as part of their construction is a ML task in itself. The approach consists of two NNs: an encoder and a decoder (see Figure 4). A discrete representation, such as a fingerprint bit vector x, is input to the encoder network that learns to represent molecules in a continuous low-dimensional space, commonly referred to as the latent space, and a decoder is trained to translate the latent space back into the discrete representation, x′. Training a VAE involves simultaneous minimization of two loss functions, the first of which drives the system to encode and decode the latent space into the discrete space. The second loss function is known as the regularization loss, and it helps to organize a compact yet information-rich latent space, thus reducing the likelihood of overfitting. The effort to continuously represent molecules using a VAE is usually tightly coupled to a particular ML task, namely, generative modeling, in which molecules with specific properties are generated, as discussed in detail in Section 2.4.
2.3. Data-Driven Property Prediction
Perhaps the most common problem in molecular data science is the prediction of molecular properties, e.g., boiling point, given an arbitrary molecule (Figure 5). Thus far, we have been concerned with data management, curation, and representation of molecules, but these steps are ultimately in service of the core goals of molecular data science: predicting molecular properties and generating new molecules with given properties. Here, we focus on the task of predicting the properties of molecules.
2.3.1. Historical perspective. The use of statistical methods to develop QSPR dates back to at least 1884, with Mills’s (83) prediction of melting points in relation to mixture composition. Since that time, the methods have developed considerably, seeing a revolution of sorts for pharmaceutical discovery applications in the 1960s. The idea of a simple property, such as boiling point, evolved to reflect more complex contextual behaviors of molecules like binding affinity to pharmaceutical targets and the “property” label replaced with “activity,” and thus QSAR is frequently used in certain application spaces (84, 85). Therefore, QSAR and QSPR are in many ways equivalent, and herein the terms are used interchangeably. Initially developed for ligand-based pharmaceutical design (86), QSAR is basically defined as a quantitative mathematical relationship that links a discrete molecular structure with pharmacological activities. The early days of these methods used
22 Ashraf et al.


Classification:
a prediction task of identifying the class or classes an input belongs to, such as toxic or nontoxic
Regression:
a prediction task that yields a numerical quantity such as a melting point
MW
Tb Cn1cnc2n(C)c(= O)n(C)c(= O)c12
Graph
RDKit/PyChem
Features
N CH3
H3C
H3C
N
O
O
N
N F = ma
CV ∂E
∂T V
=
Neural networks
COMPLEXITY
QSAR Physics-informed ML
ab c
Figure 5
Property prediction of molecules from linear quantitative structure–activity relationships (QSAR) to physics-informed machine learning (ML) methods. A variety of statistical and ML approaches are available to the molecular data scientist for the task of predicting a molecule’s properties or activities. Three common methods are depicted. (a) A simple linear model relating molecular weight (M) to boiling point (Tb). (b) A molecule is encoded according to an information-rich representation, which is input into a trained neural network that can predict a property or activity of the molecule. (c) Similar to in b, a molecule is encoded into a representation and input into a trained neural network, but training of the network is constrained by physical laws.
simple linear models constructed from a small set of compounds (87). More recently, however, twodimensional (88) and three-dimensional (89) QSAR models have been created, their use has grown and become more diversified, and they remain a major virtual screening tool for large chemical databases in molecular docking applications (89).
2.3.2. Neural network predictions. Structure–activity relationships are very complex in practice, however, frequently arising from difficult-to-determine, separate, nonlinear effects and processes. Therefore, much of the recent focus in the field is focused on developing QSAR models free from the mathematical limitations of linear modeling as facilitated by NN, which are effective as universal function approximators (90). However, development of robust models for molecular property prediction through NNs ultimately relies on possessing a high-quality data set in which the property or activity of a large number of example molecules is known from data mining, HT simulation, or HT experiment, complemented by appropriate and information-rich molecular representations. Next, the practitioner must undertake a careful selection of model architectures and utilize rigorous accuracy metrics and model training practices (91). Put simply, a NN learns, through the training process, to map input molecular representations to an output or outputs, which are the properties of interest. The properties may be classifications, such as toxic or nontoxic, or may take the form of regression, such as predicting a molecule’s LD50 (mean lethal dose). The network architecture is critical to the success of the training and can be simple and contain an input layer of dimensionality equivalent to the size of the molecular representation that feeds into a densely connected set of hidden layers, before passing to an output layer (Figure 5), or may take on additional complexities, with important architectures in the literature being convolutional neural networks (CNN) and recurrent neural networks (RNN). A properly trained NN can be used as a rapid screening tool to quickly predict the properties or activities of a particular molecule. However, one serious issue with NN-based predictive models, and the class of deep neural networks (DNNs) in particular, is that they often contain hundreds of thousands, if not millions, of trainable and tunable parameters that require many training examples to converge on accurate representations (92). In spite of steep data requirements, DNNs and NNs have been used frequently to predict molecular electronic structure properties (93), adsorption on catalytic surfaces (94), exciton transport (95), ionic liquids (96), and other applications (97). Additionally, some approaches
www.annualreviews.org • Data Science in Chemical Engineering 23


Transfer learning: a machine learning approach that transfers knowledge learned in a model to related but different tasks to bootstrap model accuracy
Supervised learning: a machine learning task in which known, labeled input data are used to train a model to reproduce labels
Unsupervised learning: a machine learning task in which no labels are available or known and training is a process of learning the labels
Clustering: an
unsupervised learning task that organizes input samples into a space that may reveal hidden structure of the information
exist to reduce the data input requirements for a given learning task, including transfer learning (98), which takes a NN model trained for a given prediction task with high accuracy and retrains it for a similar yet distinct task. This is effective only when the operative tasks are sufficiently similar so that critical aspects of learned mapping function are relevant across the application spaces, e.g., transferring a model that predicts freezing point to boiling point for organic small molecules because both tasks involve functions of the homo-molecular interactions of the system.
2.3.3. Clustering predictions. NNs as described above are a canonical example of a supervised learning task. The models are supervised in that they are given training data labeled with the property or activity of interest and learn under that supervision. This is in contrast to unsupervised learning methods, such as clustering. In this approach, the curated data set contains only the molecular representation of molecules. A set of input molecules are grouped into clusters in the space of the molecular representation or a transformation thereof, such as principal component analysis. Under this approach, practitioners may be able to discover hidden relationships between molecules that are encoded in the input features. Additionally, when a new molecule is clustered, its properties can be inferred from those molecules in neighboring regions, for example, whether a molecule is toxic (99).
2.3.4. Physics-informed machine learning. Though NN models have been widely successful in predicting molecular properties, these models are often highly complex, such that they lack transferability and interpretability, and are occasionally referred to as black box models. These black box models typically require a very large curated data set to learn a reliable model but fail to extrapolate or occasionally make predictions that fail to satisfy basic laws or constraints of the physical systems they intend to represent. As an alternative, Raissi et al. (100) proposed physicsinformed ML, a hybrid combination of physics-based models and NN architectures that uses the full potential of both physics and data to develop a physically consistent model. Two primary approaches are available: (a) training a network based on a data set consisting of many calculations of a physical system solved numerically and (b) imposing additional constraints in the loss function that penalize any physically inconsistent outputs during training. The second approach is similar to regularization that is commonly used to guard against overfitting. The output of such a network architecture results in a new form of universal function approximator in which the laws of physics are encoded as prior information. Because of the interpretability of such models, they are now used frequently in predictive models, such as predicting properties of glasses (101) and pharmaceutical discovery (102). In addition, these physics-informed models are also used in developing NN potentials for atomistic simulations, because these systems are dynamic, and respecting the laws of physics is a basic requirement for such systems (103).
2.4. Generative Methods
A major challenge in chemical engineering is to accelerate the discovery of novel molecules and materials with lower production cost and better performance than existing alternatives or, alternately, molecules or materials whose properties are precisely tuned for their application. Historically, this is often performed through trial and error, whereby an initial candidate or lead molecule is identified, similar molecules are synthesized and evaluated, and the process is repeated until a satisfactory result is achieved. Owing to both time and practicality constraints, this approach significantly limits the possible exploration of the vast chemical space, which the Chemical Space Project estimates contains 166.4 billion molecules when constrained to just a maximum of 17 atoms consisting of C, N, O, S, and halogens (F, Cl, Br, I, and At) (104). In
24 Ashraf et al.


Variational autoencoder
Encoder Inference
Latent distribution
Decoder Generative
x
μ
σ x'
Generative adversarial network
Discriminator D(x)
Activation
x ~ p(x)
z ~ p(z)
True False
Generator G(z)
RReeaall ddaattaa
Prior
Real
Fake
Initialization Fitness evaluation
Selection Crossover Mutation
Genetic algorithm
Figure 6
Methods for generating new molecules with target properties and activities. (top to bottom) Genetic algorithms (GAs), generative adversarial networks (GANs), and variational autoencoders (VAEs). GAs borrow from natural selection to evolve molecules that meet target performance characteristics by extensive sampling. GANs use game theoretic approaches to place a molecule generator in competition with a discriminator that simultaneously improves both and results in a generator able to suggest new molecules with the intended property or activity. VAEs use a continuous representation of molecules to optimize the molecular structure with respect to the property of interest, rapidly yielding new molecules with the desired property.
contrast, the set of pharmacologically relevant small pharmaceutical-like molecules is estimated at approximately ∼1060 (64). Through the steadfast, heroic efforts of experimentalists, the direct experimental approach has been successful despite these limitations. To circumvent the above limitations, molecular data scientists have embarked on what is known as the inverse design problem (105). That is, given a set of performance parameters, the molecules are generated to satisfy those parameters with molecules from the complex chemical space; thus, these methods are named as the generative methods (see Figure 6). This approach requires sampling the vast chemical space while searching for global maxima or minima, depending on the task, of the desired properties. However, we can expect that all but the most trivial of properties will have a complex landscape relative to the molecular space. Therefore, the inverse design problem usually requires substantial computational resources and sophisticated sampling approaches. Finally, when continuous representation of molecules is possible for a design task, it is generally preferred over representations that are discrete. Recall that unlike in a continuous representation, in a discrete representation, we cannot perform gradient-based optimization in the chemical space relative to a property, and therefore, we must sample molecules directly.
2.4.1. Genetic algorithms. One direct sampling approach for generative tasks is the genetic algorithm (GA) (106). As the name suggests, GAs take their inspiration from genetics and natural selection, and they belong to a generic class of multidimensional optimization problems that have
www.annualreviews.org • Data Science in Chemical Engineering 25


been used for a variety of purposes outside the chemical sciences. A GA optimization task begins with an initial population of molecules, often selected at random or from a set of known molecules with the best-performing properties of interest. At each generation or step through the algorithm, the highest-performing molecules, as evaluated by a property prediction function, are retained, and the lowest performing are discarded. This is intended to mimic natural selection. As we saw in Section 2.3, NN provides an effective method for building a high-fidelity property prediction algorithm. Next, the top-performing molecules are kept intact but also copied and subjected to genetic manipulation, e.g., by perturbing the molecular structures through bond breaking or forming or adding or removing moieties or via recombination of two or more high-performing molecules, in an effort to mimic sexual reproduction. The process is then repeated with the new population of molecules, including the original top performers from the previous step and their mutations and recombinations. GA optimization terminates when a molecule achieves sufficient performance according to the property predictor. To circumvent the reality that similar molecules may have very different properties or activities, the GA’s use of recombination and mutation may allow the sampling to circumvent local minima in the property-derived chemical space. One of the earliest applications of GAs in molecular modeling can be dated back to 1992, when Blommers et al. (107) applied a GA to derive the solution structure of the dinucleotide photodimer. Since then, GAs have been used often in the inverse design of catalysts (108), pharmaceutical design (109), polymers (110), alloys (111), semiconductors (112), and photovoltaic materials (113). Recently, Jensen (114) demonstrated that a graph-based GA can be as good as or possibly better than state-of-the-art ML approaches for certain applications if designed properly. Despite GAs’ success, they suffer from the aforementioned critical limitation: discrete optimization in an exceptionally large chemical space. Without the presence of a defined property gradient relative to molecular structure, this method must patiently sample the chemical space, which somewhat resembles the experimental trial-and-error approach. This is where the generative models stemming from the field of ML are proving useful. Though these deep generative models can be created in many ways, in this review, we focus on only two of the most popular: generative adversarial networks (GANs) (115) and VAEs (82).
2.4.2. Generative adversarial networks. In a GAN, an approach rooted in game theory, two submodels are trained in tandem in response to a competition. One submodel, the so-called generator network, takes an input, often noise generated according to a distribution, and generates a molecular representation. The other submodel, the so-called discriminator network, tries to determine whether inputs it is exposed to are real molecules from the training set or fake molecules generated by the generative network. As their names imply, most implementations of these two submodels take the form of a NN of some kind. A round of training involves using the generative network to create a batch of fake molecules to which real training examples of known molecules are added. The discriminator network then tries to classify the molecules in the batch as real or fake. The models are updated such that the discriminator improves at its classification task of identifying real and fake molecules, and the generative network improves at generating molecules that increasingly mimic real molecules from the training set. Through this adversarial competition, both submodels learn and improve. Their competition is referred to as zero-sum, meaning that when the generative network successfully fools the discriminator into misclassifying its molecules as being real, it will have no updates made to its parameters, and the discriminator will be penalized via updates to its parameters. Conversely, when the discriminator successfully distinguishes between generative network molecules and those from the training set, it remains unchanged, and the generative network receives the training updates. This process is carried out until the generative
26 Ashraf et al.


network’s output is indistinguishable from the training data as evaluated by the discriminator, at which time the generative network can be used to generate new molecular samples with the intended properties. Often, a high-fidelity property predictor is still used to screen the generated molecules, as the generative network will yield molecules with a distribution of desired property values. The choice of the nature of the two submodels is up to the practitioner. ORGANIC (116) provides a thorough suite of examples of GAN applications for molecular generation of small molecules, pharmaceutical-like compounds, and organic photovoltaics. The authors used RNNs (117) for the generator network, a CNN (118) for the discriminator network, and a traditional feedforward NN for the property predictor, though they note non-NN tools such as DFT can substitute for the predictor if computational time is not an impediment. To optimize molecular properties in GANs and the RNN generator, a gradient with respect to the desired property is still needed. Therefore, in molecular design, GANs usually are associated with reinforcement learning (119), which provides several approaches for this gradient-based optimization problem, such as Q-learning (120) and policy gradients (121). Thus, a combination of these two algorithms has been used in multiple inverse design studies (122, 123). GAN use is subject to several potential problems. Two common issues are mode collapse and convergence failure. In mode collapse, the generator successfully learns to produce real molecules, but the variety of those molecules is extremely limited. The generator has effectively learned a very narrow but successful model that fools the discriminator but does not produce a rich set of useful molecules. Modifications to the GAN approach have been proposed that have had success in reducing or eliminating the mode collapse problem (124, 125). In a convergence failure scenario, as the generator attains a high level of success in fooling the discriminator, the training of the discriminator generally yields no improvement, and the feedback in the system introduces noise into the generator training. This can be identified by a collapse in the generator quality, indicating that it had previously reached its peak performance. Thus, close attention should be paid to the performance of the two submodels during training, and training may need to be terminated and the generator model rolled back if it begins to perform worse after a period of improvement.
2.4.3. Variational autoencoders for generative modeling. Most recently, an entirely gradient-based generative model has become an attractive option: the VAE. As described previously (Section 2.2.3), the encoder of a VAE maps the molecules to vectors in an information-rich yet low-dimensional latent space, organization of which is informed by decoder–encoder interaction. One of the most attractive features of the latent space is that it converts discrete molecules to continuous vector representations, therefore enabling both sampling and generation of new molecules from this latent space through the decoder- and gradient-based optimization of their properties. This continuous representation also allows interpolation between molecules with different properties to generate a new molecule with a desired set of properties, which significantly improves its capability as a generative model. To use a VAE for generative modeling, a property prediction model, again typically a NN, is also required that can accurately predict the property of interest for a molecule directly from its latent space representation. Because the latent space is continuous, we can perform an optimization of the property using the property prediction model to identify regions of the latent space with the desired property; decoding those areas of the latent space can thereby directly generate molecules with the desired property. Clearly, all of the components of this system require substantial training and must be as high fidelity as possible. If the latent space is poorly organized, the decoder is considerably flawed, and if the property prediction is underfit, overfit, or otherwise inaccurate, the VAE approach will fail to generate reasonable molecules.
www.annualreviews.org • Data Science in Chemical Engineering 27


Sanitization
Synthesizability
Safety
Invalid
Chemically/structurally
Unsynthesizable
Novel molecules
from generative models
Valid, synthesizable, and safe molecules
Unsafe
Environmentally
O
R
OH
Figure 7
Filtering proposed molecules based on chemical feasibility, synthesizability, and other constraints. Molecular leads are subject to post-generative screening for their chemical feasibility (i.e., Are they physically possible and chemically stable?) and their synthesizability (i.e., Can they be synthesized in a cost-effective manner, e.g., their safety or biodegradability?) Molecule leads that do meet these criteria are discarded, whereas those passing through are ready for further testing.
Gómez-Bombarelli et al. (51) provided perhaps the first application of VAEs in molecular design; they laid the groundwork for additional variants of VAEs (126, 127) for generating molecular graphs but also in the overall applications of deep learning in chemical design (60). VAEs have been used successfully in inverse design of small pharmaceutical-like molecules (51), metalloproteins and novel protein folds (128), solid-state materials (129), ionic liquids (66), and nanoporous materials (130).
2.5. Feasibility and Synthesizability
Once a candidate molecule has been generated that meets the needs of the design task, it must be evaluated for its feasibility and synthesizability constraints (Figure 7). Additionally, other constraints may be imposed at this stage, including toxicity and biodegradability. Typically, these additional constraints can be addressed via the addition of models similar in construction to the property prediction model, with the difference that they are trained to predict, e.g., LD50 or biodegradability. Here we focus on the first two of these, chemical feasibility and predicting or otherwise identifying the synthesis path and parameters for candidate molecules.
2.5.1. Molecular feasibility. Structure generation methods may not explicitly incorporate the rules of chemistry and instead may rely on learning these rules implicitly from examples in their training data. Often, that learning is imperfect, and candidate molecules with impressive property predictions contain infeasible aspects like covalent bonding arrangements that violate possible hybridization states. Thankfully, software tool kits exist to provide rapid feedback on the feasibility of candidate molecules, including the aforementioned RDKit. It can evaluate a molecule against a standard set of criteria and will flag each criterion that is in violation of the rules of chemistry. Occasionally, this process is referred to as sanitization. Offending molecules can be discarded, but this does require generating additional leads until a feasible structure is identified.
28 Ashraf et al.


2.5.2. Synthesizability. With a candidate molecule in hand that achieves a target performance metric and is a feasible structure, one must now consider how it can be synthesized. Some have proposed basing synthetic accessibility, or how difficult synthesis may be for a given molecule, on the structural complexity of the molecule, i.e., the number of rings or ring fusions or molecular weight (131, 132). However, structural complexity is only a proxy for synthesis feasibility, and therefore this is not a complete solution (131, 133, 134). Nevertheless, some of these approaches have demonstrated reasonable success. One such metric is Ertl & Schuffenhauer’s SAscore (132), which ranks molecules based on a scoring system from 1, easy to make, to 10, difficult to make. The SAscore is calculated based on the concept of fragmenting molecules according to a set of rules. In addition, it also adds a complexity penalty and symmetry bonus for the molecules. SAscore was designed to align its predictions against judgments made by synthetic chemists of pharmaceuticallike molecules. By contrast, SCScore (135) predicts the number of synthesis steps required using a ML model trained on a database of reactions, Reaxys (136). SYBA (SYnetheic Bayesian Accessibility) (131) classifies organic compounds as easy to synthesize or hard to synthesize. SYBA gives a rapid fragment score obtained by Bayesian probabilistic modeling, and the score is summed as positive for easy-to-synthesize compounds and vice versa. Synthesis accessibility is not the sole metric that could determine synthesizability, which could also be affected by, for example, the number of reagents, synthesis steps, and structural shapes (137, 138). Beyond merely ranking synthesizability, newer methods, such as MOLECULE CHEF (139), use generative models for predicting retrosynthesis pathways, which generate reactant molecules and obtain a final product molecule from those molecules using a reaction predictive model. Apart from retrosynthesis methods, many computer-aided methods focus on obtaining the synthesis planning of molecules. Feng et al. (140) summarize the developments in computer-assisted synthetic analysis. Finally, another approach to infer retrosynthetic pathways for molecules is to use methods based on computational linguistics, including the methods discussed in Section 2.1.1. For example, TF-IDF has been used to score the bond types in molecules and to obtain potential breakage sites (141). And other approaches have used machine translation, the process of using ML to translate between one human language and another, to translate the SMILES of the reactants to the appropriate products, along with an estimate of confidence (142). These approaches are encouraging, but they highlight the nature of the problem, which is that the community as a whole has been more successful in predicting forward synthesis routes, i.e., reactants to the products, than retrosynthesis routes, with the exception of single-step retrosynthesis. In the case of singlestep retrosynthesis, the accuracy for many models is similar. Very recently, however, Schwaller et al. (143) proposed a transformer model for retrosynthesis that overcomes these limitations. The model can predict complete retrosynthetic pathways via hypergraph exploration, which shows incredible promise.
3. PERSPECTIVES AND OUTLOOK
As the tools described above continue to mature, the community can expect increasingly capable automated workflows for molecular evaluation and discovery. With the rise in automation, the community should be cognizant of several aspects: the educational needs of future generations, the ethical implications of automation and AI for molecular design, and the accessibility and fairness of HT approaches.
www.annualreviews.org • Data Science in Chemical Engineering 29


Regarding education, there are essentially two routes the community can take. Learners with deep experience in the chemical sciences can be further trained as practitioners who are aware of best practices in data science to apply these methods to molecular systems. Or, alternatively, data scientists with a deep knowledge of the fundamental principles of data and computer science can be trained with chemical knowledge. Although both approaches have demonstrated success, and the authors of this review arise from both of these educational vectors, it may be that the depth of chemical knowledge is more important than knowing the internal workings of NN backpropagation. In keeping with this approach, there are already open-source educational materials that begin with a gentle approach to data science and conclude with a high level of fluency in ML best practices and software engineering (144). With regard to ethics, much ethical education in the chemical sciences and engineering tends to focus on research integrity and not on the concept of “We can, but should we?” That is, if our AI tool kits develop sufficient autonomy that one can ask a system to design and synthesize a molecule with arbitrary properties, they can certainly be used to produce novel chemical weapons with no defined antidote or treatment regime. In this regard, we may look to the extensive efforts of bioethicists and scaffold our ethical training based on what has been developed there in terms of genetic manipulation of microbes, plants, and animals. We expect that our professional societies, such as the American Institute of Chemical Engineers and the American Chemical Society, will play an important role in bridging this gap and updating relevant professional codes of conduct and ethics. Finally, as the specialized equipment required for HT efforts becomes available, be it HPC for simulation or robotics for experimentation, we must ensure equitable access to these resources. Pharmaceutical development in particular has revealed that first-world diseases that primarily affect wealthy populations who can afford to pay for treatments get moved to the front of the queue, leaving large populations of the world suffering from diseases that are likely treatable but are not as profitable. As we seek to apply the tools of molecular data science to solving pressing problems around the world, ranging from climate change to clean energy and fresh water, the feasibility constraints applied to the molecular data science life cycle must quantify the benefit (or potential risk) to everyone on the planet, and we urge advance planning for these considerations and incorporation into all aspects of chemical engineering data science thinking. We are enthusiastic about the bright promise this area of research holds for making positive impacts across chemical engineering, and we look forward to continued rapid growth and development in this subfield.
SUMMARY POINTS
1. Concepts from machine learning, data science, and artificial intelligence are rapidly infusing chemical engineering research
2. Molecular data science—the subfield of molecular design, discovery, and optimizationis at the leading edge of development and application of these methods across chemical engineering.
3. The molecular data science approach is composed of five distinct phases with a feedback loop for improvements in methodology and clear pathways for molecular design under real-world constraints.
4. The pace at which these approaches are being adopted in chemical engineering calls for clear inclusion of ethics and equity at all stages of the molecular data science life cycle.
30 Ashraf et al.


DISCLOSURE STATEMENT
The authors are not aware of any affiliations, memberships, funding, or financial holdings that might be perceived as affecting the objectivity of this review.
ACKNOWLEDGMENTS
The authors would like to thank Dr. Wesley Beckner and Orion Dollar for thoughtful comments on language. J.P. would like to acknowledge the funding from NRT-DESE: Data Intensive Research Enabling Clean Technologies (DIRECT) under grant no. NSF #1633216. C.A., D.A.C.B., and J.P. would like to acknowledge support from HDR: I-DIRSE-FW: Accelerating the Engineering Design and Manufacturing Life-Cycle with Data Science under grant no. NSF #1934292.
LITERATURE CITED
1. Beck DAC, Carothers JM, Subramanian VR, Pfaendtner J. 2016. Data science: accelerating innovation and discovery in chemical engineering. AIChE J. 62(5):1402–16 2. Jain A, Ong SP, Hautier G, Chen W, Richards WD, et al. 2013. Commentary: The Materials Project: a materials genome approach to accelerating materials innovation. APL Mater. 1:011002 3. Saal JE, Kirklin S, Aykol M, Meredig B, Wolverton C. 2013. Materials design and discovery with high-throughput density functional theory: the Open Quantum Materials Database (OQMD). JOM 65(11):1501–9 4. Curtarolo S, Setyawan W, Wang S, Xue J, Yang K, et al. 2012. AFLOWLIB.ORG: a distributed materials properties repository from high-throughput ab initio calculations. Comput. Mater. Sci. 58:227–35 5. Draxl C, Scheffler M. 2019. The NOMAD laboratory: from data sharing to artificial intelligence. J. Phys. Mater. 2(3):036001 6. Graves A, Schmidhuber J. 2005. Framewise phoneme classification with bidirectional LSTM networks. Neural Netw. 18(5–6):602–10 7. Tshitoyan V, Dagdelen J, Weston L, Dunn A, Rong Z, et al. 2019. Unsupervised word embeddings capture latent knowledge from materials science literature. Nature 571(7763):95–98 8. Kim E, Huang K, Tomala A, Matthews S, Strubell E, et al. 2017. Machine-learned and codified synthesis parameters of oxide materials. Sci. Data 4(1):170127 9. Jensen Z, Kim E, Kwon S, Gani TZH, Román-Leshkov Y, et al. 2019. A machine learning approach to zeolite synthesis enabled by automatic literature data extraction. ACS Cent. Sci. 5(5):892–99 10. Kim E, Huang K, Saunders A, McCallum A, Ceder G, Olivetti E. 2017. Materials synthesis insights from scientific literature via text extraction and machine learning. Chem. Mater. 29(21):9436–44 11. Kim E, Huang K, Jegelka S, Olivetti E. 2017. Virtual screening of inorganic materials synthesis parameters with deep learning. npj Comput. Mater. 3:53 12. Mysore S, Jensen Z, Kim E, Huang K, Chang H-S, et al. 2019. The materials science procedural text corpus: annotating materials synthesis procedures with shallow semantic structures. arXiv: 1905.06939 [cs.CL] 13. Spangler S, Myers JN, Stanoi I, Kato L, Lelescu A, et al. 2014. Automated hypothesis generation based on mining scientific literature. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1877–86. New York: Assoc. Comput. Mach. 14. Bakkar N, Kovalik T, Lorenzini I, Spangler S, Lacoste A, et al. 2018. Artificial intelligence in neurodegenerative disease research: use of IBM Watson to identify additional RNA-binding proteins altered in amyotrophic lateral sclerosis. Acta Neuropathol. 135(2):227–47 15. Wu HC, Luk RWP, Wong KF, Kwok KL. 2008. Interpreting TF-IDF term weights as making relevance decisions. ACM Trans. Inf. Syst. 26(3):1–37 16. Mikolov T, Chen K, Corrado G, Dean J. 2013. Efficient estimation of word representations in vector space. arXiv:1301.3781 [cs.CL] 17. Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26, ed. CJC
www.annualreviews.org • Data Science in Chemical Engineering 31


Burges, L Bottou, M Welling, Z Ghahramani, KQ Weinberger, pp. 3111–19. Red Hook, NY: Curran Assoc. 18. Swain MC, Cole JM. 2016. ChemDataExtractor: a toolkit for automated extraction of chemical information from the scientific literature. J. Chem. Inf. Model. 56(10):1894–904 19. Ruddigkeit L, van Deursen R, Blum LC, Reymond J-L. 2012. Enumeration of 166 billion organic small molecules in the Chemical Universe Database GDB-17. J. Chem. Inf. Model. 52(11):2864–75 20. Ramakrishnan R, Dral PO, Rupp M, von Lilienfeld OA. 2014. Quantum chemistry structures and properties of 134 kilo molecules. Sci. Data 1:140022 21. Klein C, Sallai J, Jones TJ, Iacovella CR, McCabe C, Cummings PT. 2016. A hierarchical, component based approach to screening properties of soft matter. In Foundations of Molecular Modeling and Simulation: Select Papers from FOMMS 2015, ed. RQ Snurr, CS Adjiman, DA Kofke, pp. 79–92. Singapore: Springer 22. Klein C, Summers AZ, Thompson MW, Gilmer JB, McCabe C, et al. 2019. Formalizing atom-typing and the dissemination of force fields with foyer. Comput. Mater. Sci. 167:215–27 23. Mobley DL, Bannan CC, Rizzi A, Bayly CI, Chodera JD, et al. 2018. Escaping atom types in force fields using direct chemical perception. J. Chem. Theory Comput. 14(11):6076–92 24. Jo S, Kim T, Iyer VG, Im W. 2008. CHARMM-GUI: a web-based graphical user interface for CHARMM. J. Comput. Chem. 29(11):1859–65 25. Martínez L, Andrade R, Birgin EG, Martínez JM. 2009. PACKMOL: a package for building initial configurations for molecular dynamics simulations. J. Comput. Chem. 30(13):2157–64 26. Adorf CS, Dodd PM, Ramasubramani V, Glotzer SC. 2018. Simple data and workflow management with the signac framework. Comput. Mater. Sci. 146:220–29 27. Adorf CS, Ramasubramani V, Dice BD, Henry MM, Dodd PM, Glotzer SC. 2019. glotzerlab/signac. Zenodo. https://doi.org/10.5281/zenodo.3603501
28. Humbert MT, Zhang Y, Maginn EJ. 2019. PyLAT: Python LAMMPS Analysis Tools. J. Chem. Inf. Model. 59(4):1301–5 29. Michaud-Agrawal N, Denning EJ, Woolf TB, Beckstein O. 2011. MDAnalysis: a toolkit for the analysis of molecular dynamics simulations. J. Comput. Chem. 32(10):2319–27 30. Mol. Sci. Softw. Inst. 2020. What is SEAMM? https://molssi-seamm.github.io/ 31. Hachmann J, Afzal MAF, Haghighatlari M, Pal Y. 2018. Building and deploying a cyberinfrastructure for the data-driven design of chemical systems and the exploration of chemical space. Mol. Simul. 44(11):921–29 32. Behler J, Parrinello M. 2007. Generalized neural-network representation of high-dimensional potentialenergy surfaces. Phys. Rev. Lett. 98(14):146401 33. Gao X, Ramezanghorbani F, Isayev O, Smith JS, Roitberg AE. 2020. TorchANI: a free and open source PyTorch-based deep learning implementation of the ANI neural network potentials. J. Chem. Inf. Model. 60(7):3408–15 34. Furka A. 2002. Combinatorial chemistry: 20 years on. Drug Discov. Today 7:1–4 35. Feher M, Schmidt JM. 2003. Property distributions: differences between drugs, natural products, and molecules from combinatorial chemistry. J. Chem. Inf. Comput. Sci. 43(1):218–27 36. Balkenhohl F, von dem Bussche-Hünnefeld C, Lansky A, Zechel C. 1996. Combinatorial synthesis of small organic molecules. Angew. Chem. Int. Ed. Engl. 35(20):2288–337 37. Smith GP. 1985. Filamentous fusion phage: novel expression vectors that display cloned antigens on the virion surface. Science 228(4705):1315–17 38. Sidhu SS. 2000. Phage display in pharmaceutical biotechnology. Curr. Opin. Biotechnol. 11(6):610–16 39. Häse F, Roch LM, Aspuru-Guzik A. 2019. Next-generation experimentation with self-driving laboratories. Trends Chem. 1(3):282–91 40. Häse F, Roch LM, Aspuru-Guzik A. 2018. Chimera: enabling hierarchy based multi-objective optimization for self-driving laboratories. Chem. Sci. 9(39):7642–55 41. Gromski PS, Granda JM, Cronin L. 2020. Universal chemical synthesis and discovery with “The Chemputer.” Trends Chem. 2(1):4–12 42. Bai Y, Wilbraham L, Slater BJ, Zwijnenburg MA, Sprick RS, Cooper AI. 2019. Accelerated discovery of organic polymer photocatalysts for hydrogen evolution from water through the integration of experiment and theory. J. Am. Chem. Soc. 141(22):9063–71
32 Ashraf et al.


43. Roch LM, Häse F, Kreisbeck C, Tamayo-Mendoza T, Yunker LPE, et al. 2020. ChemOS: an orchestration software to democratize autonomous discovery. PLOS ONE 15(4):e0229862 44. Brown E. 2016. Disability awareness: the fight for accessibility. Nature 532(7597):137–39 45. Soong R, Agmata K, Doyle T, Jenne A, Adamo T, Simpson A. 2018. Combining the maker movement with accessibility needs in an undergraduate laboratory: a cost-effective text-to-speech multipurpose, universal chemistry sensor hub (MUCSH) for students with disabilities. J. Chem. Educ. 95(12):2268–72 46. Soong R, Agmata K, Doyle T, Jenne A, Adamo A, Simpson AJ. 2019. Rethinking a timeless titration experimental setup through automation and open-source robotic technology: making titration accessible for students of all abilities. J. Chem. Educ. 96(7):1497–501 47. Tran O’Leary J, Peek N. 2019. Machine-o-Matic: a programming environment for prototyping digital fabrication workflows. In The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology, ed. F Guimbretiére, pp. 134–36. New York: Assoc. Comput. Mach. https://doi. org/10.1145/3332167.3356897
48. Peek N, Neil G. 2018. Mods: browser-based rapid prototyping workflow composition. In Recalibration on Imprecision and Infidelity: Proceedings of the 38th Annual Conference of the Association for Computer Aided Design in Architecture, ed. P Anzalone, M Del Signore, AJ Wit, pp. 66–71. Fargo, ND: Acadia Publ. 49. Pendleton IM, Cattabriga G, Li Z, Najeeb MA, Friedler SA, et al. 2019. Experiment specification, capture and laboratory automation technology (ESCALATE): a software pipeline for automated chemical experimentation and data management. MRS Commun. 9(3):846–59 50. Li Z, Najeeb MA, Alves L, Sherman A, Parrilla PC, et al. 2019. Robot-Accelerated Perovskite Investigation and Discovery (RAPID): 1. Inverse temperature crystallization. ChemRxiv. https://doi.org/10. 26434/chemrxiv.10013090.v1
51. Gómez-Bombarelli R, Wei JN, Duvenaud D, Hernández-Lobato JM, Sánchez-Lengeling B, et al. 2018. Automatic chemical design using a data-driven continuous representation of molecules. ACS Cent. Sci. 4(2):268–76 52. Gupta A, Müller AT, Huisman BJH, Fuchs JA, Schneider P, Schneider G. 2018. Generative recurrent networks for de novo drug design. Mol. Inform. 37(1–2):1700111 53. Segler MHS, Kogej T, Tyrchan C, Waller MP. 2018. Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS Cent. Sci. 4(1):120–31 54. Estrada E, Guevara N, Gutman I, Rodriguez L. 1998. Molecular connectivity indices of iterated line graphs. A new source of descriptors for QSPR and QSAR studies. SAR QSAR Environ. Res. 9(3–4):22940 55. Estrada E. 1996. Spectral moments of the edge adjacency matrix in molecular graphs. 1. Definition and applications to the prediction of physical properties of alkanes. J. Chem. Inf. Comput. Sci. 36(4):844–49 56. Liu S, Cao C, Li Z. 1998. Approach to estimation and prediction for normal boiling point (NBP) of alkanes based on a novel molecular distance-edge (MDE) vector, λ. J. Chem. Inf. Comput. Sci. 38(3):38794 57. Coley CW, Barzilay R, Green WH, Jaakkola TS, Jensen KF. 2017. Convolutional embedding of attributed molecular graphs for physical property prediction. J. Chem. Inf. Model. 57(8):1757–72 58. Duvenaud DK, Maclaurin D, Iparraguirre J, Bombarell R, Hirzel T, et al. 2015. Convolutional networks on graphs for learning molecular fingerprints. In Advances in Neural Information Processing Systems 28, ed. C Cortes, ND Lawrence, DD Lee, M Sugiyama, R Garnett, pp. 2224–32. Red Hook, NY: Curran Assoc. 59. Kearnes S, McCloskey K, Berndl M, Pande V, Riley P. 2016. Molecular graph convolutions: moving beyond fingerprints. J. Comput. Aided Mol. Des. 30(8):595–608 60. Elton DC, Boukouvalas Z, Fuge MD, Chung PW. 2019. Deep learning for molecular design—a review of the state of the art. Mol. Syst. Des. Eng. 4(4):828–49 61. O’Boyle NM. 2012. Towards a universal SMILES representation: a standard method to generate canonical SMILES based on the InChI. J. Cheminformat. 4:22 62. Weininger D. 1988. SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. J. Chem. Inf. Model. 28(1):31–36 63. Koichi S, Iwata S, Uno T, Koshino H, Satoh H. 2007. Algorithm for advanced canonical coding of planar chemical structures that considers stereochemical and symmetric information. J. Chem. Inf. Model. 47(5):1734–46
www.annualreviews.org • Data Science in Chemical Engineering 33


64. Virshup AM, Contreras-García J, Wipf P, Yang W, Beratan DN. 2013. Stochastic voyages into uncharted chemical space produce a representative library of all possible drug-like compounds. J. Am. Chem. Soc. 135(19):7296–303 65. Gaulton A, Hersey A, Nowotka M, Bento AP, Chambers J, et al. 2017. The ChEMBL database in 2017. Nucleic Acids Res. 45:D945–54
66. Beckner W, Ashraf CM, Lee J, Beck DAC, Pfaendtner J. 2020. Continuous molecular representations of ionic liquids. J. Phys. Chem. B 124(38):8347–57 67. Krenn M, Häse F, Nigam A, Friederich P, Aspuru-Guzik A. 2020. Self-Referencing Embedded Strings (SELFIES): a 100% robust molecular string representation. arXiv:1905.13741 [cs.LG] 68. Xue L, Godden J, Gao H, Bajorath J. 1999. Identification of a preferred set of molecular descriptors for compound classification based on principal component analysis. J. Chem. Inf. Comput. Sci. 39(4):699–704 69. McGregor MJ, Muskal SM. 1999. Pharmacophore fingerprinting. 1. Application to QSAR and focused library design. J. Chem. Inf. Comput. Sci. 39(3):569–74 70. Durant JL, Leland BA, Henry DR, Nourse JG. 2002. Reoptimization of MDL keys for use in drug discovery. J. Chem. Inf. Comput. Sci. 42(6):1273–80 71. Rogers D, Hahn M. 2010. Extended-connectivity fingerprints. J. Chem. Inf. Model. 50(5):742–54 72. Carhart RE, Smith DH, Venkataraghavan R. 1985. Atom pairs as molecular features in structure-activity studies: definition and applications. J. Chem. Inf. Comput. Sci. 25(2):64–73 73. Nilakantan R, Bauman N, Dixon JS, Venkataraghavan R. 1987. Topological torsion: a new molecular descriptor for SAR applications. Comparison with other descriptors. J. Chem. Inf. Comput. Sci. 27(2):8285 74. Hall LH, Kier LB. 1995. Electrotopological state indices for atom types: a novel combination of electronic, topological, and valence state information. J. Chem. Inf. Comput. Sci. 35(6):1039–45 75. Gedeck P, Rohde B, Bartels C. 2006. QSAR—how good is it in practice? Comparison of descriptor sets on an unbiased cross section of corporate data sets. J. Chem. Inf. Model. 46(5):1924–36 76. Stiefl N, Watson IA, Baumann K, Zaliani A. 2006. ErG: 2D pharmacophore descriptions for scaffold hopping. J. Chem. Inf. Model. 46(1):208–20 77. Landrum G, Tosco P, Kelley B, Sriniker, Gedeck, et al. 2020. rdkit/rdkit: 2020_03_1 (Q1 2020) Release. Rec., Zenodo. https://zenodo.org/record/3732262#.X672_shKhjV
78. Cao D-S, Xu Q-S, Hu Q-N, Liang Y-Z. 2013. ChemoPy: freely available python package for computational biology and chemoinformatics. Bioinformatics 29(8):1092–94 79. Axen SD, Huang X-P, Cáceres EL, Gendelev L, Roth BL, Keiser MJ. 2017. A simple representation of three-dimensional molecular structure. J. Med. Chem. 60(17):7393–409 80. Artese A, Cross S, Costa G, Distinto S, Parrotta L, et al. 2013. Molecular interaction fields in drug discovery: recent advances and future perspectives. WIREs Comput. Mol. Sci. 3(6):594–613 81. Kuhn C, Beratan DN. 1996. Inverse strategies for molecular design. J. Phys. Chem. 100(25):10595–99 82. Kingma DP, Welling M. 2014. Auto-encoding variational Bayes. arXiv:1312.6114 [cs.Stat] 83. Mills EJ. 1884. On melting-point and boiling-point as related to chemical composition. Lond. Edinb. Dublin Philos. Mag. J. Sci. 17(105):173–87
84. Leo A, Hansch C, Church C. 1969. Comparison of parameters currently used in the study of structureactivity relationships. J. Med. Chem. 12(5):766–71 85. Cherkasov A, Muratov EN, Fourches D, Varnek A, Baskin II, et al. 2014. QSAR modeling: Where have you been? Where are you going to? J. Med. Chem. 57(12):4977–5010 86. Hansch C, Fujita T. 1964. p-σ-π analysis. A method for the correlation of biological activity and chemical structure. J. Am. Chem. Soc. 86(8):1616–26 87. Wells PR. 1963. Linear free energy relationships. Chem. Rev. 63(2):171–219 88. Roy K, Das RN. 2014. A review on principles, theory and practices of 2D-QSAR. Curr. Drug Metab. 15(4):346–79 89. Rasulev B. 2016. Recent developments in 3D QSAR and molecular docking studies of organic and nanostructures. In Handbook of Computational Chemistry, ed. J Leszczynski, A Kaczmarek-Kedziera, T Puzyn, MG Papadopoulos, H Reis, MK Shukla, pp. 2133–61. Cham, Switz.: Springer Int. 90. Hornik K, Stinchcombe M, White H. 1989. Multilayer feedforward networks are universal approximators. Neural Netw. 2(5):359–66
34 Ashraf et al.


91. Danielson ML, Hu B, Shen J, Desai PV. 2017. In silico ADME techniques used in early-phase drug discovery. In Translating Molecules into Medicines: Cross-Functional Integration at the Drug DiscoveryDevelopment Interface, ed. SN Bhattachar, JS Morrison, DR Mudra, DM Bender, pp. 81–117. Cham, Switz.: Springer Int. 92. Lake BM, Salakhutdinov R, Tenenbaum JB. 2015. Human-level concept learning through probabilistic program induction. Science 350(6266):1332–38 93. Montavon G, Rupp M, Gobre V, Vazquez-Mayagoitia A, Hansen K, et al. 2013. Machine learning of molecular electronic properties in chemical compound space. New J. Phys. 15(9):095003 94. Ulissi ZW, Tang MT, Xiao J, Liu X, Torelli DA, et al. 2017. Machine-learning methods enable exhaustive searches for active bimetallic facets and reveal active site motifs for CO2 reduction. ACS Catal. 7(10):6600–8 95. Häse F, Valleau S, Pyzer-Knapp E, Aspuru-Guzik A. 2016. Machine learning exciton dynamics. Chem. Sci. 7(8):5139–47 96. Beckner W, Mao CM, Pfaendtner J. 2018. Statistical models are able to predict ionic liquid viscosity across a wide range of chemical functionalities and experimental conditions. Mol. Syst. Des. Eng. 3:25363 97. Zubatyuk R, Smith JS, Leszczynski J, Isayev O. 2019. Accurate and transferable multitask prediction of chemical properties with an atoms-in-molecules neural network. Sci. Adv. 5(8):eaav6490 98. Yamada H, Liu C, Wu S, Koyama Y, Ju S, et al. 2019. Predicting materials properties with little data using shotgun transfer learning. ACS Cent. Sci. 5(10):1717–30 99. Sharma AK, Srivastava GN, Roy A, Sharma VK. 2017. ToxiM: a toxicity prediction tool for small molecules developed using machine learning and chemoinformatics approaches. Front. Pharmacol. 8:880 100. Raissi M, Perdikaris P, Karniadakis GE. 2019. Physics-informed neural networks: a deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. J. Comput. Phys. 378:686–707 101. Liu H, Fu Z, Yang K, Xu X, Bauchy M. 2019. Machine learning for glass science and engineering: a review. J. Non-Cryst. Solids X 4:100036
102. Moon S, Zhung W, Yang S, Lim J, Kim WY. 2020. PIGNet: a physics-informed deep learning model toward generalized drug-target interaction predictions. arXiv:2008.12249 [cs.Q-Bio] 103. Pun GPP, Batra R, Ramprasad R, Mishin Y. 2019. Physically informed artificial neural networks for atomistic modeling of materials. Nat. Commun. 10:2339 104. Reymond J-L. 2015. The chemical space project. Acc. Chem. Res. 48(3):722–30 105. Ferguson AL. 2017. Machine learning and data science in soft materials engineering. J. Phys. Condens. Matter 30(4):043002
106. Holland JH. 1992. Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence. Cambridge, MA: MIT Press. 1st ed.
107. Blommers MJJ, Lucasius CB, Kateman G, Kaptein R. 1992. Conformational analysis of a dinucleotide photodimer with the aid of the genetic algorithm. Biopolymers 32(1):45–52 108. Froemming NS, Henkelman G. 2009. Optimizing core-shell nanoparticle catalysts with a genetic algorithm. J. Chem. Phys. 131(23):234103 109. Douguet D, Thoreau E, Grassy G. 2000. A genetic algorithm for the automated generation of small organic molecules: drug design using an evolutionary algorithm. J. Comput. Aided Mol. Des. 14(5):44966 110. Silva CM, Biscaia EC. 2003. Genetic algorithm development for multi-objective optimization of batch free-radical polymerization reactors. Comput. Chem. Eng. 27(8):1329–44 111. Anijdan SHM, Bahrami A, Hosseini HRM, Shafyei A. 2006. Using genetic algorithm and artificial neural network analyses to design an Al-Si casting alloy of minimum porosity. Mater. Des. 27(7):605–9 112. d’Avezac M, Luo J-W, Chanier T, Zunger A. 2012. Genetic-algorithm discovery of a direct-gap and optically allowed superstructure from indirect-gap Si and Ge semiconductors. Phys. Rev. Lett. 108(2):027401 113. Kanal IY, Hutchison GR. 2017. Rapid computational optimization of molecular properties using genetic algorithms: searching across millions of compounds for organic photovoltaic materials. arXiv:1707.02949 [physics.ap-ph]
www.annualreviews.org • Data Science in Chemical Engineering 35


114. Jensen JH. 2019. A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space. Chem. Sci. 10(12):3567–72 115. Goodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, et al. 2014. Generative adversarial nets. arXiv. 1406.2661 [stat.ML] 116. Sanchez-Lengeling B, Outeiral C, Guimaraes GL, Aspuru-Guzik A. 2017. Optimizing distributions over molecular space. An Objective-Reinforced Generative Adversarial Network for Inverse-design Chemistry (ORGANIC). ChemRxiv. https://doi.org/10.26434/chemrxiv.5309668.v2
117. Sherstinsky A. 2020. Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network. Physica 404:132306 118. Kim Y. 2014. Convolutional neural networks for sentence classification. arXiv:1408.5882 [cs.CL] 119. Kaelbling LP, Littman ML, Moore AW. 1996. Reinforcement learning: a survey. J. Artif. Intell. Res. 4:237–85 120. Mnih V, Kavukcuoglu K, Silver D, Graves A, Antonoglou I, et al. 2013. Playing Atari with deep reinforcement learning. arXiv:1312.5602 [cs.LG] 121. Silver D, Huang A, Maddison C, Guez A, Sifrev L, et al. 2016. Mastering the game of Go with deep neural networks and tree search. Nature 529:484 122. Putin E, Asadulaev A, Vanhaelen Q, Ivanenkov Y, Aladinskaya AV, et al. 2018. Adversarial threshold neural computer for molecular de novo design. Mol. Pharm. 15(10):4386–97 123. Dan Y, Zhao Y, Li X, Li S, Hu M, Hu J. 2020. Generative adversarial networks (GAN) based efficient sampling of chemical composition space for inverse design of inorganic materials. npj Comput. Mater. 6:84 124. Metz L, Poole B, Pfau D, Sohl-Dickstein J. 2017. Unrolled generative adversarial networks. arXiv:1611.02163 [cs.Stat] 125. Arjovsky M, Chintala S, Bottou L. 2017. Wasserstein GAN. arXiv:1701.07875 [stat.ML] 126. Simonovsky M, Komodakis N. 2018. GraphVAE: towards generation of small graphs using variational autoencoders. arXiv:1802.03480 [cs.LG] 127. Jin W, Barzilay R, Jaakkola T. 2018. Junction tree variational autoencoder for molecular graph generation. arXiv:1802.04364 [cs.LG] 128. Greener JG, Moffat L, Jones DT. 2018. Design of metalloproteins and novel protein folds using variational autoencoders. Sci. Rep. 8:16189 129. Noh J, Kim J, Stein HS, Sanchez-Lengeling B, Gregoire JM, et al. 2019. Inverse design of solid-state materials via a continuous representation. Matter 1(5):1370–84 130. Yao Z, Sanchez-Lengeling B, Bobbitt NS, Bucior BJ, Kumar SGH, et al. 2020. Inverse design of nanoporous crystalline reticular materials with deep generative models. ChemRxiv. Preprint. https:// doi.org/10.26434/chemrxiv.12186681.v1
131. Voršilák M, Koláˇr M, Cˇ melo I, Svozil D. 2020. SYBA: Bayesian estimation of synthetic accessibility of organic compounds. J. Cheminform. 12:35 132. Ertl P, Schuffenhauer A. 2009. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. J. Cheminform. 1:8 133. Huang Q, Li L-L, Yang S-Y. 2011. RASA: a rapid retrosynthesis-based scoring method for the assessment of synthetic accessibility of drug-like molecules. J. Chem. Inf. Model. 51(10):2768–77 134. Gillet VJ, Myatt G, Zsoldos Z, Johnson AP. 1995. SPROUT, HIPPO and CAESA: tools for de novo structure generation and estimation of synthetic accessibility. Perspect. Drug Discov. Des. 3(1):34–50 135. Coley CW, Rogers L, Green WH, Jensen KF. 2018. SCScore: synthetic complexity learned from a reaction corpus. J. Chem. Inf. Model. 58(2):252–61 136. Lawson AJ, Swienty-Busch J, Géoui T, Evans D. 2014. The making of Reaxys—towards unobstructed access to relevant chemistry information. In The Future of the History of Chemical Information, ed. LR McEwen, RE Buntrock, pp. 127–48. Washington, DC: Am. Chem. Soc. 137. Popova M, Isayev O, Tropsha A. 2018. Deep reinforcement learning for de novo drug design. Sci. Adv. 4(7):eaap7885 138. Gao W, Coley CW. 2020. The synthesizability of molecules proposed by generative models. J. Chem. Inf. Model. 60(12):5714–23
36 Ashraf et al.


139. Bradshaw J, Paige B, Kusner MJ, Segler MHS, Hernández-Lobato JM. 2019. A model to search for synthesizable molecules. arXiv:1906.05221 [Phys. Stat] 140. Feng F, Lai L, Pei J. 2018. Computational chemical synthesis analysis and pathway design. Front. Chem. 6:199 141. Cadeddu A, Wylie EK, Jurczak J, Wampler-Doty M, Grzybowski BA. 2014. Organic chemistry as a language and the implications of chemical linguistics for structural and retrosynthetic analyses. Angew. Chem. Int. Ed. 53(31):8108–12 142. Schwaller P, Laino T, Gaudin T, Bolgar P, Hunter CA, et al. 2019. Molecular Transformer: a model for uncertainty-calibrated chemical reaction prediction. ACS Cent. Sci. 5(9):1572–83 143. Schwaller P, Petraglia R, Zullo V, Nair VH, Haeuselmann RA, et al. 2020. Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy. Chem. Sci. 11(12):3316–25 144. Beck D, Pfaendtner J, Curtis C, Prakash A, Wolf C, Montoni N. 2020. UWDIRECT/ UWDIRECT.github.io v2020a. Rec., Zenodo. https://zenodo.org/record/3572827#.X673VchKhjU
www.annualreviews.org • Data Science in Chemical Engineering 37